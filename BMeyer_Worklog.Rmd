---
title: "BMeyer_Worklog"
author: "bemeyer@alaska.edu"
date: "11/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



This document will be used to record work done by Benjamin Meyer, UAF Research Technician, on importing and QA/QC of data files for the SWSHP Bristol Bay Thermal Diversity Project.

Hours view-able at this Google Sheet, submitted bi-weekly to UAonline: https://docs.google.com/spreadsheets/d/1iS4BnXhsgqgQmZyixOiFC9R6uyZO4FeliF3bUjpRauo/edit#gid=0

<br> 

11/2/20 

Github is used in the project for version control and collaboration.  Below are some notes on including Github in workflow for this project:

* Original data files (csv and excel) are large and best not to have to push/pull from Github with each work session.  Thus the folders "data" and "output" are ignored via the .gitignore file.  To access original data files, they may be downloaded to your local directory from Google Drive at https://drive.google.com/drive/u/3/folders/1XQ2rkLnbsHB_hTaNCUnd6u8-iO_cuuNV

* At the beginning of each work session, use the "Pull with Rebase" button to synch all files in this .Rproj with the Github repo.

* At appropriate moments during work flow, in addition to saving the file(s) being worked on, use the "commit" button to commit an edit to your file(s).

* When ready to synch your .Rproj to the Github repo, click the "Commit" button, write an appropriate commit message, click the box next to each file in the "Staged" column, click "Commit", then use the "Push" arrow to synch to Github.

* Best practice is for only one person to be editing a file at a time, to avoid conflicted versions

<br>

11/5/20
Completed draft of UAA-ACCS data import/clean.  Began work on FWS data, not yet successful in reading in sheets; inidvidual files are saved in tabs.  Looking through previous examples from R Shaftel.

11/6/20
Continued working on UW data.  Sorted through some erroneous date/time formats and read in entire dataset.  Currently working to identify/eliminate data from logger malfunction or exposure.


11/9/20
Made some progress on data_UW.Rmd.  Was stuck on some coding syntax questions, had conversation w/ Becky for 1/2 hr.  She made a variety of notes and some code to address questions.  Decided that data can be QA/QC'd and excised at the scale of daily mean, less useful to look at original observations at sub-daily level.

Able to read in all FWS data, combined w/ metadata to assign AKOATS_ID and waterbody name.

11/10/20
AM: work in progress to ID erroneous UW data.  PM: Further work on data_UW.Rmd

11/11/20
Worked on data_NPS.Rmd.  Read in of inconsistent file formats proved challenging.  Some site names in the "Pilot Sites" file did not have unique metadata matches in the "Site_Info" file.  Site coordinates were all unique though, so the data files are to be QA/QC'd separately and treated separately for now.  Able to read in all data except lake buoy sites; work in progress.  

11/13/20
AM and PM: completed NPS read in and QA QC.  Saved Lake and Beaches & streams files separately because lakes have depth data column.  Started work on USGS data read in. the USGS R package "readNWIS" works very nicely.

11/14/20
Completed read-in of USGS and CIK data.  Working on resolving numerous site name conflicts with UW data. Will send email to Becky w/ questions for J Carter on Monday.

11/16/20
Completed read-in of all data.  Working on metdata summary. Just about to hit ~40 hrs total time worked.
