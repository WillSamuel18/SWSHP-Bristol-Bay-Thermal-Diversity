---
title: "ACCS-UAA Water Temp Data Import"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```


Read in data from ACCS folder
```{r}
accs.data <-list.files(path = paste0("data/ACCS-UAA/"),
               pattern = "*.csv", 
               full.names = T) %>% 
    map_df(~read_csv(., col_types = cols(.default = "c"),col_names = T)) 

# note: some files have a "location description" column and others do not

# rename, transform, and eliminate columns to match final field names described in Project_notes.Rmd
accs.data <- accs.data %>%
  select(AKOATS_ID,sampleDate,sampleTime,Temperature) %>%
  transform(sampleDate = as.Date(mdy(sampleDate)),
            sampleTime = hms::as_hms(sampleTime),
            Temperature = as.numeric(Temperature)) 


# for visualization and data QA/QC, we want waterbody name and/or SiteID associated with each dataset

# read in metadata file and associate SiteID with AKOATS_ID
md <- read.csv("data/ACCS-UAA/metadata/SiteLevelMetadata_Bogan.csv") %>%
  select(AKOATS_ID,SiteID,Waterbody_name)

# use info from pdf at "data/ACCS-UAA/metadata/doi_10.5063_F1028PR9-METADATA.pdf" to assign waterbody names in blank spaces
md.x <- md %>%
  filter(AKOATS_ID <= 764) %>%
  select(-Waterbody_name)

# 760 = Koktuli River
# 761 = Tributary of Koktuli River 1
# 762 = Tributary of Koktuli River 2
# 763 = tributary of Koktuli River 3
# 764 = tributary of Newhalen River

# create completed metadata table
Waterbody_name <- c("Koktuli River", "Tributary of Koktuli River 1", "Tributary of Koktuli River 2","Tributary of Koktuli River 3","Tributary of Newhalen River")
AKOATS_ID <- 760:764
md.x1 <- data.frame(Waterbody_name,AKOATS_ID)

md.x <- left_join(md.x,md.x1,by = "AKOATS_ID")

# rejoin metadata w/ manually input site names back to UAA-ACCS metadata
md <- md %>%
  filter(AKOATS_ID > 764) %>%
  bind_rows(md.x) %>%
  transform(AKOATS_ID = as.character(AKOATS_ID))

# associate SiteID and Waterbody_name with overall UAA-ACCS temperature dataset
accs.data <- left_join(accs.data,md,by = "AKOATS_ID")
  
```

<br>

Perform a quick visualization to see extent and form of data
```{r}
accs.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Waterbody_name,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID, scales = "free_y") +
  ggtitle("Original Logger Data by Site and Year")

```

<br>

Looks like we have some malfunctioning loggers at site 1664 and maybe some exposure data at other sites.  Lets identify these data as not usable by assigning a "0" in the "useData" column.

Process to identify and remove streams & beaches erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

```{r}

# list of suspect sites to manually examine w/ ggplotly 
suspect.accs.sites <-data.frame(c("Bonanza Creek",
                                  "Tazimina River",
                                  "mutsk02",
                                  "mutsk09",
                                  "muekm23"))
colnames(suspect.accs.sites) <- "SiteID"

# specify year or years to visualize
y <- c("2014")

# create ggplotly chart
ggplotly(
  p <- accs.data %>%
  inner_join(suspect.accs.sites) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "muekm23"
         # put hash tag at next line to see all years for the site
          ,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )

```

<br>

If further data is is identified to be flagged, edit the csv file at "data/flagged_data/NPS_flagged_data.csv"



<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
accs_flagged_data <- read.csv("data/flagged_data/ACCS_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
accs_flagged_data <- accs.data %>% 
  rename(Agency_ID = SiteID) %>%
  mutate(year = year(sampleDate)) %>%
  inner_join(accs_flagged_data,by = c("Agency_ID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year,-day) %>%
  rename(SiteID = Agency_ID) 

# remove 
accs_nonflagged_data <- anti_join(accs.data,accs_flagged_data,by = c("SiteID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
accs.data <- bind_rows(accs_flagged_data,accs_nonflagged_data)

# remove extraneous objects
rm(p,accs_flagged_data,accs_nonflagged_data,suspect.accs.sites)

# data to be excised is now designated with "useData = 0"

```


<br>

Perform a quick visualization of "cleaned up" data
```{r}
accs.data %>%
  filter(useData == 1) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Waterbody_name,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Waterbody_name) +
  ggtitle("Corrected Logger Data by Site and Year")

```

<br>

Improvement is evident, probably more cleaning to be done at sub-daily mean level.

<br>

Interactive data summary table
```{r}
# create data summary table
accs.data.summary <- accs.data %>%
  filter(useData == 1) %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,AKOATS_ID,Waterbody_name,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

accs.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)
    
```

<br>

Export csv of combined datasets, with newly identified "do not use data" signified with a "0" in the useData column, to data folder associated with this .Rproj
```{r}
# reorder columns
x <- accs.data %>% select(AKOATS_ID,SiteID,sampleDate,sampleTime,Temperature,useData)

# export csv
write.csv(x,"output/UAA-ACCS.csv", row.names = F)

```
