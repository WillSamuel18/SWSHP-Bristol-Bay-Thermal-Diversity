---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```

<br>

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201104.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in single-logger-per-site datasets (streams, beaches)
```{r}
# read in all sites that are not multi-level lake logger sites or pilot sites
nps.data.1 <- list.files(path = "data/NPS Bartz",
         pattern="*.csv", 
         full.names = T) %>% 
   map_df(function(x) read_csv(x,skip = 15, col_types = cols(.default = "c"),col_names = F) %>% 
            mutate(filename=gsub(".csv","",basename(x))))
colnames(nps.data.1) <- c("a","DateTime","Temperature","b","c","d","File for B_Shaftel") 
nps.data.1 <- nps.data.1 %>% select(-a,-b,-c,-d)

# assign AgencyID and waterbody name
nps.data.1 <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  right_join(nps.data.1) %>%
  select(-"File for B_Shaftel") %>%
  # make up a SiteID name for plotting
  mutate(SiteID = paste0(Agency_ID,"_",Waterbody_Name)) %>%
  # format col type
  transform(Temperature = as.numeric(Temperature))


# read and format in "All_Pilot_Sites_20201104.csv"
nps.data.2 <- read.csv("data/NPS Bartz/Pilot_sites/All_Pilot_Sites_20201104.csv") %>%
  mutate(DateTime = paste(Date,Time)) %>%
  select(-X,-Date,-Time) %>%
  pivot_longer(!DateTime, names_to = "SiteID", values_to = "Temperature") %>%
  transform(Temperature = as.numeric(Temperature))

# we want to use SiteID to associate with metadata.  However, there are two issues:

# primary issue: site names (column names) in "All_Pilot_Sites_20201104.csv" do not have an exact match anywhere in the "Site_Info.xlsx" metadata file.  Thus we will manually generate a dataframe to match site names to metadata

# a secondary issue: in "Site_Info.xlsx", neither the "Agency_ID" column nor "Waterbody_Name" columns have unique IDs for each site.  

# Solution: manually create join table to associate names in "Site_Info.xlsx" with names in "All_Pilot_Sites_20201104.csv"

# FYI: the two sites are 36m apart; but should still be QA/QC'd separately, so keep separate at this stage of analysis

sitenames.1 <- data.frame(c("Margot_Water","Up_A_Tree_Water",
                      "Idavain_Water","Chulitna_Water",
                      "Kijik_Beach_Water","Port_Alsworth_Water",
                      "Little_Kijik_Water","Tazimina_Water",
                      "Tlikakila_Water_A","Tlikakila_Water_B",
                      "Savonoski_Water"))
colnames(sitenames.1) <- "SiteID"


sitenames.2 <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  filter(`File for B_Shaftel` == "All_Pilot_Sites_20201104") %>%
  select(-`File for B_Shaftel`) 

sitenames <- bind_cols(sitenames.1,sitenames.2)

# join nps.data.2 to assigned site names 
nps.data.2 <- left_join(nps.data.2,sitenames,by = "SiteID") %>%
  filter(!is.na(Temperature))

# join "pilot data" with other single-logger-per-site data
nps.data <- bind_rows(nps.data.1,nps.data.2)

# remove extraneous objects
rm(nps.data.1,nps.data.2,sitenames,sitenames.1,sitenames.2)
```

<br>

Read in multiple-logger-per-site datasets (buoys @ lake centers)
```{r}
# read in all sites that ARE multi-level lake logger sites 

# Notes
# - There is not a consistent number of depth levels (ranges from ~ 5 - 10 depths)

## one single file read-in & format example
#nps.lakes <- tibble()
#filepath <- "data/NPS Bartz/Multilevel_Lake_Sites/KIJIL_01_temp_20201104.csv"
#file <- read.csv(filepath, skip = 3) %>%
#  filter(X != "Timestamp (UTC-08:00)") 
#colnames(file) <- sub("Water.Temp\\.", "Depth", colnames(file))
#file <- file %>%
#  rename(DateTime = X) %>%
#  pivot_longer(!DateTime, names_to = "Depth", values_to = "Temperature")
#file <- file %>% mutate(SiteID = basename(filepath))
#nps.lakes <- bind_rows(nps.lakes,file) 


## read in and bind multiple files
all_paths <- list.files("data/NPS Bartz/Multilevel_Lake_Sites",
                        full.names = T)

# iterate over all files in folder
nps.lakes <- tibble()
for(i in all_paths){
file <- read.csv(i, skip = 3) %>%
  filter(X != "Timestamp (UTC-08:00)") 
colnames(file) <- sub("Water.Temp\\.", "Depth", colnames(file))
file <- file %>%
  rename(DateTime = X) %>%
  pivot_longer(!DateTime, names_to = "Depth", values_to = "Temperature")
file <- file %>% mutate(SiteID = basename(i))
nps.lakes <- bind_rows(nps.lakes,file) 
}

# create Agency_ID and Waterbody Name column


```

<br>

Visualization & ID of erroneous data
```{r}

```



Save output data
```{r}

# output lake data file seperately, since it has an additional column for "depth" that all the other data does not

# write streams & beaches csv


# write lakes csv


```



* Among sites and years, various types of loggers are employed (sonde, level logger, HOBO, etc)


