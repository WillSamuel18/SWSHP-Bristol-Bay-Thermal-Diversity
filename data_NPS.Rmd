---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```

<br>

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201104.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in single-logger-per-site datasets (streams, beaches)
```{r}
# read in all sites that are not multi-level lake logger sites or pilot sites
nps.data.1 <- list.files(path = "data/NPS Bartz",
         pattern="*.csv", 
         full.names = T) %>% 
   map_df(function(x) read_csv(x,skip = 15, col_types = cols(.default = "c"),col_names = F) %>% 
            mutate(filename=gsub(".csv","",basename(x))))
colnames(nps.data.1) <- c("a","DateTime","Temperature","b","c","d","File for B_Shaftel") 
nps.data.1 <- nps.data.1 %>% select(-a,-b,-c,-d)

# assign AgencyID and waterbody name
nps.data.1 <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  right_join(nps.data.1) %>%
  select(-"File for B_Shaftel") %>%
  # make up a SiteID name for plotting
  mutate(SiteID = paste0(Agency_ID,"_",Waterbody_Name)) %>%
  # format col type
  transform(Temperature = as.numeric(Temperature))


# read and format in "All_Pilot_Sites_20201104.csv"
nps.data.2 <- read.csv("data/NPS Bartz/Pilot_sites/All_Pilot_Sites_20201104.csv") %>%
  # modify Date format to use "-" instead of "/" so that lubridate can parse it
  separate(Date,sep = "/",into = c("m","d","y")) %>% 
  mutate(Date = paste0(y,"-",m,"-",d),
         DateTime = paste(Date,Time)) %>%
  select(-X,-Date,-Time,-m,-d,-y) %>%
  # create long format
  pivot_longer(!DateTime, names_to = "SiteID", values_to = "Temperature") %>%
  transform(Temperature = as.numeric(Temperature))

# we want to use SiteID to associate with metadata.  However, there are two issues:

# primary issue: site names (column names) in "All_Pilot_Sites_20201104.csv" do not have an exact match anywhere in the "Site_Info.xlsx" metadata file.  Thus we will manually generate a dataframe to match site names to metadata

# a secondary issue: in "Site_Info.xlsx", neither the "Agency_ID" column nor "Waterbody_Name" columns have unique IDs for each site.  

# Solution: manually create join table to associate names in "Site_Info.xlsx" with names in "All_Pilot_Sites_20201104.csv"

# FYI: the two sites are 36m apart; but should still be QA/QC'd separately, so keep separate at this stage of analysis

sitenames.1 <- data.frame(c("Margot_Water","Up_A_Tree_Water",
                      "Idavain_Water","Chulitna_Water",
                      "Kijik_Beach_Water","Port_Alsworth_Water",
                      "Little_Kijik_Water","Tazimina_Water",
                      "Tlikakila_Water_A","Tlikakila_Water_B",
                      "Savonoski_Water"))
colnames(sitenames.1) <- "SiteID"


sitenames.2 <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  filter(`File for B_Shaftel` == "All_Pilot_Sites_20201104") %>%
  select(-`File for B_Shaftel`) 

sitenames <- bind_cols(sitenames.1,sitenames.2)

# join nps.data.2 to assigned site names 
nps.data.2 <- left_join(nps.data.2,sitenames,by = "SiteID") %>%
  filter(!is.na(Temperature))

# join "pilot data" with other single-logger-per-site data
nps.data <- bind_rows(nps.data.1,nps.data.2)

# assign AgencyID and waterbody name, and create standard column names as described in "Project_notes.Rmd"

nps.data <- nps.data %>%
  transform(DateTime = ymd_hms(DateTime)) %>%
  filter(!is.na(DateTime)) %>%
  mutate(sampleDate = date(DateTime),
         sampleTime = hms::as_hms(DateTime)) %>%
  select(-DateTime)

# remove extraneous objects
rm(nps.data.1,nps.data.2,sitenames,sitenames.1,sitenames.2)
```

<br>

Read in multiple-logger-per-site datasets (buoys @ lake centers)
```{r}
# read in all sites that ARE multi-level lake logger sites 

# Notes
# - There is not a consistent number of depth levels (ranges from ~ 5 - 10 depths)
# From "Site_Info.xlsx": "Loggers are attached to an anchored floating line (i.e., a "temperature array") at [various] depths year round... a surface (0-1m) logger is attached during the open water season.

## example for reading in one single file read-in & format
#nps.lakes <- tibble()
#filepath <- "data/NPS Bartz/Multilevel_Lake_Sites/KIJIL_01_temp_20201104.csv"
#file <- read.csv(filepath, skip = 3) %>%
#  filter(X != "Timestamp (UTC-08:00)") 
#colnames(file) <- sub("Water.Temp\\.", "Depth", colnames(file))
#file <- file %>%
#  rename(DateTime = X) %>%
#  pivot_longer(!DateTime, names_to = "Depth", values_to = "Temperature")
#file <- file %>% mutate(SiteID = basename(filepath))
#nps.lakes <- bind_rows(nps.lakes,file) 


## read in and bind multiple files using a for loop
# create list of file paths
all_paths <- list.files("data/NPS Bartz/Multilevel_Lake_Sites",
                        full.names = T)

# iterate over all files in folder
nps.lakes <- tibble()
for(i in all_paths){
file <- read.csv(i, skip = 3) %>%
  filter(X != "Timestamp (UTC-08:00)") 
colnames(file) <- sub("Water.Temp\\.", "Depth", colnames(file))
file <- file %>%
  rename(DateTime = X) %>%
  pivot_longer(!DateTime, names_to = "Depth", values_to = "Temperature") %>%
  # remove rows with missing observations (e.g., surface during frozen-surface season)
  filter(Temperature != "")
file <- file %>% mutate(`File for B_Shaftel` = basename(i)) %>%
  mutate(`File for B_Shaftel` = gsub(".csv", "", `File for B_Shaftel`))
nps.lakes <- bind_rows(nps.lakes,file) 
}
rm(file, all_paths)


# assign AgencyID and waterbody name, and create standard column names as described in "Project_notes.Rmd"
nps.lakes <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  right_join(nps.lakes) %>%
  transform(DateTime = ymd_hms(DateTime),
            Temperature = as.numeric(Temperature)) %>%
  mutate(sampleDate = date(DateTime),
         sampleTime = hms::as_hms(DateTime)) %>%
  select(-`File.for.B_Shaftel`,-DateTime)
  
# Note: to reiterate; neither "Waterbody_Name" nor "Agency_ID" alone contain unique identifiers for each site.  In combination however, Waterbody_Name + Agency_ID is a unique identifier.

```

<br>

Create a quick visualization and summary table to see extent and form of original data.

* Summary table, streams & beaches
```{r}
# create data summary table
nps.data.summary <- nps.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

# render table
nps.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```
We can see there is at least some air exposure in this dataset (minTemp ~ -32C, maxTemp ~36C)

<br>


Summary table, lake buoys
```{r}
# create data summary table
nps.lakes.summary <- nps.lakes %>%
  mutate(year = year(sampleDate)) %>%
  group_by(Agency_ID,Waterbody_Name,year,Depth) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

# render table
nps.lakes.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```
The lake buoys dataset does not immediately appear to have any air exposure data, which makes sense since the loggers were all suspended at various depth profiles!

<br>

Visualize streams & beaches data
```{r}
nps.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Streams & Beaches, Original Logger Data by Site and Year - Daily Mean")

```

<br>

The above visualization indicates there is clearly some air exposure data in our dataset.  We will use ggplotly to examine suspect datasets one at a time, and manually generate a table of erroneous data to be excised.

<br>

Process to identify and remove erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

```{r}

# list of suspect sites to manually examine w/ ggplotly 
suspect.nps.sites <-data.frame(c("KATM_lbrooo_lvl_Lake Brooks",
                                 "KATM_lbrooo_temp_Lake Brooks",
                                 "KATM_naknlo_lvl_Naknek Lake",
                                 "KATM_naknlo_temp_Naknek Lake",
                                 "LACL_kijilo_lvl_Kijik Lake",
                                 "LACL_lclaro_lvl_Lake Clark",
                                 "Port_Alsworth_Water",
                                 "Savonoski_Water"))
colnames(suspect.nps.sites) <- "SiteID"

# specify year or years to visualize
y <- c("2019")

# create ggplotly chart
ggplotly(
  p <- nps.data %>%
  inner_join(suspect.nps.sites) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Savonoski_Water"
         # put hash tag at next line to see all years for the site
         ,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )

```

If further data is is identified to be flagged, edit the csv file at "data/flagged_data/NPS_flagged_data.csv"

<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
nps_flagged_data <- read.csv("data/flagged_data/NPS_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
nps_flagged_data <- nps.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(nps_flagged_data,by = c("SiteID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year,-day)

# remove 
nps_nonflagged_data <- anti_join(nps.data,nps_flagged_data,by = c("SiteID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
nps.data <- bind_rows(nps_flagged_data,nps_nonflagged_data)

# remove extraneous objects
rm(p,nps_flagged_data,nps_nonflagged_data)

# data to be excised is now designated with "useData = 0"

```


Save output data
```{r}

# output lake data file seperately, since it has an additional column for "depth" that all the other data does not

# write streams & beaches csv


# write lakes csv


```



* Among sites and years, various types of loggers are employed (sonde, level logger, HOBO, etc)


