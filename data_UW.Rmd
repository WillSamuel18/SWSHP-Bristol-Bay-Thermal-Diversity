---
title: "UW Water Temp Data Pre-2017"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```

<br>

Read in data from folder "Jackie Carter UW"
```{r}
uw.data <-list.files(path = paste0("data/Jackie Carter UW/Jackie Carter/"),
               pattern = "*.csv", 
               full.names = T) %>%
# include file name read in for later use in resolving site name conflicts
   map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
            mutate(data_SiteID = gsub(".csv","",basename(x))))

```

<br>

Notes on site name consistency:

* Once we have UW metadata, we'll need to examine the Lynx creek site locations carefully.  Naming convention(s) may not have been consistent across years.  E.g.:
```{r}
(lynxCrkNames <- uw.data %>%
  dplyr::filter(grepl("Lynx",StationName)) %>%
  select(StationName) %>%
  distinct() %>%
  rename(SiteID = StationName))
```

<br>

Notes on data consistency: 

* Date format for sites "Aleknagik Big Whitefish Creek" and "Aleknagik Little Whitefish Creek" is in format "01-Apr-12", which lubridate will not parse.  

* Date and time formats for part of the "Aleknagik Wood River" file for 2009 have some errors and/or inconsistencies.  Some of the cells in the time column begin with ":". Also, some cells in the date column end with a single digit, and the year is sometimes formatted with either 2 digits or 4 digits.  Some of these features are likely artifacts of some previous data sorting/cleaning process.

* Need to treat import and prep of these three data sets separately.


<br>

Separate specified data sets discussed above
```{r}
# define function to exclude multiple strings in a column
'%ni%' <- Negate('%in%')

# specify datasets to be excluded (all years)
exclude_wf <- c("Aleknagik Big Whitefish Creek","Aleknagik Little Whitefish Creek")

# specify datasets to be excluded (one watershed, one year)
StationName <- "Aleknagik Wood River"
YearSampled <- "2009"
exclude_wr <- data.frame(StationName,YearSampled)

# apply exclusions
uw.data <- uw.data %>%
  anti_join(exclude_wr) %>%
  filter(StationName %ni% exclude_wf)

```

<br>

Rename, transform, and eliminate columns to match final field names described in Project_notes.Rmd
```{r}
uw.data <- uw.data %>%
  rename(SiteID = StationName,
         sampleDate = Date,
         sampleTime = Time) %>%
  select(SiteID,sampleDate,sampleTime,Temperature,data_SiteID) %>%
  transform(sampleDate = as.Date(mdy(sampleDate)),
            sampleTime = hms::as.hms(sampleTime),
            Temperature = as.numeric(Temperature)) 

```

<br>

Perform separate import & prep process for the three datasets previously described

* Import Aleknagik wood river 2009
```{r}
# Aleknagik wood river 2009
uw.data.1 <-list.files(path = paste0("data/Jackie Carter UW/Jackie Carter/"),
               pattern = "*.csv", 
               full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
            mutate(data_SiteID = gsub(".csv","",basename(x)))) %>%
  # retain only wood river 2009 data
  inner_join(exclude_wr) %>%
  rename(SiteID = StationName,
         sampleDate = Date,
         sampleTime = Time) %>%
  select(SiteID,sampleDate,sampleTime,Temperature,data_SiteID) %>%
  
  ## fix date column
  # remove extraneous digits from date column by taking it apart and putting it back together 
  separate(sampleDate, sep = "/", into = c("month","day","year")) %>%
  separate(year, sep = " ", into = "year") %>%
  # we know these observations are from 2009 from the original YearSampled column, thus:
  mutate(year = "2009",
         sampleDate = ymd(paste0(year,"-",month,"-",day))) %>%
  select(-month,-day,-year) %>%
  transform(Temperature = as.numeric(Temperature))

  ## fix time column
  ### for dates 9/1/2009 - 9/3/2009, there is some potentially confusing data.  Based on looking at the original data, here is my best guess for what happened: 9 observations were made at appx. 14:00 on 9/1/2009 and 9/2/2009, and 4 observations at appx. 14:00 on 9/3/2009.  Plan of action: average values on a daily basis and assign a single temperature value to time 14:00 for each of those days.

### specify erroneous dates
erroneous_dates <- data.frame(c("2009-09-01","2009-09-02","2009-09-03"))
colnames(erroneous_dates) <- "sampleDate"
erroneous_dates <- erroneous_dates %>%transform(sampleDate = as.Date(sampleDate))

### create corrected data subset for 9/1/2009 - 9/3/2009
uw.data.2 <- uw.data.1 %>%
  inner_join(erroneous_dates) %>%
  transform(Temperature = as.numeric(Temperature)) %>%
  group_by(sampleDate) %>%
  summarise(Temperature = mean(Temperature)) %>%
  mutate(SiteID = "Aleknagik Wood River",
         sampleTime = "14:00:00")

### remove erroneous original data from Aleknagik wood river 2009 data
uw.data.1 <- uw.data.1 %>% anti_join(erroneous_dates)

### rejoin corrected subset to Aleknagik wood river 2009 data
uw.data.1 <- bind_rows(uw.data.1,uw.data.2)
  
# ensure column classes are identical to overall UW dataset
uw.data.1 <- uw.data.1 %>%
  transform(sampleTime = hms::as.hms(sampleTime),
            Temperature = as.numeric(Temperature)) 

# rejoin Aleknagik wood river 2009 data to overall UW dataset
uw.data <- bind_rows(uw.data,uw.data.1)

```

<br>

* Import Aleknagik Big Whitefish Creek and Aleknagik Little Whitefish Creek
```{r}
# Aleknagik Big Whitefish Creek and Aleknagik Little Whitefish Creek
uw.data.1 <-list.files(path = paste0("data/Jackie Carter UW/Jackie Carter/"),
               pattern = "*.csv", 
               full.names = T) %>% 
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
            mutate(data_SiteID = gsub(".csv","",basename(x)))) %>%
  # retain only data from "Aleknagik Big Whitefish Creek" and "Aleknagik Little Whitefish Creek"
  filter(StationName %in% exclude_wf) %>%
  rename(SiteID = StationName,
         sampleDate = Date,
         sampleTime = Time) %>%
  select(SiteID,sampleDate,sampleTime,Temperature,data_SiteID) 


## fix date column
## months are in month abbreviation format.  need to convert to digits    
## create dataframe matching months to numeric digits
month <- month.abb
month.num <- 1:12
month.conv <- data.frame(month,month.num)

# make date format parse-able by lubridate by taking it apart and putting it back together 
uw.data.1 <- uw.data.1 %>% 
  # convert month format from abbrevaition to numeric
  separate(sampleDate, sep = "-", into = c("day","month","year"), remove = F) %>%
  left_join(month.conv) %>%
  select(-month) %>%
  rename(month = month.num) %>%
  # convert year from two digits to four
  mutate(year = paste0(20,year)) %>%
  # create correctly formatted date column
  mutate(sampleDate = ymd(paste0(year,"-",month,"-",day))) %>%
  select(-year,-month,-day) %>%
  transform(Temperature = as.numeric(Temperature),
            sampleTime = hms::as.hms(sampleTime))

# rejoin Aleknagik Big Whitefish Creek and Aleknagik Little Whitefish Creek back to overall UW dataset
uw.data <- bind_rows(uw.data,uw.data.1)

```

<br>

Associate and AKOATS_ID with each site where possible

* Naming Conflict Resolution Tasks
- Resolve Lynx Creek site names conflicts (awaiting email from J Carter as of 11/16/20)
- Use sheet from R Shaftel "uw sites_JC_rss.xlsx" to resolve most other conflicts except Lynx Creek
- See notes column in UW metadata for suggestions on how to resolve these conflicts

```{r}
# read in UW metadata resolutions emailed from R Shaftel on 11/13/20
uw.metadata <- read_excel("data/Jackie Carter UW/uw sites_JC_rss.xlsx", sheet = "Sheet1") %>%
  # We will resolve Lynx Creek site names separately, so exclude at this step
  filter(data_SiteID != "UW-FRI_LynxCreekALL_Temps") %>%
  # use only one AKOATS_ID for sites with multiple entries
  separate(AKOATS_ID, sep = " and", into = "AKOATS_ID") %>%
  select(data_SiteID,AKOATS_ID)

# join AKOATS_IDs to UW water temp data.  Make sure to use the "data_SiteID" column as a join key
 uw.data <- left_join(uw.data,uw.metadata,by = "data_SiteID")

# there will be some further resolution of UW metadata and SiteIDs in "metadata_AllSites.Rmd", as some UW sites not have assigned AKOATS_ID values


## Lynx creek naming conflicts - to do

# Notes from Jackie C: "Daniel puts multiple temp loggers in Lynx Creek.  There is a lake at the top that feeds warm water into the stream, so he puts one right below that (Lynx Lake Trib, Lake Trib, etc).  There is also a cold tribuary about 400-500m below the lake that feeds in significantly colder water to the main channel, like 5-8C (Cold Trib, Lynx Cold, etc).  Then there is the main channel, and the temp logger is near the bottom to give an integrated temp (Lynx, Lynx lower, Lynx bottom).  As I was importing 2020 data, I realized that we have way too many names for the same location over the years.  I'm not sure if the AKOATS team just combined all the Lynx temps to average or what the decisions were to get to LynxCreekALL_temps"

# will send email to R Shaftel on 11/16/20 to pass on Lynx Creek resolution questions to J Carter.  See file "data/Jackie Carter UW/UW_sitename_conflicts_resolved.xlsx" in progress.



```


<br>


Perform a quick visualization and summary table to see extent and form of original data.

* Summary table

```{r}
# create data summary table
uw.data.summary <- uw.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

uw.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)
    
```

<br>

* Visualization

```{r}
uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Original Logger Data by Site and Year - Daily Mean")

```

<br>

Notes:

* Among the whole data set, there are some max temps > 50 C, indicating some logger malfunction and/or air exposure.  There is also a minTemp of -21.5 C

* Different types of instruments were employed to collect UW water temp data over the years, including iButtons, Stage gauges, Tidbits, and Level loggers.

* Note 11/9/20: one file is "LynxCreekALL", is the site the same or did the naming convention change?

<br>

***

Identify and eliminate likely erroneous data 

* Step 1: Start by visually examining datasets that include observations >25 C.  Does anything about these datasets suggest exposure or malfunction? (Or temperatures recorded pre/post deployment in water?)


<br>

Plot 1 
```{r}
# remove extraneous temporary objects from previous steps
rm(erroneous_dates, exclude_wr,exclude_wf,month.conv,uw.data.1,uw.data.2,lynxCrkNames)

# identify datasets that include observations >25C
above.25 <- uw.data %>%
  filter(Temperature >= 25) %>%
  mutate(year = year(sampleDate)) %>%
  select(SiteID,year) %>%
  distinct()

# plot reduced dataset of site/years that contain >25C observations -- daily means
uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data by Site and Year - Datasets w/ >25 C")

```
<br>

Process to identify and remove erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

1.) Examine & remove bad data from data sets w/ observations >25C
```{r}
# list of watersheds w/ 25C observations to examine w/ ggplotly object
l <- above.25 %>%
  select(-year) %>%
  distinct()

y <- c("2012","2016")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Nerka Lynx Creek"
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data, Lynx Creek (Example)")
  )
```

<br>

2.) Examine & remove bad data from data sets w/ known suspect data

Notes, from "data/Jackie Carter UW/UA email saved at "data/Jackie Carter UW/UA Mail - Bristol Bay Temperature Data.pdf": "I have noticed some weird stuff going on with some of the older data. I think it may be an artifact of how our former database manager imported the data (bulk import, no distinction between logger type so there are multiple readings per hour but they aren't really as similar as one might expect). The known affected locations and years are: Aleknagik Bear Creek 2010-2013, Nerka Fenno Creek 2010-2013, and Nerka Pick Creek 2006-2012."

Examine the three data sets specified above
```{r}
# potentially erroneous data sets:
# Nerka Fenno Creek 2010-2013
# Nerka Pick Creek 2006-2012
# Aleknagik Bear Creek 2010-2013 --> this one not found anywhere in our dataset:
alk <- uw.data %>%
  dplyr::filter(grepl("Bear",SiteID)) %>%
  select(SiteID) %>%
  distinct()

# list of watersheds to examine w/ ggplotly object
l <- c("Nerka Fenno Creek","Nerka Pick Creek")
z <- uw.data %>%
  filter(SiteID %in% l) %>%
  select(SiteID) %>%
  distinct()

# specify year or years to display here
y <- c("2006")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(z) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Nerka Seventh Creek"
         # hashtag line below if want to see all years for a site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )

```

<br>

3.) Examine & remove bad data from all other datasets (e.g. exposure data)

```{r}

# list of all watersheds to examine w/ ggplotly object
# do a quick manual run-through this list one at a time to spot visually apparent logger exposure(s)
l <- data.frame(unique(uw.data$SiteID))

# specify year or years to display here
y <- c("2015")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Aleknagik Little Whitefish Creek"
         # hashtag line below if want to see all years for a site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )


# note: creating a Shiny app for this job would be really useful!  e.g., to be able to easily toggle between various sites and years
```


If further data is is identified to be flagged, edit the csv file at "data/flagged_data/UW_flagged_data.csv"


<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
uw_flagged_data <- read.csv("data/flagged_data/UW_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
uw_flagged_data <- uw.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(uw_flagged_data,by = c("SiteID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year,-day)

# remove 
uw_nonflagged_data <- anti_join(uw.data,uw_flagged_data,by = c("SiteID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
uw.data <- bind_rows(uw_flagged_data,uw_nonflagged_data)

# remove extraneous objects
rm(above.25,l,p,uw_flagged_data,uw_nonflagged_data,alk)

# how much data is being flagged?
uw.data %>%
  group_by(useData) %>%
  summarise(ct = n())

```


<br>

To do: try using functions in "Temp_flags_function.R" to IDeven more bad data.

<br>


11/10/20: Ready to output file as soon as we get metadata!

Export csv of combined datasets, with newly identified "do not use data" signified with a "0" in the useData column, to data folder associated with this .Rproj
```{r}
# use "data_SiteID" terms for SiteIDs
 uw.data <- uw.data %>%
   select(-SiteID) %>%
   rename(SiteID = data_SiteID)

# re-export when lynx creek site names are resolved

# reorder columns
x <- uw.data %>% select(AKOATS_ID,SiteID,sampleDate,sampleTime,Temperature,useData)

# export csv
write.csv(x,"output/UW.csv", row.names = F)

```

