---
title: "UW Water Temp Data Pre-2017"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```

<br>

Read in data from folder "Jackie Carter UW"
```{r}

# most data pre-2017
uw.data <-list.files(path = paste0("data/Jackie Carter UW/Jackie Carter/"),
               pattern = "*.csv", 
               full.names = T) %>%
# include file name read in for later use in resolving site name conflicts
   map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
            mutate(data_SiteID = gsub(".csv","",basename(x)))) %>%
    select(-X9,-SampleType,-MeasurementType,-Lake,-YearSampled) %>%
  # exclude Lynx creek sites, as corrected versions were provided separately in an email on 11/17/20
  dplyr::filter(!grepl("Lynx",StationName)) %>%
  rename(sampleDate = Date,
         sampleTime = Time)

# separately-provided data pre-2017 (emailed on 11/17/20)
## lynx creek sites
uw.lynx <- read_csv("data/Jackie Carter UW/Jackie Carter - 2020 data request/UW-FRI_LynxCreekALL_Temps_JC.csv", col_types = cols(.default = "c")) %>%
  mutate(data_SiteID = "UW-FRI_LynxCreekALL_Temps_JC") %>%
  transform(Date = as.Date(mdy(Date))) %>%
  select(-SampleType,-MeasurementType) %>%
  mutate_all(as.character)

## aleknagik bear creek site (one site)
uw.aleknagik.bear <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/froRSS_AleknagikBearTemps_2008-2017.xlsx") %>%
  mutate(data_SiteID = "") %>%
  # fix time format
  separate(Time,sep = " ", into = c("yr","Time")) %>%
  transform(Time = hms::as_hms(Time),
            Date = as.Date(Date)) %>%
  select(-yr) %>%
  mutate_all(as.character)

# data post-2017 (44 sites)
uw.post2017 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "2018-2020 data") %>%
  filter(!is.na(Temperature)) %>%
  select(-MeasurementType,-SampleType) %>%
  # fix time format
  separate(Time,sep = " ", into = c("yr","Time")) %>%
  transform(Time = hms::as_hms(Time),
            Date = as.Date(Date)) %>%
  select(-yr) %>%
  mutate(data_SiteID = "") %>%
  mutate_all(as.character) 

```

<br>

Join the multiple data frames in to single object
```{r}
# join data tables that are ready
uw.data.2 <- bind_rows(uw.lynx,uw.aleknagik.bear,uw.post2017) %>%
  # prep and rename
  rename(SiteID = StationName,
         sampleDate = Date,
         sampleTime = Time) %>%
  transform(sampleDate = ymd(sampleDate)) %>%
  select(SiteID,sampleDate,sampleTime,Temperature,data_SiteID) 

# remove extraneous objects
rm(uw.lynx,uw.aleknagik.bear,uw.post2017)
```

<br>

Notes on data format consistency: 

* The data frame "uw.data needs some special treatment and prep before joining with the other data due to various format inconsistencies

* Date format for sites "Aleknagik Big Whitefish Creek" and "Aleknagik Little Whitefish Creek" is in format "01-Apr-12", which lubridate will not parse.  

* Date and time formats for part of the "Aleknagik Wood River" file for 2009 have some errors and/or inconsistencies.  Some of the cells in the time column begin with ":". Also, some cells in the date column end with a single digit 1-10, and the year is sometimes formatted with either 2 digits or 4 digits.  Some of these features are likely artifacts of some previous data sorting/cleaning process.


<br>

Separate specified data sets discussed above
```{r}
# define function to exclude multiple strings in a column
'%ni%' <- Negate('%in%')

# specify datasets to be excluded (all years)
exclude_wf <- c("Aleknagik Big Whitefish Creek","Aleknagik Little Whitefish Creek")

# specify datasets to be excluded (one watershed, one year)
StationName <- "Aleknagik Wood River"
YearSampled <- "2009"
exclude_wr <- data.frame(StationName,YearSampled)

# apply exclusions
uw.data <- uw.data %>%
  anti_join(exclude_wr) %>%
  filter(StationName %ni% exclude_wf) %>% 
  # format dates
  mutate(sampleDate = mdy(sampleDate))

```


<br>

Perform separate import & prep process for the three datasets previously described

* Import and correct Aleknagik wood river 2009 data
```{r}
# Aleknagik wood river 2009
uw.data.x <-list.files(path = paste0("data/Jackie Carter UW/Jackie Carter/"),
               pattern = "*.csv", 
               full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
            mutate(data_SiteID = gsub(".csv","",basename(x)))) %>%
  # retain only wood river 2009 data
  inner_join(exclude_wr) %>%
  rename(sampleDate = Date,
         sampleTime = Time) %>%
  select(StationName,sampleDate,sampleTime,Temperature,data_SiteID) %>%
  
  ## fix date column
  # remove extraneous digits from date column by taking it apart and putting it back together 
  separate(sampleDate, sep = "/", into = c("month","day","year")) %>%
  separate(year, sep = " ", into = "year") %>%
  # we know these observations are from 2009 from the original YearSampled column, thus:
  mutate(year = "2009",
         sampleDate = ymd(paste0(year,"-",month,"-",day))) %>%
  select(-month,-day,-year) %>%
  transform(Temperature = as.numeric(Temperature),
            sampleDate = as.character(sampleDate))

  ## fix time column
  ### for dates 9/1/2009 - 9/3/2009, there is some potentially confusing data.  Based on looking at the original data, here is my best guess for what happened: 9 observations were made at appx. 14:00 on 9/1/2009 and 9/2/2009, and 4 observations at appx. 14:00 on 9/3/2009.  Plan of action: average values on a daily basis and assign a single temperature value to time 14:00 for each of those days.

### specify erroneous dates
erroneous_dates <- data.frame(c("2009-09-01","2009-09-02","2009-09-03"))
colnames(erroneous_dates) <- "sampleDate"
erroneous_dates <- erroneous_dates %>% transform(sampleDate = as.character(sampleDate))

### create corrected data subset for 9/1/2009 - 9/3/2009
uw.data.y <- uw.data.x %>%
  inner_join(erroneous_dates) %>%
  transform(Temperature = as.numeric(Temperature)) %>%
  group_by(sampleDate) %>%
  summarise(Temperature = mean(Temperature)) %>%
  mutate(StationName = "Aleknagik Wood River",
         sampleTime = "14:00:00")

### remove erroneous original data from Aleknagik wood river 2009 data
uw.data.x <- uw.data.x %>% anti_join(erroneous_dates)

### rejoin corrected subset to Aleknagik wood river 2009 data
uw.data.x <- bind_rows(uw.data.x,uw.data.y)
  
# ensure column classes are identical to overall UW dataset
uw.data.x <- uw.data.x %>%
  transform(sampleTime = hms::as_hms(sampleTime),
            Temperature = as.character(Temperature),
            sampleDate = ymd(sampleDate)) 

# rejoin Aleknagik wood river 2009 data to overall UW dataset
uw.data <- bind_rows(uw.data,uw.data.x)

rm(uw.data.x,uw.data.y,erroneous_dates,exclude_wr)
```

<br>

* Import Aleknagik Big Whitefish Creek and Aleknagik Little Whitefish Creek
```{r}
# Aleknagik Big Whitefish Creek and Aleknagik Little Whitefish Creek
uw.data.x <-list.files(path = paste0("data/Jackie Carter UW/Jackie Carter/"),
               pattern = "*.csv", 
               full.names = T) %>% 
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
            mutate(data_SiteID = gsub(".csv","",basename(x)))) %>%
  # retain only data from "Aleknagik Big Whitefish Creek" and "Aleknagik Little Whitefish Creek"
  filter(StationName %in% exclude_wf) %>%
  rename(SiteID = StationName,
         sampleDate = Date,
         sampleTime = Time) %>%
  select(SiteID,sampleDate,sampleTime,Temperature,data_SiteID) 


## fix date column
## months are in month abbreviation format.  need to convert to digits    
## create dataframe matching months to numeric digits
month <- month.abb
month.num <- 1:12
month.conv <- data.frame(month,month.num)

# make date format parse-able by lubridate by taking it apart and putting it back together 
uw.data.x <- uw.data.x %>% 
  # convert month format from abbrevaition to numeric
  separate(sampleDate, sep = "-", into = c("day","month","year"), remove = F) %>%
  left_join(month.conv) %>%
  select(-month) %>%
  rename(month = month.num) %>%
  # convert year from two digits to four
  mutate(year = paste0(20,year)) %>%
  # create correctly formatted date column
  mutate(sampleDate = ymd(paste0(year,"-",month,"-",day))) %>%
  select(-year,-month,-day) %>%
  transform(sampleTime = hms::as.hms(sampleTime)) %>%
  mutate_all(as.character) %>%
  transform(sampleDate = ymd(sampleDate))

# prep uw.data to join rest of data
uw.data <- uw.data %>%
  rename(SiteID = StationName) 

# rejoin Aleknagik Big Whitefish Creek and Aleknagik Little Whitefish Creek back to overall UW dataset
uw.data <- bind_rows(uw.data,uw.data.x)

rm(uw.data.x,month.conv)

```

<br>

Join and format all data frames read-in
```{r}

uw.data <- bind_rows(uw.data,uw.data.2) %>%
  transform(sampleDate = as.Date(sampleDate),
            sampleTime = hms::as_hms(sampleTime),
            Temperature = as.numeric(Temperature)) 

rm(uw.data.2)

```



<br>

Associate and AKOATS_ID with each site where possible

* Naming Conflict Resolution Tasks

- Use sheet from R Shaftel "uw_metadata_thru_2017.xlsx" emailed from J Carter as of 11/17/20)
- See notes column in UW metadata for suggestions on how to resolve these conflicts

```{r}
# remove sites specified not be used in metadata notes
sites_remove <- c("Nerka Lynx Creek Beach","Nerka Lynx Creek Middle")
# apply exclusions
uw.data <- uw.data %>%
  filter(SiteID %ni% sites_remove) 

# rename "Nerka Lynx Creek Upper" as "Nerka Lynx Lake Tributary" as specified in metadata notes
uw.data <- uw.data %>%
  mutate(SiteID = str_replace(SiteID,"Nerka Lynx Creek Upper","Nerka Lynx Lake Tributary"))


# read in UW metadata resolutions emailed from R Shaftel on 11/13/20
uw.metadata <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  # filter out unresolved sites
  filter(AKOATS_ID != "?",
         AKOATS_ID != "NA") %>%
  # use only one AKOATS_ID for sites with multiple entries
  separate(AKOATS_ID, sep = " and", into = "AKOATS_ID") %>%
  separate(AKOATS_ID, sep = " or", into = "AKOATS_ID") %>%
  select(StationName,AKOATS_ID) %>%
  rename(SiteID = StationName) 

# join AKOATS IDs
uw.data <- left_join(uw.data,uw.metadata, by = "SiteID")
 
# there will later be some further resolution of UW metadata and SiteIDs in "metadata_AllSites.Rmd", as some UW sites not have assigned AKOATS_ID values

```


<br>


Perform a quick visualization and summary table to see extent and form of original data.

* Summary table

```{r}
# create data summary table
uw.data.summary <- uw.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

uw.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)
    
```

<br>

* Visualization

```{r}
uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Original Logger Data by Site and Year - Daily Mean")

```

<br>

Notes:

* Among the whole data set, there are some max temps > 50 C, indicating some logger malfunction and/or air exposure.  There is also a minTemp of -21.5 C

* Different types of instruments were employed to collect UW water temp data over the years, including iButtons, Stage gauges, Tidbits, and Level loggers.

<br>

***

Identify and eliminate likely erroneous data 

* Step 1: Start by visually examining datasets that include observations >25 C.  Does anything about these datasets suggest exposure or malfunction? (Or temperatures recorded pre/post deployment in water?)


<br>

Plot 1 
```{r}
# remove extraneous temporary objects from previous steps
rm(uw.data.summary,uw.metadata)

# identify datasets that include observations >25C
above.25 <- uw.data %>%
  filter(Temperature >= 25) %>%
  mutate(year = year(sampleDate)) %>%
  select(SiteID,year) %>%
  distinct()

# plot reduced dataset of site/years that contain >25C observations -- daily means
uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data by Site and Year - Datasets w/ >25 C")

```
<br>

Process to identify and remove erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

1.) Examine & remove bad data from data sets w/ observations >25C
```{r}
# list of watersheds w/ 25C observations to examine w/ ggplotly object
l <- above.25 %>%
  select(-year) %>%
  distinct()

y <- c("2020")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Nerka Agulukpak River"
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data (Example)")
  )
```

<br>

2.) Examine & remove bad data from data sets w/ known suspect data

Notes, from "data/Jackie Carter UW/UA email saved at "data/Jackie Carter UW/UA Mail - Bristol Bay Temperature Data.pdf": "I have noticed some weird stuff going on with some of the older data. I think it may be an artifact of how our former database manager imported the data (bulk import, no distinction between logger type so there are multiple readings per hour but they aren't really as similar as one might expect). The known affected locations and years are: Aleknagik Bear Creek 2010-2013, Nerka Fenno Creek 2010-2013, and Nerka Pick Creek 2006-2012."

Examine the three data sets specified above
```{r}
# potentially erroneous data sets:
# Nerka Fenno Creek 2010-2013
# Nerka Pick Creek 2006-2012
# Aleknagik Bear Creek 2010-2013 

# list of watersheds to examine w/ ggplotly object
l <- c("Nerka Fenno Creek","Nerka Pick Creek","Aleknagik Bear Creek")
z <- uw.data %>%
  filter(SiteID %in% l) %>%
  select(SiteID) %>%
  distinct()

# specify year or years to display here
y <- c("2006")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(z) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Aleknagik Bear Creek"
         # hashtag line below if want to see all years for a site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data (Example)")
  )

```

<br>

3.) Examine & remove bad data from all other datasets (e.g. exposure data)

<br>

```{r}

# list of all watersheds to examine w/ ggplotly object
# do a quick manual run-through this list one at a time to spot visually apparent logger exposure(s)
l <- data.frame(unique(uw.data$SiteID))

# specify year or years to display here
y <- c("2020","2019","2015")
# specify site
site <- "Nerka Teal Creek"

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == site
         # hashtag line below if want to see all years for a site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )

# note: creating a Shiny app for this job would be really useful!  e.g., to be able to easily toggle between various sites and years
```


If further data is is identified to be flagged, edit the csv file at "data/flagged_data/UW_flagged_data.csv"


<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
uw_flagged_data <- read.csv("data/flagged_data/UW_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
uw_flagged_data <- uw.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(uw_flagged_data,by = c("SiteID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year,-day)

# apply flags
uw_nonflagged_data <- anti_join(uw.data,uw_flagged_data,by = c("SiteID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
uw.data <- bind_rows(uw_flagged_data,uw_nonflagged_data)

# remove extraneous objects
rm(above.25,l,p,uw_flagged_data,uw_nonflagged_data,alk)

# how much data is being flagged?
uw.data %>%
  group_by(useData) %>%
  summarise(ct = n())

# calc % of flagged data
# (11457/(1089838 +11457))*100
# 1.04% of data is flagged

```


<br>

To do: try using functions in "Temp_flags_function.R" to IDeven more bad data.

<br>


11/10/20: Ready to output file as soon as we get metadata!

Export csv of combined datasets, with newly identified "do not use data" signified with a "0" in the useData column, to data folder associated with this .Rproj
```{r}
# prep for export
 uw.data <- uw.data %>%
   select(-data_SiteID) 

# reorder columns
x <- uw.data %>% select(AKOATS_ID,SiteID,sampleDate,sampleTime,Temperature,useData)

# export csv
write.csv(x,"output/UW.csv", row.names = F)

```

