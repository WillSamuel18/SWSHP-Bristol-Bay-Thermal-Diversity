---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```

<br>

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201104.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in single-logger-per-site datasets (streams, beaches)
```{r}
# read in all sites that are not multi-level lake logger sites or pilot sites
nps.data.1 <- list.files(path = "data/NPS Bartz",
         pattern="*.csv", 
         full.names = T) %>% 
   map_df(function(x) read_csv(x,skip = 15, col_types = cols(.default = "c"),col_names = F) %>% 
            mutate(filename=gsub(".csv","",basename(x))))
colnames(nps.data.1) <- c("a","DateTime","Temperature","b","c","d","File for B_Shaftel") 
nps.data.1 <- nps.data.1 %>% select(-a,-b,-c,-d)

# assign AgencyID and waterbody name
nps.data.1  <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  right_join(nps.data.1) %>%
  select(-"File for B_Shaftel") %>%
  # make up a SiteID name for plotting
  #mutate(SiteID = paste0(Agency_ID,"_",Waterbody_Name)) %>%
  # format col type
  transform(Temperature = as.numeric(Temperature))

# read and format in "All_Pilot_Sites_20201104.csv"
nps.data.2 <- read.csv("data/NPS Bartz/Pilot_sites/All_Pilot_Sites_20201104.csv") %>%
  # modify Date format to use "-" instead of "/" so that lubridate can parse it
  separate(Date,sep = "/",into = c("m","d","y")) %>% 
  mutate(Date = paste0(y,"-",m,"-",d),
         DateTime = paste(Date,Time)) %>%
  select(-X,-Date,-Time,-m,-d,-y) %>%
  # create long format
  pivot_longer(!DateTime, names_to = "SiteID", values_to = "Temperature") %>%
  transform(Temperature = as.numeric(Temperature)) %>%
  rename(Waterbody_Name = SiteID) %>%
  filter(!is.na(Temperature))

# we want to use SiteID to associate with metadata.  However, site names (column names) in "All_Pilot_Sites_20201104.csv" do not have an exact match anywhere in the "Site_Info.xlsx" metadata file.  Thus we will manually generate a dataframe to match site names to metadata

# FYI: the two Tlikila sites are 36m apart; but should still be QA/QC'd separately, so keep separate at this stage of analysis

# format waterbody names from fws-provided metadata
nps.data.2.names <- read_excel("data/NPS Bartz/NPS_sitename_conflicts_resolved.xlsx", skip = 4, sheet = "Sheet1") %>%
   select(Agency_ID,Waterbody_Name)
  
# site names alternately use "water" or river" after site name...
nps.data.2 <- left_join(nps.data.2,nps.data.2.names,by = "Waterbody_Name")

# join "pilot data" with other single-logger-per-site data
nps.data <- bind_rows(nps.data.1,nps.data.2)

# assign AgencyID and waterbody name, and create standard column names as described in "Project_notes.Rmd"
nps.data <- nps.data %>%
  transform(DateTime = ymd_hms(DateTime)) %>%
  filter(!is.na(DateTime)) %>%
  mutate(sampleDate = date(DateTime),
         sampleTime = hms::as_hms(DateTime)) %>%
  select(-DateTime)

# remove extraneous objects
rm(nps.data.1,nps.data.2,nps.data.2.names)
```

<br>

Read in multiple-logger-per-site datasets (buoys @ lake centers)
```{r}
# read in all sites that ARE multi-level lake logger sites 

# Notes
# - There is not a consistent number of depth levels (ranges from ~ 5 - 10 depths)
# From "Site_Info.xlsx": "Loggers are attached to an anchored floating line (i.e., a "temperature array") at [various] depths year round... a surface (0-1m) logger is attached during the open water season.

## example for reading in one single file read-in & format
#nps.lakes <- tibble()
#filepath <- "data/NPS Bartz/Multilevel_Lake_Sites/KIJIL_01_temp_20201104.csv"
#file <- read.csv(filepath, skip = 3) %>%
#  filter(X != "Timestamp (UTC-08:00)") 
#colnames(file) <- sub("Water.Temp\\.", "Depth", colnames(file))
#file <- file %>%
#  rename(DateTime = X) %>%
#  pivot_longer(!DateTime, names_to = "Depth", values_to = "Temperature")
#file <- file %>% mutate(SiteID = basename(filepath))
#nps.lakes <- bind_rows(nps.lakes,file) 


## read in and bind multiple files using a for loop
# create list of file paths
all_paths <- list.files("data/NPS Bartz/Multilevel_Lake_Sites",
                        full.names = T)

# iterate over all files in folder
nps.lakes <- tibble()
for(i in all_paths){
file <- read.csv(i, skip = 3) %>%
  filter(X != "Timestamp (UTC-08:00)") 
colnames(file) <- sub("Water.Temp\\.", "Depth", colnames(file))
file <- file %>%
  rename(DateTime = X) %>%
  pivot_longer(!DateTime, names_to = "Depth", values_to = "Temperature") %>%
  # remove rows with missing observations (e.g., surface during frozen-surface season)
  filter(Temperature != "")
file <- file %>% mutate(`File for B_Shaftel` = basename(i)) %>%
  mutate(`File for B_Shaftel` = gsub(".csv", "", `File for B_Shaftel`))
nps.lakes <- bind_rows(nps.lakes,file) 
}
rm(file, all_paths, i)


# assign AgencyID and waterbody name, and create standard column names as described in "Project_notes.Rmd"
nps.lakes <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  right_join(nps.lakes) %>%
  transform(DateTime = ymd_hms(DateTime),
            Temperature = as.numeric(Temperature)) %>%
  mutate(sampleDate = date(DateTime),
         sampleTime = hms::as_hms(DateTime)) %>%
  select(-`File.for.B_Shaftel`,-DateTime) %>%
  # make depth a numeric value
  mutate(Depth = gsub("Depth","",Depth),
         Depth = as.numeric(gsub("m","",Depth))) 

```

<br>

Create a quick visualization and summary table to see extent and form of original data.

* Summary table, streams & beaches
```{r}
# create data summary table
nps.data.summary <- nps.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(Agency_ID,Waterbody_Name,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

# render table
nps.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```
We can see there is at least some air exposure in this dataset (minTemp ~ -32C, maxTemp ~36C)

<br>


Summary table, lake buoys
```{r}
# create data summary table
nps.lakes.summary <- nps.lakes %>%
  mutate(year = year(sampleDate)) %>%
  group_by(Agency_ID,Waterbody_Name,year,Depth) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

# render table
nps.lakes.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```
The lake buoys dataset does not immediately appear to have any air exposure data, which makes sense since the loggers were all suspended at various depth profiles!

<br>

Visualize streams & beaches data
```{r}
nps.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Streams & Beaches, Original Logger Data by Site and Year - Daily Mean")

```

<br>

The above visualization indicates there is clearly some air exposure data in our streams & beaches dataset.  We will use ggplotly to examine suspect datasets one at a time, and manually generate a table of erroneous data to be excised.

<br>

Process to identify and remove streams & beaches erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

```{r}

# list of suspect sites to manually examine w/ ggplotly 
suspect.nps.sites <-data.frame(c("KATM_lbrooo_lvl",
                                 "KATM_lbrooo_temp",
                                 "KATM_naknlo_lvl",
                                 "KATM_naknlo_temp",
                                 "LACL_kijilo_lvl",
                                 "LACL_lclaro_lvl",
                                 "LACL_porta_beach_water",
                                 "KATM_savor_stream_water",
                                 "KATM_naknlo_continuous_wq",
                                 "KATM_margc_stream_water",
                                 "LACL_tlikr_stream_sediment"))
colnames(suspect.nps.sites) <- "Agency_ID"

# specify year or years to visualize
y <- c("2019")

# create ggplotly chart
ggplotly(
  p <- nps.data %>%
  inner_join(suspect.nps.sites) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(Agency_ID == "KATM_lbrooo_temp"
         # put hash tag at next line to see all years for the site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Daily Mean Data")
  )

```

If further data is is identified to be flagged, edit the csv file at "data/flagged_data/NPS_flagged_data.csv"

<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
nps_flagged_data <- read.csv("data/flagged_data/NPS_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
nps_flagged_data <- nps.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(nps_flagged_data,by = c("Agency_ID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year
         #,-day
         )

# remove 
nps_nonflagged_data <- anti_join(nps.data,nps_flagged_data,by = c("Agency_ID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
nps.data <- bind_rows(nps_flagged_data,nps_nonflagged_data)

# remove extraneous objects
rm(p,nps_flagged_data,nps_nonflagged_data,suspect.nps.sites)

# data to be excised is now designated with "useData = 0"

```

<br>

Re-Visualize cleaned-up streams & beaches data
```{r}
nps.data %>%
  # filter out flagged data
  filter(useData == 1) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Streams & Beaches, Corrected Logger Data by Site and Year - Daily Mean")

```

<br>

Lake profile data visualization
```{r}
nps.lakes %>%
  group_by(Waterbody_Name,Depth,sampleDate) %>%
  summarise(daily_mean_temp = mean(Temperature)) %>%
  mutate(day = yday(sampleDate),
         year = year(sampleDate)) %>%
  ggplot(aes(daily_mean_temp, Depth, color = as.factor(year))) +
  geom_point(position = position_dodge(width = 1.2)) +
  scale_y_reverse() +
  facet_grid(Waterbody_Name ~ ., scales = "free_y") +
  # this next line is not working; why?
  theme(strip.text = element_text(angle = 90))
  
  #scale_y_continuous(breaks = c(122,152,182,213,244,274,304),
   #                  labels = c("May","June","July","Aug","Sept", "Oct","Nov")) 

```

<br>

Assign AKOATS_IDs to observations
```{r}

# working here 11/17/20 1:50 PM
akoats.meta <- read_excel("data/AKOATS_DATA_2020_working.xlsx") %>%
  rename(AKOATS_ID = seq_id) %>%
  select(AKOATS_ID,Agency_ID)

# streams & beaches
nps.data <- left_join(nps.data,akoats.meta)

# lakes
nps.lakes <- left_join(nps.lakes,akoats.meta)

```


<br>

Save output data
```{r}

# output lake data file seperately, since it has an additional column for "depth" that all the other data does not

# write streams & beaches csv
## reorder columns
x <- nps.data %>% 
  select(AKOATS_ID,Agency_ID,sampleDate,sampleTime,Temperature,useData)
# export csv
write.csv(x,"output/NPS_streams_beaches.csv", row.names = F)

# write lakes csv
## reorder columns
x <- nps.lakes %>% 
  mutate(useData = 1) %>%
  select(AKOATS_ID,Agency_ID,Depth,sampleDate,sampleTime,Temperature,useData) 

# export csv
write.csv(x,"output/Lakes/NPS_lakes.csv", row.names = F)

```



* Among sites and years, various types of loggers are employed (sonde, level logger, HOBO, etc)


