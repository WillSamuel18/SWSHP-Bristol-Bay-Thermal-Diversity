---
title: "UW Water Temp Data Pre-2017"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

# clear environment
rm(list=ls())

# load packages
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(tidyverse)

```

<br>

# Read in data and format

Some data problems found below that I can email Jackie about:

* missing time entries for Nerka Pick Creek in 2011/2012
* missing time entries for Aleknagik Eagle Creek in 2011
* bad time entries (missing hour) for Aleknagik Wood River in September 2009

## Data files through 2017 - SASAP project

Read in data from folder "Jackie Carter UW". These are the 51 files of stream temperature data originally provided for SASAP project, but never archived on KNB. Data are through 2017. Keep file name when reading in data in case it is needed to inspect date-time formats or other formatting problems. Also, keep other information from Jackie, such as Lake and sample type. We will want sample type to indicate logger accuracy for AKTEMP. And, lake information is a good check that we are locating sites in the correct region of the Wood River watershed. Exclude the original Lynx Creek data file in this folder. Jackie provided a new version with only 5 station names and correct locations later.


```{r}
uw.data <- list.files(path = ("data/Jackie Carter UW/Jackie Carter/"),
                      pattern = "*.csv", 
                      full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
           mutate(file_name = gsub(".csv","",basename(x)))) %>%
  select(-X9, -YearSampled) %>% 
  mutate(Temperature = as.numeric(Temperature))

```

Ben noticed some strange date entries for "Alegnagik Wood River" where there were numbers after dates and the hours are missing in the time field. Only three days of bad data, best to remove it altogether since date-time are unknown. Added to list to email Jackie about. (Note that there are a few valid times on these dates, but not enough to correctly calculate daily statistics. Remove all for now and see if these are fixable.)

```{r}
uw.data %>% 
  filter(StationName == "Aleknagik Wood River") %>% 
  mutate(date_len = nchar(Date)) %>% 
  distinct(StationName, Date, date_len) 

#remove 65 entries with bad dates/times for Aleknagik Wood River
uw.data <- uw.data %>%
  mutate(date_len = nchar(Date)) %>% 
  filter(!(StationName == "Aleknagik Wood River" & date_len == 11)) %>% 
  select(-date_len)

```

Format dates and times. Most dates are "%m/%d/%y" 2-digit year. But, Big Whitefish and Little Whitefish have different format, "%d-%b-%y". There are a some sites with missing time entries, about ~2000 total data records. These have been listed out at top along with the site. For now, can just filter to remove these. Email Jackie once all data formatting issues are found and see if any can be fixed.


```{r}
uw.data %>% 
  group_by(StationName) %>%
  slice_head() %>% 
  arrange(StationName)

#1936 with missing times
uw.data %>% 
  filter(is.na(Time)) %>% 
  count(file_name, StationName, SampleType, MeasurementType)

uw.data <- uw.data %>% 
  mutate(sampleDate = parse_date_time(Date, orders = c("d-b-y", "m/d/y")),
         sampleTime = as_hms(parse_date_time(Time, orders = c("I:M:S p", "H:M:S"))))

#1 parsing error remaining that is  filled with NA
uw.data %>% 
  filter(is.na(sampleTime), !is.na(Time)) 

#remove records with missing time information
uw.data <- uw.data %>% 
  filter(!is.na(sampleTime))

```

50 files were read in and there are a total of 55 sites with data.

```{r}
uw.data %>% 
  count(file_name, StationName, year = year(sampleDate)) %>%
  arrange(year, file_name, StationName) %>% 
  pivot_wider(names_from = year, values_from = n)

```


## Date files received in November 2020

Becky emailed with Jackie in November 2020 to get metadata information for the 51 data files sent over in 2017 for the SASAP project and also received some new data files.

* data file for sites on Lynx Creek - 5 stations
* data file for Aleknagik Bear Creek for data through 2017
* data file for all stream sites from 2018 through 2020  



```{r}
uw.lynx <- read_csv("data/Jackie Carter UW/Jackie Carter - 2020 data request/UW-FRI_LynxCreekALL_Temps_JC.csv", 
                    col_types = cols(.default = "c")) %>%
  mutate(file_name = "UW-FRI_LynxCreekALL_Temps_JC") %>%
  transform(sampleDate = as.Date(mdy(Date)),
            sampleTime = as_hms(parse_date_time(Time, "I:M:S p")),
            Temperature = as.numeric(Temperature)) 

uw.aleknagik.bear <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/froRSS_AleknagikBearTemps_2008-2017.xlsx") %>%
  mutate(file_name = "froRSS_AleknagikBearTemps_2008-2017",
         sampleTime = as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature)) 

uw.post2017 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "2018-2020 data", cell_rows(1:329775)) %>%
  mutate(sampleTime = hms::as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature),
         file_name = "forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps")

```

Explore data.

```{r}
summary(uw.lynx)
uw.lynx %>% 
  count(StationName, year(sampleDate))

summary(uw.aleknagik.bear)
uw.aleknagik.bear %>% 
  count(StationName, year(sampleDate))

summary(uw.post2017)
uw.post2017 %>% 
  count(StationName, year(sampleDate))

```



<br>

Join the multiple data frames in to single object. Start with all information, then pare down to just the fields that we need. For AKTEMP, may try to extract more metadata information from measurement type field to get sensor accuracy for data table.

```{r}
keep <- c("file_name", "Lake", "StationName", "SampleType", "MeasurementType", "sampleDate", "sampleTime", "Temperature")

uw.data.2 <- bind_rows(uw.data %>% select(one_of(keep)),
                       uw.lynx %>% select(one_of(keep)),
                       uw.aleknagik.bear %>% select(one_of(keep)),
                       uw.post2017 %>% select(one_of(keep))) %>% 
  rename(SiteID = StationName)

uw.data.2 %>% 
  filter(SiteID == "Aleknagik Bear Creek") %>% 
  summary()

# remove extraneous objects
rm(uw.data, uw.lynx, uw.aleknagik.bear, uw.post2017)
```

Summary csv to add as attributes to leaflet map.

```{r}
uw.data.2 %>% 
  group_by(SiteID) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/uw_data_summ.rds")

uw.data.2 %>% 
  filter(SiteID == "Agulukpak River") %>% 
  count(year(sampleDate))
```



<br>

# Metadata information


Associate an AKOATS_ID with each site where possible

* Naming Conflict Resolution Tasks

- Use sheet from R Shaftel "uw_metadata_thru_2017.xlsx" emailed from J Carter as of 11/17/20)
- See notes column in UW metadata for suggestions on how to resolve these conflicts

```{r}
# remove sites specified not be used in metadata notes
sites_remove <- c("Nerka Lynx Creek Beach","Nerka Lynx Creek Middle")
# apply exclusions
uw.data <- uw.data %>%
  filter(SiteID %ni% sites_remove) 

# rename "Nerka Lynx Creek Upper" as "Nerka Lynx Lake Tributary" as specified in metadata notes
uw.data <- uw.data %>%
  mutate(SiteID = str_replace(SiteID,"Nerka Lynx Creek Upper","Nerka Lynx Lake Tributary"))


# read in UW metadata resolutions emailed from R Shaftel on 11/13/20
uw.metadata <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  # filter out unresolved sites
  filter(AKOATS_ID != "?",
         AKOATS_ID != "NA") %>%
  # use only one AKOATS_ID for sites with multiple entries
  separate(AKOATS_ID, sep = " and", into = "AKOATS_ID") %>%
  separate(AKOATS_ID, sep = " or", into = "AKOATS_ID") %>%
  select(StationName,AKOATS_ID) %>%
  rename(SiteID = StationName) 

# join AKOATS IDs
uw.data <- left_join(uw.data,uw.metadata, by = "SiteID")
 
# there will later be some further resolution of UW metadata and SiteIDs in "metadata_AllSites.Rmd", as some UW sites not have assigned AKOATS_ID values

```


<br>


Perform a quick visualization and summary table to see extent and form of original data.

* Summary table

```{r}
# create data summary table
uw.data.summary <- uw.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

uw.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)
    
```

<br>

* Visualization

```{r}
uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Original Logger Data by Site and Year - Daily Mean")

```

<br>

Notes:

* Among the whole data set, there are some max temps > 50 C, indicating some logger malfunction and/or air exposure.  There is also a minTemp of -21.5 C

* Different types of instruments were employed to collect UW water temp data over the years, including iButtons, Stage gauges, Tidbits, and Level loggers.

<br>

***

Identify and eliminate likely erroneous data 

* Step 1: Start by visually examining datasets that include observations >25 C.  Does anything about these datasets suggest exposure or malfunction? (Or temperatures recorded pre/post deployment in water?)


<br>

Plot 1 
```{r}
# remove extraneous temporary objects from previous steps
rm(uw.data.summary,uw.metadata)

# identify datasets that include observations >25C
above.25 <- uw.data %>%
  filter(Temperature >= 25) %>%
  mutate(year = year(sampleDate)) %>%
  select(SiteID,year) %>%
  distinct()

# plot reduced dataset of site/years that contain >25C observations -- daily means
uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data by Site and Year - Datasets w/ >25 C")

```
<br>

Process to identify and remove erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

1.) Examine & remove bad data from data sets w/ observations >25C
```{r}
# list of watersheds w/ 25C observations to examine w/ ggplotly object
l <- above.25 %>%
  select(-year) %>%
  distinct()

y <- c("2020")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Nerka Agulukpak River"
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data (Example)")
  )
```

<br>

2.) Examine & remove bad data from data sets w/ known suspect data

Notes, from "data/Jackie Carter UW/UA email saved at "data/Jackie Carter UW/UA Mail - Bristol Bay Temperature Data.pdf": "I have noticed some weird stuff going on with some of the older data. I think it may be an artifact of how our former database manager imported the data (bulk import, no distinction between logger type so there are multiple readings per hour but they aren't really as similar as one might expect). The known affected locations and years are: Aleknagik Bear Creek 2010-2013, Nerka Fenno Creek 2010-2013, and Nerka Pick Creek 2006-2012."

Examine the three data sets specified above
```{r}
# potentially erroneous data sets:
# Nerka Fenno Creek 2010-2013
# Nerka Pick Creek 2006-2012
# Aleknagik Bear Creek 2010-2013 

# list of watersheds to examine w/ ggplotly object
l <- c("Nerka Fenno Creek","Nerka Pick Creek","Aleknagik Bear Creek")
z <- uw.data %>%
  filter(SiteID %in% l) %>%
  select(SiteID) %>%
  distinct()

# specify year or years to display here
y <- c("2006")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(z) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Aleknagik Bear Creek"
         # hashtag line below if want to see all years for a site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data (Example)")
  )

```

<br>

3.) Examine & remove bad data from all other datasets (e.g. exposure data)

<br>

```{r}

# list of all watersheds to examine w/ ggplotly object
# do a quick manual run-through this list one at a time to spot visually apparent logger exposure(s)
l <- data.frame(unique(uw.data$SiteID))

# specify year or years to display here
y <- c("2011")
# specify site
site <- "Nerka Rainbow Creek"

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == site
         # hashtag line below if want to see all years for a site
         ,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )

# note: creating a Shiny app for this job would be really useful!  e.g., to be able to easily toggle between various sites and years
```


If further data is is identified to be flagged, edit the csv file at "data/flagged_data/UW_flagged_data.csv"


<br>


Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations


# read in start/end dates for flagged data
uw_flags <- read_csv("data_cleaning/flagged_data/UW_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 




# create table of data to be flagged w/ "useData = 0"
uw_flagged_data <- uw.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(uw_flags,by = c("SiteID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year,-day)




############ NOTE ###############

### Note: the "inner_join" step of the above pipe is behaving oddly.  In this step we want to create a table of all rows to be flagged with useData = 0.  However only a (seemingly random) subset of the sites, years, and days specified in "UW_flagged_data.csv" are being recognized by inner_join.  As a result, not all of the data specified to be flagged as useData = 0 is being flagged.

# Checked through spelling and column classes several times to ensure matches.  As of 11/21/20 not yet diagnosed what is happening at this step.  

# Verbal description of code pipes intended to label data with useData = 0:
# A.) Periods of daily means ID'd to be excluded are manually specified in "UW_flagged_data.csv" by SiteID, start day, end day, and year.

# B.) A dataframe of data to be labeled with useData = 0 is generated using an inner_join pipe with the manually ID'd data.

# C.) A dataframe of data to be labeled with useData = 1 in generated using an anti_join pipe; labeing all data not specified as useData = 0 as useData = 1.

# D.) The two flagged dataframes are joined using bind_rows.

# Step B of the above procedure is not functining.  E.g., search for "Aleknagik Ice Creek" in "uw_flags" and "uw_flagged_data" dataframes.
##################################




# apply flags
uw_nonflagged_data <- anti_join(uw.data,uw_flagged_data,by = c("SiteID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
uw.data <- bind_rows(uw_flagged_data,uw_nonflagged_data)

# remove extraneous objects
rm(above.25,l,p,uw_flagged_data,uw_nonflagged_data,alk)

# how much data is being flagged?
uw.data %>%
  group_by(useData) %>%
  summarise(ct = n())

# calc % of flagged data
# (11457/(1089838 +11457))*100
# 1.04% of data is flagged

```


<br>

To do: try using functions in "Temp_flags_function.R" to IDeven more bad data.

<br>


11/10/20: Ready to output file as soon as we get metadata!

Export csv of combined datasets, with newly identified "do not use data" signified with a "0" in the useData column, to data folder associated with this .Rproj
```{r}
# prep for export
 uw.data <- uw.data %>%
   select(-data_SiteID) 

# reorder columns
x <- uw.data %>% select(AKOATS_ID,SiteID,sampleDate,sampleTime,Temperature,useData)

# export csv
write.csv(x,"output/UW.csv", row.names = F)

```

