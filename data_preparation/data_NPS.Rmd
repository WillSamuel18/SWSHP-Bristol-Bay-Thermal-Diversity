---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document created by Benjamin Meyer (benjamin.meyer.ak@gmail.com) and last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))


# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```


# Metadata

NPS provided a sites info file in excel that has the agency id (aka SiteID) and waterbody names that need to be added to the data files.

Edited the site info worksheet so that the site_name field in the pilot data is a separate entry for joining. Also, transferred the A and B descriptors for the Tlikakila sites to the agency id and removed them from the waterbody name.

No Akoats ids on their site info worksheet. Merge with the akoats metadata to identify which sites we have data for. There look to be a lot more sites in AKOATS under Dan Young's name.

FYI: the two Tlikila sites are 36m apart and are basically duplicates. Looking at the data (see below), site B has lots of missing data and air temperatures and doesn't provide any new data to the time series. Just keep site A and use the AKOATS id that Krista provided - see email 

```{r}
nps.metadata  <- read_excel("data/NPS Bartz/Site_Info_RSS.xlsx", skip = 4) %>% 
  mutate(SourceName = "npsSWAN", 
         Contact_person = "Krista Bartz") %>% 
  select(SiteID = Agency_ID, Waterbody_name = Waterbody_Name, Latitude = Lat, Longitude = Long, 
         SourceName, Contact_person, `File for B_Shaftel`, site_name)

nps.metadata

```


```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```

Looks like Telaquana Lake array is not in AKOATS, but that lake is outside Bristol Bay anyways so should be removed. Also, No Savonoski River in AKOATS - Krista said that is a new site. Tlikakila River site A can be assigned to akoats id 1893 and remove site B - see figure using two datasets below.

```{r}
nps.metadata <- left_join(nps.metadata, akoats.meta %>% filter(Contact_person == "Krista Bartz") %>% select(SiteID, AKOATS_ID)) 

nps.metadata <- nps.metadata %>% 
  filter(!(SiteID %in% c("TELAL_01_TEMP", "LACL_tlikr_stream_water_B"))) %>%
  mutate(SiteID = case_when(site_name == "Tlikakila_Water_A" ~ "LACL_tlikr_stream_water",
                             TRUE ~ SiteID),
         AKOATS_ID = case_when(site_name == "Tlikakila_Water_A" ~ 1893,
                               TRUE ~ AKOATS_ID))
          

#extra fields are for linking to data table, don't keep those for combined metadata.
nps.metadata %>% 
  select(AKOATS_ID, SiteID:Contact_person) %>% 
  saveRDS("output/nps_metadata.rds")


```




# Data 

NOTE - Paul sent an updated pilot sites spreadsheet on 120420 that is now being used - these data are QAed.

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201204.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in all files and separate into lake files, pilot sites file, and all other files with just one site per file.
Read in all sites that are not multi-level lake logger sites or pilot sites.

```{r}
nps.files <- list.files(path = "data/NPS Bartz", pattern="*.csv", full.names = T)

lake.file.ind <- c("_01_", "_02_", "_03_")

#lake files only, but remove telaquana lake bc not in bristol bay
nps.lake.files <- nps.files[grepl(paste(lake.file.ind, collapse="|"), nps.files)] 
nps.lake.files <- nps.lake.files[!grepl("TELAL", nps.lake.files)]


#single site files - removing lake files and pilot sites
nps.single.files <- nps.files[!grepl(paste0(paste(lake.file.ind, collapse="|"), "|Pilot"), nps.files)]

```

Single files.

```{r}
nps.data.1 <-  nps.single.files %>%
  map_df(function(x) read_csv(x, skip = 15, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(x))))

nps.data.1 <- nps.data.1 %>% 
  mutate(sampleDate = as.Date(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         sampleTime = as_hms(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         Temperature = as.numeric(X3)) %>% 
  rename(Approval = X4,
         Grade = X5,
         Qualifiers = X6) %>% 
  select(-X1, -X2, -X3)

nps.data.1 %>% distinct(Approval)
nps.data.1 %>% distinct(Grade)
nps.data.1 %>% distinct(Qualifiers)

nps.data.1 <- left_join(nps.data.1, nps.metadata %>% select(SiteID, `File for B_Shaftel`, Waterbody_name), 
          by = c("file_name" = "File for B_Shaftel"))

nps.data.1
```

Pilot sites.

```{r}
nps.data.2 <- read.csv("data/NPS Bartz/All_Pilot_Sites_20201204.csv") %>%
  mutate(sampleDate = as.Date(Date, format = "%m/%d/%Y"),
         sampleTime = as_hms(parse_date_time(Time, orders = "I:M:S p"))) %>%
  select(-X,-Date,-Time) %>%
  pivot_longer(cols = Little_Kijik_Water:Savonoski_Water, names_to = "site_name", values_to = "Temperature") %>%
  filter(!is.na(Temperature))

nps.data.2 %>% distinct(site_name)
```

Site names in data file don't match waterbody names or agency ids and unfortunately neither are unique - same agency id for two datasets on tlikikila river and two sites on lake clark. Edited the site info file so that Tlikakila sites are unique (A and B) and also added the site_names in this pilot file so that the data can be correctly linked to the agency IDs provided by Krista and Paul.

```{r}
nps.data.2 <- left_join(nps.data.2, nps.metadata %>% select(site_name, SiteID, Waterbody_name))

nps.data.2 %>% distinct(site_name, SiteID, Waterbody_name)
```

Check on differences between two tlikakila river sites, these are basically the same site and can be combined. Site B doesn't look very good, lots of missing data and strange values in early 2018. Just use Site A and assign it to the AKOATS ID provided by Krista.

```{r}

p <- nps.data.2 %>% 
  filter(site_name %in% c("Tlikakila_Water_A", "Tlikakila_Water_B")) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meanT, color = SiteID, linetype = SiteID)) +
  geom_line(size = 0.3)

ggplotly(p)

```

```{r}
nps.data.2 <- nps.data.2 %>% 
  filter(!(site_name == "Tlikakila_Water_B"))
```


Lake data: buoys @ lake centers with temperatures from different depths.


```{r}
nps.lakes <- tibble()
for(i in nps.lake.files){
  dat <- read_csv(i, skip = 3) %>%
    filter(X1 != "Timestamp (UTC-08:00)") 
  colnames(dat) <- sub("Water.Temp\\.", "", colnames(dat))
  dat <- dat %>%
    pivot_longer(-X1, names_to = "Depth", values_to = "Temperature") %>% 
    mutate(Temperature = as.numeric(Temperature),
           Depth = as.numeric(gsub("m", "", Depth)),
           sampleDate = as.Date(parse_date_time(X1, orders = "ymdHMS")),
           sampleTime = as_hms(parse_date_time(X1, orders = "ymdHMS")))
  dat <- dat %>% 
    mutate(file_name = gsub(".csv", "", basename(i))) 
  nps.lakes <- bind_rows(nps.lakes, dat) 
}


nps.lakes
```

Assign AgencyID and waterbody name, and create standard column names as described in "Project_notes.Rmd"

```{r}
nps.lakes <- left_join(nps.lakes, nps.metadata %>% select(SiteID, `File for B_Shaftel`, Waterbody_name), 
          by = c("file_name" = "File for B_Shaftel"))


nps.lakes %>% distinct(SiteID, Waterbody_name)
```

Merge datasets and add a UseData field. Note that in 2019 it looked like there were valid temps greater than 25 at Lake Brooks, set that cutoff higher so those data are not removed.

```{r}
keep <- c("SiteID", "sampleDate", "sampleTime", "Temperature", "Waterbody_name", "Depth")

nps.data <- bind_rows(nps.data.1 %>% select(one_of(keep)),
                      nps.data.2 %>% select(one_of(keep)),
                      nps.lakes %>% select(one_of(keep))) %>% 
  mutate(UseData = case_when(Temperature < -1 ~ 0,
                             Temperature > 30 ~ 0,
                             TRUE ~ 1))

nps.lakes %>% distinct(SiteID, Waterbody_name)

#see data review below - some additional flags on lake data before saving.
# saveRDS(nps.data, "output/nps_data.rds")



```


Summary csv to add as attributes to leaflet map.

```{r}
nps.data %>% 
  group_by(SiteID, Waterbody_name) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/nps_data_summ.rds")

```



# Data review

From Krista and Paul - all data from pilot study workbook have been QAed in addition to the lake temperature arrays. Krista has also plotted all level logger data from 6/1-9/30 and everything looked fine. 

Pilot study sites:

```{r}
pilot_sites <- nps.metadata %>% 
  filter(!site_name == "NA") %>% 
  select(SiteID)

left_join(pilot_sites, nps.data) %>% 
  filter(UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE)) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")
```


Lake temperature arrays:


```{r}
lake_sites <- nps.metadata %>% 
  filter(grepl(paste(lake.file.ind, collapse="|"), `File for B_Shaftel`)) %>% 
  select(SiteID)

pdf("output/Lake array raw data.pdf", width = 11, height = 8.5)

for (i in 1:nrow(lake_sites)){
  site <- lake_sites %>% slice(i)
  p1 <- left_join(site, nps.data) %>%
    filter(UseData == 1) %>% 
    group_by(Waterbody_name, SiteID, Depth, sampleDate) %>% 
    summarize(meanT = mean(Temperature)) %>%
    # complete(SiteID, Depth, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
    mutate(year = year(sampleDate),
           mo_day = format(sampleDate, "%m-%d")) %>%
    ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(Depth))) +
    geom_line() +
    scale_x_date(date_breaks = "3 months", date_labels = "%b") +
    facet_wrap(~ year) 
    labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
         title = site %>% pull(SiteID)) +
    theme_bw() +
    theme(legend.position = "bottom")
  print(p1)
  
}

dev.off()

```


All other sites.

```{r}
other_sites <- nps.metadata %>% 
  filter(!(SiteID %in% c(lake_sites %>% pull(SiteID), pilot_sites %>% pull(SiteID)))) %>% 
  select(SiteID)

other_sites

left_join(other_sites, nps.data) %>% 
  filter(UseData == 1) %>%
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/NPS Other Sites raw data yearround.pdf", width = 8.5, height = 11)
  
```

Look at data just from June - September for these sites, which Krista said has been QAed.

```{r}
left_join(other_sites, nps.data) %>% 
  filter(UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  filter(month(sampleDate) %in% 6:9) %>% 
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/NPS Other Sites raw data summer.pdf", width = 8.5, height = 11)

```

Lake Brooks only.

```{r}

nps.data %>% 
  filter(SiteID == "KATM_lbrooo_lvl", UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d")) %>% 
  filter(month(sampleDate) %in% 5:10, !(year %in% c(2009:2011, 2014))) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT)) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ year, ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "KATM_lbrooo_lvl") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/Lake Brooks summer.pdf", width = 8.5, height = 11)


p <- nps.data %>% 
  filter(SiteID == "KATM_lbrooo_lvl", UseData == 1, year(sampleDate) == 2006) %>% 
  mutate(dt = parse_date_time(paste(sampleDate, sampleTime), orders = "ymd HMS")) %>% 
  complete(SiteID, dt = seq.POSIXt(min(dt), max(dt), by = "hour")) %>%
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()

ggplotly(p)

nps.data %>% 
  slice(1:5) %>% 
  mutate(paste(sampleDate, sampleTime))
```

Lake Clark only.

```{r}
nps.data %>% 
  filter(SiteID == "LACL_lclaro_lvl", UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d")) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT)) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ year, ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "KATM_lbrooo_lvl") +
  theme_bw() +
  theme(legend.position = "bottom")

p <- nps.data %>% 
  filter(SiteID == "LACL_lclaro_lvl", UseData == 1, year(sampleDate) == 2011) %>% 
  mutate(dt = parse_date_time(paste(sampleDate, sampleTime), orders = "ymd HMS")) %>% 
  complete(SiteID, dt = seq.POSIXt(min(dt), max(dt), by = "hour")) %>%
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()

ggplotly(p)



```


Read in excel report where I entered the dates flagged as malfunction or air temperatures from Krista's stage deployment.
Also filter other_sites to just june - september since that is the data that have been qaed.

```{r}
nps_flags <- read_csv("data_preparation/flagged_data/NPS_flagged_data.csv") %>% 
  mutate(flag = 0, 
         sampleDate = as.Date(sampleDate, format = "%m/%d/%Y")) %>% 
  select(SiteID, sampleDate, flag) %>% 
  filter(!is.na(SiteID))

nps.data <- left_join(nps.data, nps_flags) %>% 
  mutate(UseData = case_when(flag == 0 ~ 0,
                             TRUE ~ UseData)) %>% 
  select(-flag)

nps.data <- nps.data %>% 
  mutate(UseData = case_when(SiteID %in% (other_sites %>% pull(SiteID)) & !(month(sampleDate) %in% 6:9) ~ 0,
                             TRUE ~ UseData))
         
nps.data %>% 
  filter(UseData == 1) %>% 
  group_by(SiteID, year(sampleDate)) %>% 
  summarize(mx = max(Temperature, na.rm = TRUE)) %>% 
  arrange(desc(mx))
```

Double check that usedata field assigned correctly to october to may.

```{r}
left_join(other_sites, nps.data) %>% 
  group_by(Waterbody_name, SiteID, sampleDate, UseData) %>% 
  summarize(meanT = mean(Temperature)) %>%
  # complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, group = as.factor(year), color = as.factor(UseData))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

```



Everything looks better in original plot above after adding additional flags. Save final dataset.

```{r}

saveRDS(nps.data, "output/nps_data.rds")

```



# Data review code by Ben
(not updated, everything below written by Ben.)

Create a quick visualization and summary table to see extent and form of original data.

* Summary table, streams & beaches
```{r}
# create data summary table
nps.data.summary <- nps.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,Waterbody_name,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

# render table
nps.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```
We can see there is at least some air exposure in this dataset (minTemp ~ -32C, maxTemp ~36C)

The lake buoys dataset does not immediately appear to have any air exposure data, which makes sense since the loggers were all suspended at various depth profiles!

<br>

Visualize streams & beaches data
```{r}
nps.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Streams & Beaches, Original Logger Data by Site and Year - Daily Mean")

```

<br>

The above visualization indicates there is clearly some air exposure data in our streams & beaches dataset.  We will use ggplotly to examine suspect datasets one at a time, and manually generate a table of erroneous data to be excised.

<br>

Process to identify and remove streams & beaches erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

```{r}

# list of suspect sites to manually examine w/ ggplotly 
suspect.nps.sites <-data.frame(c("KATM_lbrooo_lvl",
                                 "KATM_lbrooo_temp",
                                 "KATM_naknlo_lvl",
                                 "KATM_naknlo_temp",
                                 "LACL_kijilo_lvl",
                                 "LACL_lclaro_lvl",
                                 "LACL_porta_beach_water",
                                 "KATM_savor_stream_water",
                                 "KATM_naknlo_continuous_wq",
                                 "KATM_margc_stream_water",
                                 "LACL_tlikr_stream_sediment"))
colnames(suspect.nps.sites) <- "Agency_ID"

# specify year or years to visualize
y <- c("2019")

# create ggplotly chart
ggplotly(
  p <- nps.data %>%
  inner_join(suspect.nps.sites) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(Agency_ID == "KATM_lbrooo_temp"
         # put hash tag at next line to see all years for the site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Daily Mean Data")
  )

```

If further data is is identified to be flagged, edit the csv file at "data/flagged_data/NPS_flagged_data.csv"

<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
nps_flagged_data <- read.csv("data/flagged_data/NPS_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
nps_flagged_data <- nps.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(nps_flagged_data,by = c("Agency_ID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year
         #,-day
         )

# remove 
nps_nonflagged_data <- anti_join(nps.data,nps_flagged_data,by = c("Agency_ID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
nps.data <- bind_rows(nps_flagged_data,nps_nonflagged_data)

# remove extraneous objects
rm(p,nps_flagged_data,nps_nonflagged_data,suspect.nps.sites)

# data to be excised is now designated with "useData = 0"

```

<br>

Re-Visualize cleaned-up streams & beaches data
```{r}
nps.data %>%
  # filter out flagged data
  filter(useData == 1) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Streams & Beaches, Corrected Logger Data by Site and Year - Daily Mean")

```

<br>

Lake profile data visualization
```{r}
nps.lakes %>%
  group_by(Waterbody_Name,Depth,sampleDate) %>%
  summarise(daily_mean_temp = mean(Temperature)) %>%
  mutate(day = yday(sampleDate),
         year = year(sampleDate)) %>%
  ggplot(aes(daily_mean_temp, Depth, color = as.factor(year))) +
  geom_point(position = position_dodge(width = 1.2)) +
  scale_y_reverse() +
  facet_grid(Waterbody_Name ~ ., scales = "free_y") +
  # this next line is not working; why?
  theme(strip.text = element_text(angle = 90))
  
  #scale_y_continuous(breaks = c(122,152,182,213,244,274,304),
   #                  labels = c("May","June","July","Aug","Sept", "Oct","Nov")) 

```

<br>

Assign AKOATS_IDs to observations
```{r}

# working here 11/17/20 1:50 PM
akoats.meta <- read_excel("data/AKOATS_DATA_2020_working.xlsx") %>%
  rename(AKOATS_ID = seq_id) %>%
  select(AKOATS_ID,Agency_ID)

# streams & beaches
nps.data <- left_join(nps.data,akoats.meta)

# lakes
nps.lakes <- left_join(nps.lakes,akoats.meta)

```


<br>

Save output data
```{r}

# output lake data file seperately, since it has an additional column for "depth" that all the other data does not

# write streams & beaches csv
## reorder columns
x <- nps.data %>% 
  select(AKOATS_ID,Agency_ID,sampleDate,sampleTime,Temperature,useData)
# export csv
write.csv(x,"output/NPS_streams_beaches.csv", row.names = F)

# write lakes csv
## reorder columns
x <- nps.lakes %>% 
  mutate(useData = 1) %>%
  select(AKOATS_ID,Agency_ID,Depth,sampleDate,sampleTime,Temperature,useData) 

# export csv
write.csv(x,"output/Lakes/NPS_lakes.csv", row.names = F)

```



* Among sites and years, various types of loggers are employed (sonde, level logger, HOBO, etc)


