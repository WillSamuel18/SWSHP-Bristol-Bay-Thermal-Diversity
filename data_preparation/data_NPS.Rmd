---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))


# load packages
library(googledrive)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(tidyverse)

```


# Metadata

NPS provided a sites info file in excel that has the agency id (aka SiteID) and waterbody names that need to be added to the data files.

Edited the site info worksheet so that the site_name field in the pilot data is a separate entry for joining. Also, transferred the A and B descriptors for the Tlikakila sites to the agency id and removed them from the waterbody name.

No Akoats ids on their site info worksheet. Merge with the akoats metadata to identify which sites we have data for. There look to be a lot more sites in AKOATS under Dan Young's name.

FYI: the two Tlikila sites are 36m apart and are basically duplicates. Looking at the data (see below), site B has lots of missing data and air temperatures and doesn't provide any new data to the time series. Just keep site A and use the AKOATS id that Krista provided - see email 

```{r}
nps.metadata  <- read_excel("data/NPS Bartz/Site_Info_RSS.xlsx", skip = 4) %>% 
  mutate(SourceName = "npsSWAN", 
         Contact_person = "Krista Bartz") %>% 
  select(SiteID = Agency_ID, Waterbody_name = Waterbody_Name, Latitude = Lat, Longitude = Long, 
         SourceName, Contact_person, `File for B_Shaftel`, site_name)

nps.metadata 

```

Note that Krista has level loggers and temp loggers at four sites. We should check which to keep, per email on 5/14/21 it may be better to keep the temp loggers. Check in data below to see if the time series match and how well they are correlated.

```{r}
lvl_loggers <- nps.metadata %>% 
  filter(grepl("lvl", SiteID)) %>% 
  mutate(lvl_short = substr(SiteID, 1, 11)) %>% 
  pull(lvl_short)

nps.metadata %>% 
  filter(grepl(paste(lvl_loggers, collapse = "|"), SiteID)) %>% 
  arrange(SiteID) 
```


```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,
         Sensor_accuracy,Waterbody_name, Waterbody_type) %>%
  # rename(AKOATS_ID = seq_id,
  #        SiteID = Agency_ID) %>% 
  mutate(seq_id = as.numeric(seq_id),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta %>% 
  filter(Contact_person == "Krista Bartz")

```

Looks like Telaquana Lake array is not in AKOATS, but that lake is outside Bristol Bay anyways so should be removed. Also, No Savonoski River in AKOATS - Krista said that is a new site. Tlikakila River site A can be assigned to akoats id 1893 and remove site B - see figure using two datasets below.

```{r}
intersect(names(nps.metadata), names(akoats.meta))
remove <- names(nps.metadata)

akoats.meta %>% dplyr::select(-one_of(remove))

nps.metadata <- left_join(nps.metadata, akoats.meta %>% dplyr::select(-one_of(names(nps.metadata))),
                          by = c("SiteID" = "Agency_ID")) 

nps.metadata <- nps.metadata %>% 
  filter(!(SiteID %in% c("TELAL_01_TEMP", "LACL_tlikr_stream_water_B"))) %>%
  mutate(SiteID = case_when(site_name == "Tlikakila_Water_A" ~ "LACL_tlikr_stream_water",
                             TRUE ~ SiteID),
         seq_id = case_when(site_name == "Tlikakila_Water_A" ~ 1893,
                               TRUE ~ seq_id),
         Waterbody_type = case_when(is.na(Waterbody_type) ~ "S",
                                    TRUE ~ Waterbody_type))
          

#extra fields are for linking to data table, don't keep those for combined metadata.
nps.metadata %>% 
  saveRDS("output/nps_metadata.rds")

nps.metadata %>% 
  filter(Waterbody_type == "S")

#26 total, 17 are streams
```




# Data 

NOTE - Paul sent an updated pilot sites spreadsheet on 120420 that is now being used - these data are QAed.

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201204.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in all files and separate into lake files, pilot sites file, and all other files with just one site per file.

Read in all sites that are not multi-level lake logger sites or pilot sites.

```{r nps files}
nps.files <- list.files(path = "data/NPS Bartz", pattern="*.csv", full.names = T)

lake.file.ind <- c("_01_", "_02_", "_03_")

#lake files only, but remove telaquana lake bc not in bristol bay
nps.lake.files <- nps.files[grepl(paste(lake.file.ind, collapse="|"), nps.files)] 
nps.lake.files <- nps.lake.files[!grepl("TELAL", nps.lake.files)]


#single site files - removing lake files and pilot sites
nps.single.files <- nps.files[!grepl(paste0(paste(lake.file.ind, collapse="|"), "|Pilot"), nps.files)]

```

Single files.

```{r nps loggers single files}
nps.data.1 <-  nps.single.files %>%
  map_df(function(x) read_csv(x, skip = 15, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(x))))

nps.data.1 <- nps.data.1 %>% 
  mutate(sampleDate = as.Date(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         sampleTime = as_hms(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         Temperature = as.numeric(X3)) %>% 
  rename(Approval = X4,
         Grade = X5,
         Qualifiers = X6) %>% 
  select(-X1, -X2, -X3)

nps.data.1 %>% distinct(Approval)
nps.data.1 %>% distinct(Grade)
nps.data.1 %>% distinct(Qualifiers)

nps.data.1 <- left_join(nps.data.1, nps.metadata %>% select(SiteID, `File for B_Shaftel`, Waterbody_name), 
          by = c("file_name" = "File for B_Shaftel"))

nps.data.1 %>% distinct(SiteID)

nps.data.1 %>% filter(SiteID == "LACL_lclaro_lvl", sampleDate == "2015-06-06")
```


Pilot sites.

```{r nps pilot sites}
nps.data.2 <- read.csv("data/NPS Bartz/All_Pilot_Sites_20201204.csv") %>%
  mutate(sampleDate = as.Date(Date, format = "%m/%d/%Y"),
         sampleTime = as_hms(parse_date_time(Time, orders = "I:M:S p"))) %>%
  select(-X,-Date,-Time) %>%
  pivot_longer(cols = Little_Kijik_Water:Savonoski_Water, names_to = "site_name", values_to = "Temperature") %>%
  filter(!is.na(Temperature))

nps.data.2 %>% distinct(site_name)
```

Site names in data file don't match waterbody names or agency ids and unfortunately neither are unique - same agency id for two datasets on tlikikila river and two sites on lake clark. Edited the site info file so that Tlikakila sites are unique (A and B) and also added the site_names in this pilot file so that the data can be correctly linked to the agency IDs provided by Krista and Paul.

```{r pilot sites join}
nps.data.2 <- left_join(nps.data.2, nps.metadata %>% select(site_name, SiteID, Waterbody_name))

nps.data.2 %>% distinct(site_name, SiteID, Waterbody_name)
```

Check on differences between two Tlikakila river sites, these are basically the same site and can be combined. Site B doesn't look very good, lots of missing data and strange values in early 2018. Just use Site A and assign it to the AKOATS ID provided by Krista.

```{r tlikakila sites comparison}

p <- nps.data.2 %>% 
  filter(site_name %in% c("Tlikakila_Water_A", "Tlikakila_Water_B")) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meanT, color = SiteID, linetype = SiteID)) +
  geom_line(size = 0.3)

ggplotly(p)

```

```{r remove tlikakila b}
nps.data.2 <- nps.data.2 %>% 
  filter(!(site_name == "Tlikakila_Water_B"))
```


Lake data: buoys @ lake centers with temperatures from different depths.


```{r lake arrays}
nps.lakes <- tibble()
for(i in nps.lake.files){
  dat <- read_csv(i, skip = 3) %>%
    filter(X1 != "Timestamp (UTC-08:00)") 
  colnames(dat) <- sub("Water.Temp\\.", "", colnames(dat))
  dat <- dat %>%
    pivot_longer(-X1, names_to = "Depth", values_to = "Temperature") %>% 
    mutate(Temperature = as.numeric(Temperature),
           Depth = as.numeric(gsub("m", "", Depth)),
           sampleDate = as.Date(parse_date_time(X1, orders = "ymdHMS")),
           sampleTime = as_hms(parse_date_time(X1, orders = "ymdHMS")))
  dat <- dat %>% 
    mutate(file_name = gsub(".csv", "", basename(i))) 
  nps.lakes <- bind_rows(nps.lakes, dat) 
}


nps.lakes
```

Assign Agency_ID and waterbody name, and create standard column names as described in "Project_notes.Rmd"

```{r lake arrays join}
nps.lakes <- left_join(nps.lakes, nps.metadata %>% select(SiteID, `File for B_Shaftel`, Waterbody_name), 
          by = c("file_name" = "File for B_Shaftel"))


nps.lakes %>% distinct(SiteID, Waterbody_name)
```

Merge datasets and add a UseData field. Note that in 2019 it looked like there were valid temps greater than 25 at Lake Brooks, set that cutoff higher so those data are not removed.

```{r combined nps data}
keep <- c("SiteID", "sampleDate", "sampleTime", "Temperature", "Waterbody_name", "Depth")

nps.data <- bind_rows(nps.data.1 %>% select(one_of(keep)),
                      nps.data.2 %>% select(one_of(keep)),
                      nps.lakes %>% select(one_of(keep))) %>% 
  mutate(UseData = case_when(Temperature < -1 ~ 0,
                             Temperature > 30 ~ 0,
                             TRUE ~ 1))

nps.lakes %>% distinct(SiteID, Waterbody_name)

#see data review below - some additional flags on lake data before saving.
# saveRDS(nps.data, "output/nps_data.rds")



```


Add in new time series downloaded in 2021.

```{r}
nps.files.2021 <- list.files("S:\\Stream Temperature Data\\NPS Krista Bartz\\May21 Data", full.names = TRUE) 

chull <- read_csv(nps.files.2021[grepl("chull", nps.files.2021)], skip = 2, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(nps.files.2021[grepl("chull", nps.files.2021)])),
                  sampleDate = as.Date(parse_date_time(X2, orders = "mdY")),
                  sampleTime = as_hms(parse_date_time(X3, orders = "IMS p")),
                  Temperature = as.numeric(X4)) %>% 
  select(one_of(keep))

porta <- read_csv(nps.files.2021[grepl("porta", nps.files.2021)], skip = 2, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(nps.files.2021[grepl("porta", nps.files.2021)])),
                  sampleDate = as.Date(parse_date_time(X2, orders = "mdy IMS p")),
                  sampleTime = as_hms(parse_date_time(X2, orders = "mdy IMS p")),
                  Temperature = as.numeric(X3)) %>% 
  select(one_of(keep))

lclaro <- read_csv(nps.files.2021[grepl("lclaro_lvl", nps.files.2021)], skip = 15, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(nps.files.2021[grepl("lclaro_lvl", nps.files.2021)])),
                  sampleDate = as.Date(parse_date_time(X1, orders = "mdY")),
                  sampleTime = as_hms(parse_date_time(X2, orders = "IMS p")),
                  Temperature = as.numeric(X4)) %>% 
  select(one_of(keep))

lclarot <- read_csv(nps.files.2021[grepl("lclaro_temp", nps.files.2021)], skip = 2, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(nps.files.2021[grepl("lclaro_temp", nps.files.2021)])),
                  sampleDate = as.Date(parse_date_time(X2, orders = "mdy IMS p")),
                  sampleTime = as_hms(parse_date_time(X2, orders = "mdy IMS p")),
                  Temperature = as.numeric(X3)) %>% 
  select(one_of(keep))

nps.2021 <- bind_rows(chull, porta, lclaro, lclarot)

nps.2021 <- nps.2021 %>% 
  mutate(SiteID = sub("_2021.*", "", file_name),
         UseData = case_when(Temperature < -1 ~ 0,
                             Temperature > 30 ~ 0,
                             TRUE ~ 1)) %>% 
  left_join(nps.metadata %>% select(SiteID, Waterbody_name))

nps.2021 %>% distinct(file_name, SiteID, Waterbody_name)

nps.2021 %>% count(UseData) #interesting, a lot of bad data in this new dataset.
nps.2021 %>% 
  group_by(UseData) %>% 
  summarize(mean(Temperature)) #mostly air temps
```

Merge 2021 data (received in May for Lake Clark) with rest of data before performing QA. Note that Krista said more data will be coming later this summer that we can use for this project, but it may not be included in the AKSSF thermal sensitivities.

```{r}
nps.data <- bind_rows(nps.data, nps.2021 %>% select(-file_name))
```


# Data review

From Krista and Paul - all data from pilot study workbook have been QAed in addition to the lake temperature arrays. Krista has also plotted all level logger data from 6/1-9/30 and everything looked fine. 

Pilot study sites:

```{r}
pilot_sites <- nps.metadata %>% 
  filter(!site_name == "NA") %>% 
  select(SiteID)

left_join(pilot_sites, nps.data) %>% 
  filter(UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE)) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")
```


Lake temperature arrays:


```{r}
lake_sites <- nps.metadata %>% 
  filter(grepl(paste(lake.file.ind, collapse="|"), `File for B_Shaftel`)) %>% 
  select(SiteID)

pdf("output/Lake array raw data.pdf", width = 11, height = 8.5)

for (i in 1:nrow(lake_sites)){
  site <- lake_sites %>% slice(i)
  p1 <- left_join(site, nps.data) %>%
    filter(UseData == 1) %>% 
    group_by(Waterbody_name, SiteID, Depth, sampleDate) %>% 
    summarize(meanT = mean(Temperature)) %>%
    # complete(SiteID, Depth, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
    mutate(year = year(sampleDate),
           mo_day = format(sampleDate, "%m-%d")) %>%
    ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(Depth))) +
    geom_line() +
    scale_x_date(date_breaks = "3 months", date_labels = "%b") +
    facet_wrap(~ year) 
    labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
         title = site %>% pull(SiteID)) +
    theme_bw() +
    theme(legend.position = "bottom")
  print(p1)
  
}

dev.off()

```


All other sites. Note that I reviewed the four lake outlet sites below with Krista to choose the best logger with the longest time series for each to QA.

```{r}
other_sites <- nps.metadata %>% 
  filter(!(SiteID %in% c(lake_sites %>% pull(SiteID), pilot_sites %>% pull(SiteID)))) %>% 
  select(SiteID)

other_sites

left_join(other_sites, nps.data) %>% 
  filter(UseData == 1) %>%
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/NPS Other Sites raw data yearround.pdf", width = 8.5, height = 11)
  
```

Look at data just from June - September for these sites, which Krista said has been QAed.

```{r}
left_join(other_sites, nps.data) %>% 
  filter(UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  filter(month(sampleDate) %in% 6:9) %>% 
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/NPS Other Sites raw data summer.pdf", width = 8.5, height = 11)

```

Lake Brooks only.

```{r}

nps.data %>% 
  filter(SiteID == "KATM_lbrooo_lvl", UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d")) %>% 
  filter(month(sampleDate) %in% 5:10, !(year %in% c(2009:2011, 2014))) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT)) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ year, ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "KATM_lbrooo_lvl") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/Lake Brooks summer.pdf", width = 8.5, height = 11)


p <- nps.data %>% 
  filter(SiteID == "KATM_lbrooo_lvl", UseData == 1, year(sampleDate) == 2006) %>% 
  mutate(dt = parse_date_time(paste(sampleDate, sampleTime), orders = "ymd HMS")) %>% 
  complete(SiteID, dt = seq.POSIXt(min(dt), max(dt), by = "hour")) %>%
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()

ggplotly(p)

nps.data %>% 
  slice(1:5) %>% 
  mutate(paste(sampleDate, sampleTime))
```

Lake Clark only.

```{r}
nps.data %>% 
  filter(SiteID == "LACL_lclaro_lvl", UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d")) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT)) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ year, ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "KATM_lbrooo_lvl") +
  theme_bw() +
  theme(legend.position = "bottom")

p <- nps.data %>% 
  filter(SiteID == "LACL_lclaro_lvl", UseData == 1, year(sampleDate) == 2011) %>% 
  mutate(dt = parse_date_time(paste(sampleDate, sampleTime), orders = "ymd HMS")) %>% 
  complete(SiteID, dt = seq.POSIXt(min(dt), max(dt), by = "hour")) %>%
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()

ggplotly(p)



```


## Compare level logger data to temp logger data - 4 sites

Compare data from level loggers to temp loggers to sondes at 4 lake outlet sites. Note that I reviewed these decisions with Krista Bartz on 5/25/21 to decide which logger to keep -- mostly based on longest time series and which logger she trusted the most. Note that "cont_wq" suffix indicates water quality sondes with good temperature precision, "temp" are hobo prov2, with good precision, but typically much shorter time series, and "lvl" are level loggers, which aren't as good, but have longest time series. Generally, if the sondes had a long time series, those are preferred, but otherwise use sonde and/or hobo to verify level logger time series so we can trust those data and use them since they are the longest.

Notes below indicate which logger to use and any anomolies to add to the data flags excel sheet, which is imported in next step and used to flag data as UseData = 0.

lbrooo site, just the two loggers. Level logger data for 2006-2008, 2012-2013, 2015-2019. Temp logger provides a good check, but only available for four of those years. Go with level logger for this site as I don't see any major problems and it has a longer time series.

* 2006-2008, 2012-2013, added notes from KB and also obvious air temps to flag worksheet.
* 2015 looks good, they match.
* 2016 9/8-9/12 problem with level logger.
* 2016 9/20 to end of September, looks like temp logger got exposed, two air events.
* 2016 summer many of the peaks are slightly higher for the temp logger, but only by ~0.5 degree.
* 2017 looks good, they match.
* 2018 one air temp spike for level logger
* 2019 looks good, they match.


```{r}
# nps.data <- readRDS("output/nps_data.rds")

nps.data %>% 
  filter(grepl(paste(lvl_loggers, collapse = "|"), SiteID)) %>% 
  distinct(SiteID) 

nps.data %>% 
  filter(grepl("lbrooo", SiteID)) %>% 
  distinct(SiteID)

lbrooo <- nps.data %>% 
  filter(SiteID %in% c("KATM_lbrooo_lvl", "KATM_lbrooo_temp"), month(sampleDate) %in% 6:9, UseData == 1) %>%
  filter(year(sampleDate) == 2007) %>%
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S")) %>% 
  ggplot(aes(x = dt, y = Temperature, color = SiteID)) +
  geom_line(aes(linetype = SiteID))

ggplotly(lbrooo)

```


lclaro site. Level logger data for 2009-2011, 2013-2018. Temp logger provides a good check, but only available for 2014-2020. Note that LACL_claro_continuous_wq (2010, 2012-2020) is also at this location and would probably be the best to use given the problems between the level and temp loggers - time offsets and temp differences.

* 2014 logger patterns match, but they differ by > 1 degree. It's difficult to understand what caused this, was the level logger buried? Are there site notes for visits from summer 2014? Temp logger mostly spans summer months so could just go with that logger.
* 2015 patterns generally match for early part of summer when both loggers have data, but they are off by 7 hours. The temp logger seems more believable because maximum temperatures are in late afternoon, not early morning.
* 2016 same problem with temperatures, they are off by 7 hours and I *think* the temp logger is most likely correct. The maximums for the level logger are lower but only by about 0.2 degree.
* 2017 times are still off by 7 hours, same as 2014, patterns generally match, but level logger is 1 degree colder, maybe buried. Temp logger mostly spans summer months so could just go with that logger.
* 2018 temp logger has two air spikes and only goes to mid-July. Temperatures are off by 1 degree again, level logger is colder. Drop data for this year? Sonde has a large gap from aug-sept, but temp logger also missing and level logger very different times and temps, so not useful for filling in.
* for 2014, 2020, temp logger has a longer more complete summer time series and should be used over sonde.

Use combination of sonde and hobo logger for this site. Level logger time stamp may drift. Note that the sonde and hobo generally match for patterns, but the sonde is colder by about 0.5 degree in some years. Use hobo for 2014 and 2020 and sonde for all other years.

```{r}
lclaro <- nps.data %>% 
  filter(SiteID %in% c("LACL_lclaro_lvl", "LACL_lclaro_temp", "LACL_lclaro_continuous_wq"), UseData == 1, month(sampleDate) %in% 6:9) %>% 
  filter(year(sampleDate) == 2020) %>%
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S")) %>% 
  ggplot(aes(x = dt, y = Temperature, color = SiteID)) +
  geom_line(aes(linetype = SiteID))

ggplotly(lclaro)

```

lkijilo site, there are three loggers at same location with data from 2014-2020 except temp logger missing 2016. Note that stream water site is on opposite bank so could have slightly different temperatures given width at sampling location (~30 m bankfull). 

* 2014 patterns generally match but stream water site is ~0.5 degree colder. Also some strange erratic patterns in the fall the don't match smoother patterns typically seen for temperatures and at other two loggers.
* 2015 two air temp spikes in level logger that should be flagged. Stream water logger still about 0.5 degree colder.
* 2016 patterns match, Stream water logger still about 0.5 degree colder.
* 2017 patterns match, Stream water logger still about 0.5 degree colder.
* 2018 bad data for level logger and temp logger from 9/25 to end of month. But other than that same patterns and differences in temperature as before.
* 2019 patterns match, Stream water logger still about 0.5 degree colder.
* 2020 times on level logger are off, now ahead by 8 hours. Stream water logger looks buried from mid-June through July. Go with temp logger for this year.

Stream water site directly downstream of a cold water creek on LB. Use temp logger, but fill in level logger for 2016; time stamps match in 2015 and 2017 so should be accurate.


```{r}
kijilo <- nps.data %>% 
  filter(SiteID %in% c("LACL_kijilo_lvl", "LACL_kijilo_temp", "LACL_lkijr_stream_water"), UseData == 1,
         month(sampleDate) %in% 6:9) %>% 
  filter(year(sampleDate) == 2015) %>%
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S")) %>% 
  ggplot(aes(x = dt, y = Temperature, color = SiteID)) +
  geom_line(aes(linetype = SiteID))

ggplotly(kijilo)
```


naknlo site, note there are three datasets at same location. level logger has longest time series, 2006-2007, 2009-2018. The cont wq logger has data from 2012-2019 and the temp logger has data only for 2017-2018.

* 2006 air spike level logger.
* 2012 several periods where level logger is 0.5 degree warmer than cont wq. Cont wq also not complete mid June to mid Sept.
* 2013 cont wq missing most of summer and colder than level logger for august/sept.
* 2014 same pattern where cont wq tends to be colder than level logger, but only for some periods. In this year, it starts mid-August and goes until end of year. Possibly this logger is getting buried? Ask Krista if they have retrieval records.
* 2015 cont wq colder by > 1 degree entire summer. 
* 2016 cont wq colder by > 1 degree entire summer. 
* 2017 temp logger gets buried mid-July for at least a month. Level logger starts to deviate from cont wq sensor in early August and is much colder, which is opposite to the pattern from earlier years and earlier in the summer, when the cont wq was colder. Likely the level logger also got buried this year.
* 2018 all three loggers are different this year, cont wq about 1 or more degrees colder than level logger and temp logger is somewhere in between.

Probably best to use level logger for this dataset. Temp logger has very short time series and the cont wq logger is deployed center channel and weighted where it is much colder. But, in 2017, cont wq sensor is only one that doesn't get buried. Krista can look at level logger data for 2017 where the level logger switched and became colder than the sonde.


```{r}
naknlo <- nps.data %>% 
  filter(SiteID %in% c("KATM_naknlo_lvl", "KATM_naknlo_temp", "KATM_naknlo_continuous_wq"), UseData == 1,
         month(sampleDate) %in% 6:9) %>% 
  filter(year(sampleDate) == 2018) %>%
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S")) %>% 
  ggplot(aes(x = dt, y = Temperature, color = SiteID)) +
  geom_line(aes(linetype = SiteID))

ggplotly(naknlo)
```

Create some plots by year comparing the two sets of loggers at the four sites.

```{r}
lvl_loggers

pdf("output/NPS Level Logger Comparison.pdf", width = 11, height = 8.5)
for(i in 1:4) {
  plot.site <- lvl_loggers[i]
  dat <- nps.data %>% 
    filter(SiteID %in% c(paste0(plot.site, "_lvl", collapse = ""), 
                         paste0(plot.site, "_temp", collapse = "")),
           month(sampleDate) %in% 6:9)
  yrs <- dat %>% distinct(year = year(sampleDate))
  for(j in 1:nrow(yrs)) {
    plot.year <- yrs %>% slice(j) %>% pull(year)
    p <- dat %>% 
      filter(year(sampleDate) == plot.year) %>% 
      mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S")) %>% 
      ggplot(aes(x = dt, y = Temperature, color = SiteID)) +
      geom_line(aes(linetype = SiteID)) +
      labs(title = paste(plot.site, plot.year))
    print(p)
  }
}
dev.off()

```

## QA flags for NPS data

Read in excel report where I entered the dates flagged as malfunction or air temperatures from Krista's stage deployment. I also reviewed co-located loggers with Krista at four lake outlets, selected one time series to use and QAed that time series. Other loggers were used to fill in missing years as needed or removed from dataset. 

Also filter data to just june - september since that is the data that have been qaed.

Read in data flags worksheet and remove loggers at co-located sites we are not using. 

```{r}
nps_flags <- read_csv("data_preparation/flagged_data/NPS_flagged_data.csv") %>% 
  mutate(FlagStart = as.Date(parse_date_time(FlagStart, orders = "mdY")),
         FlagEnd = as.Date(parse_date_time(FlagEnd, orders = "mdY")))

remove_sites <- nps_flags %>% 
  filter(UseSite == 0) %>% 
  pull(SiteID)

nps.data <- nps.data %>% 
  mutate(UseData = case_when(SiteID %in% remove_sites ~ 0,
                             TRUE ~ UseData))
```

Expand dates for bad data or years for specific loggers that we aren't using from the co-located sites (sometimes combined loggers across years to get the most complete time series).

```{r}
nps_flag_dates <- nps_flags %>% 
  filter(!UseSite == 0) %>% 
  mutate(flagDate = map2(FlagStart, FlagEnd, ~seq(from = .x, to = .y,
                                                  by = "day"))) %>% 
  unnest() %>%
  select(SiteID, flagDate, FlagReason) 

nps.data %>% 
  count(UseData) #182687/7020543

nps.data <- left_join(nps.data, nps_flag_dates, by = c("SiteID" = "SiteID", "sampleDate" = "flagDate")) %>% 
  mutate(UseData = case_when(!is.na(FlagReason) ~ 0,
                             TRUE ~ UseData)) %>% 
  select(-FlagReason)

nps.data %>% filter(SiteID == "LACL_lclaro_temp", year(sampleDate) == 2018)
```

Finally, flag data not in the June-September window where the data review was performed.

```{r}
nps.data <- nps.data %>% 
  mutate(UseData = case_when(!(month(sampleDate) %in% 6:9) ~ 0,
                             TRUE ~ UseData))
```


Combine site names for co-located loggers at lake outlet sites in the nps.data data frame and the metadata. Note that this doesn't capture the stream water site at Kijik Lake outlet, but that was already flagged UseData = 0, so not a problem.

```{r}
nps.data <- nps.data %>% 
  mutate(SiteID2 = case_when(grepl("KATM_lbrooo", SiteID) ~ "KATM_lbrooo",
                            grepl("LACL_lclaro", SiteID) ~ "LACL_lclaro",
                            grepl("LACL_kijilo", SiteID) ~ "LACL_kijilo",
                            grepl("KATM_naknlo", SiteID) ~ "KATM_naknlo",
                            TRUE ~ SiteID)) 

nps.metadata <- nps.metadata %>% 
  mutate(SiteID2 = case_when(grepl("KATM_lbrooo", SiteID) ~ "KATM_lbrooo",
                            grepl("LACL_lclaro", SiteID) ~ "LACL_lclaro",
                            grepl("LACL_kijilo", SiteID) ~ "LACL_kijilo",
                            grepl("KATM_naknlo", SiteID) ~ "KATM_naknlo",
                            TRUE ~ SiteID)) 


```

Check time series for the co-located sites.

```{r}
nps.data %>% 
  filter(SiteID2 %in% c("KATM_naknlo", "KATM_lbrooo", "LACL_lclaro", "LACL_kijilo"), UseData == 1) %>% 
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S")) %>% 
  ggplot(aes(x = dt, y = Temperature, color = SiteID)) +
  geom_line() +
  facet_wrap(~SiteID2)

```

# Final data file and summary

Everything looks better in original plot above after adding additional flags. Save final dataset.

```{r}
saveRDS(nps.data, "data_preparation/formatted_data/nps_data.rds")
```


Summary csv to add as attributes to leaflet map.

```{r, eval = FALSE}
nps.data %>% 
  group_by(SiteID, Waterbody_name) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/nps_data_summ.rds")

```


# Save daily data and metadata

Save metadata file. Save daily data after screening for days with less than 90% of measurements. No longer saving AKTEMP file, we'll have to revisit our QA with each provider since it is incomplete and let them review and upload for sites.

Lake sites save separately as an .rds to read into lake data script. Note that no real QA done on this dataset since Krista had already done it for summer months.

Also three lakeshore sites that could be included in the thermal regime analysis, but not needed for akssf.

```{r save raw data}
source("W:/Github/AKSSF/helper_functions.R")

nps.metadata

#17 stream or river sites for akssf
save_metadata_files(nps.metadata %>% anti_join(lake_sites) %>% filter(!grepl("beach", SiteID)), "nps_kb")

nps.data.streams <- nps.data %>% 
  anti_join(lake_sites) %>%
  filter(!grepl("beach", SiteID))

# save_aktemp_files(nps.data.streams, "nps_kb")

nps.daily <- temp_msmt_freq(nps.data.streams) %>% daily_screen(.)

save_daily_files(nps.daily, "nps_kb")
```

Save lake data for separate rmarkdown report where those data are combined.

```{r}
saveRDS(left_join(lake_sites, nps.data), file = "data_preparation/formatted_data/nps_lakes.rds")
```



