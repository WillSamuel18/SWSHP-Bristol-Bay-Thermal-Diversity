---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document created by Benjamin Meyer (benjamin.meyer.ak@gmail.com) and last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))


# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
```


# Metadata

NPS provided a sites info file in excel that has the agency id (aka SiteID) and waterbody names that need to be added to the data files.

FYI: the two Tlikila sites are 36m apart; but should still be QA/QC'd separately, so keep separate at this stage of analysis. For aktemp, these can be separate points assigned to same siteid in data table.

Edited the site info worksheet so that the site_name field in the pilot data is a separate entry for joining. Also, transferred the A and B descriptors for the Tlikakila sites to the agency id and removed them from the waterbody name.

No Akoats ids on their site info worksheet. Could try to merge witk the akoats metadata, but not really important right now since they gave us lat/longs for all of their sites.

```{r}
nps.metadata  <- read_excel("data/NPS Bartz/Site_Info_RSS.xlsx", skip = 4) %>% 
  mutate(SourceName = "npsSWAN", 
         Contact_person = "Krista Bartz") %>% 
  select(SiteID = Agency_ID, Waterbody_name = Waterbody_Name, Latitude = Lat, Longitude = Long, SourceName, Contact_person)


nps.metadata

```


```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```

Looks like Telaquana Lake array is not in AKOATS, but that lake is outside Bristol Bay anyways so should be removed. Also, No Avonoski River in AKOATS and there is a Tlikikila River in AKOATS - 4 entries so not sure which to assign here. Some will just be left NA.

```{r}
nps.metadata <- left_join(nps.metadata, akoats.meta %>% filter(Contact_person == "Krista Bartz") %>% select(SiteID, AKOATS_ID)) 
  
saveRDS(nps.metadata, "output/nps_metadata.rds")
```




# Data 

NOTE - there are some updates from Paul and Krista, new qaed data that should be fixed below.

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201204.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in all files and separate into lake files, pilot sites file, and all other files with just one site per file.
Read in all sites that are not multi-level lake logger sites or pilot sites.

```{r}
nps.files <- list.files(path = "data/NPS Bartz", pattern="*.csv", full.names = T)

lake.file.ind <- c("_01_", "_02_", "_03_")

#lake files only
nps.lake.files <- nps.files[grepl(paste(lake.file.ind, collapse="|"), nps.files)]

#single site files - removing lake files and pilot sites
nps.single.files <- nps.files[!grepl(paste0(paste(lake.file.ind, collapse="|"), "|Pilot"), nps.files)]

```

Single files.

```{r}
nps.data.1 <-  nps.single.files %>%
  map_df(function(x) read_csv(x, skip = 15, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(x))))

nps.data.1 <- nps.data.1 %>% 
  mutate(sampleDate = as.Date(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         sampleTime = as_hms(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         Temperature = as.numeric(X3)) %>% 
  rename(Approval = X4,
         Grade = X5,
         Qualifiers = X6) %>% 
  select(-X1, -X2, -X3)

nps.data.1 %>% distinct(Approval)
nps.data.1 %>% distinct(Grade)
nps.data.1 %>% distinct(Qualifiers)

nps.data.1 <- left_join(nps.data.1, nps.metadata %>% select(Agency_ID, `File for B_Shaftel`, Waterbody_Name), 
          by = c("file_name" = "File for B_Shaftel"))

nps.data.1
```

Pilot sites.

```{r}
nps.data.2 <- read.csv("data/NPS Bartz/All_Pilot_Sites_20201204.csv") %>%
  mutate(sampleDate = as.Date(Date, format = "%m/%d/%Y"),
         sampleTime = as_hms(parse_date_time(Time, orders = "I:M:S p"))) %>%
  select(-X,-Date,-Time) %>%
  pivot_longer(cols = Little_Kijik_Water:Savonoski_Water, names_to = "site_name", values_to = "Temperature") %>%
  filter(!is.na(Temperature))

nps.data.2 %>% distinct(site_name)
```

Site names in data file don't match waterbody names or agency ids and unfortunately neither are unique - same agency id for two datasets on tlikikila river and two sites on lake clark. Add agency id and deal with tlikikila river separately.

```{r}
nps.data.2 %>% distinct(site_name)

nps.data.2 <- left_join(nps.data.2, nps.metadata)

nps.data.2 %>% distinct(Agency_ID, Waterbody_Name)
```

Lake data: buoys @ lake centers with tempreatures from different depths.

#STOPPED HERE TIMES NOT PARSING CORRECTLY.

```{r}


nps.lakes <- tibble()
for(i in nps.lake.files){
  dat <- read.csv(i, skip = 3) %>%
    filter(X != "Timestamp (UTC-08:00)") 
  colnames(dat) <- sub("Water.Temp\\.", "", colnames(dat))
  dat <- dat %>%
    pivot_longer(-X, names_to = "Depth", values_to = "Temperature") %>% 
    mutate(Temperature = as.numeric(Temperature),
           Depth = as.numeric(gsub("m", "", Depth)),
           sampleDate = as.Date(parse_date_time(X, orders = "Y-m-d I:M:S")),
           sampleTime = as_hms(parse_date_time(X, orders = "Y-m-d I:M:S")))
  dat <- dat %>% 
    mutate(file_name = gsub(".csv", "", basename(i))) 
  nps.lakes <- bind_rows(nps.lakes, dat) 
}


# assign AgencyID and waterbody name, and create standard column names as described in "Project_notes.Rmd"
nps.lakes <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  select("Agency_ID","File for B_Shaftel","Waterbody_Name") %>%
  right_join(nps.lakes) %>%
  transform(DateTime = ymd_hms(DateTime),
            Temperature = as.numeric(Temperature)) %>%
  mutate(sampleDate = date(DateTime),
         sampleTime = hms::as_hms(DateTime)) %>%
  select(-`File.for.B_Shaftel`,-DateTime) %>%
  # make depth a numeric value
  mutate(Depth = gsub("Depth","",Depth),
         Depth = as.numeric(gsub("m","",Depth))) 

```


```{r}

```

Merge data and save.

```{r}
keep <- c("sampleDate", "sampleTime", "Agency_ID", "Temperature", "Waterbody_Name")

nps.data <- bind_rows(nps.data.1 %>% select(one_of(keep)),
                      nps.data.2 %>% select(one_of(keep))) %>% 
  rename(Waterbody_name = Waterbody_Name,
         SiteID = Agency_ID) %>% 
  mutate(UseData = case_when(Temperature < -1 ~ 0,
                             Temperature > 25 ~ 0,
                             TRUE ~ 1))

saveRDS(nps.data, "output/nps_data.rds")

rm(nps.data.1, nps.data.2)
```


Summary csv to add as attributes to leaflet map.

```{r}
nps.data %>% 
  group_by(SiteID, Waterbody_name) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/nps_data_summ.rds")

```



# Data review

(not updated, everything below written by Ben.)

Create a quick visualization and summary table to see extent and form of original data.

* Summary table, streams & beaches
```{r}
# create data summary table
nps.data.summary <- nps.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(Agency_ID,Waterbody_Name,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

# render table
nps.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```
We can see there is at least some air exposure in this dataset (minTemp ~ -32C, maxTemp ~36C)

The lake buoys dataset does not immediately appear to have any air exposure data, which makes sense since the loggers were all suspended at various depth profiles!

<br>

Visualize streams & beaches data
```{r}
nps.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Streams & Beaches, Original Logger Data by Site and Year - Daily Mean")

```

<br>

The above visualization indicates there is clearly some air exposure data in our streams & beaches dataset.  We will use ggplotly to examine suspect datasets one at a time, and manually generate a table of erroneous data to be excised.

<br>

Process to identify and remove streams & beaches erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

```{r}

# list of suspect sites to manually examine w/ ggplotly 
suspect.nps.sites <-data.frame(c("KATM_lbrooo_lvl",
                                 "KATM_lbrooo_temp",
                                 "KATM_naknlo_lvl",
                                 "KATM_naknlo_temp",
                                 "LACL_kijilo_lvl",
                                 "LACL_lclaro_lvl",
                                 "LACL_porta_beach_water",
                                 "KATM_savor_stream_water",
                                 "KATM_naknlo_continuous_wq",
                                 "KATM_margc_stream_water",
                                 "LACL_tlikr_stream_sediment"))
colnames(suspect.nps.sites) <- "Agency_ID"

# specify year or years to visualize
y <- c("2019")

# create ggplotly chart
ggplotly(
  p <- nps.data %>%
  inner_join(suspect.nps.sites) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(Agency_ID == "KATM_lbrooo_temp"
         # put hash tag at next line to see all years for the site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Daily Mean Data")
  )

```

If further data is is identified to be flagged, edit the csv file at "data/flagged_data/NPS_flagged_data.csv"

<br>

Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations

# read in start/end dates for flagged data
nps_flagged_data <- read.csv("data/flagged_data/NPS_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 

# create table of data to be flagged w/ "useData = 0"
nps_flagged_data <- nps.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(nps_flagged_data,by = c("Agency_ID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year
         #,-day
         )

# remove 
nps_nonflagged_data <- anti_join(nps.data,nps_flagged_data,by = c("Agency_ID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
nps.data <- bind_rows(nps_flagged_data,nps_nonflagged_data)

# remove extraneous objects
rm(p,nps_flagged_data,nps_nonflagged_data,suspect.nps.sites)

# data to be excised is now designated with "useData = 0"

```

<br>

Re-Visualize cleaned-up streams & beaches data
```{r}
nps.data %>%
  # filter out flagged data
  filter(useData == 1) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,Agency_ID,Waterbody_Name,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ Agency_ID) +
  ggtitle("Streams & Beaches, Corrected Logger Data by Site and Year - Daily Mean")

```

<br>

Lake profile data visualization
```{r}
nps.lakes %>%
  group_by(Waterbody_Name,Depth,sampleDate) %>%
  summarise(daily_mean_temp = mean(Temperature)) %>%
  mutate(day = yday(sampleDate),
         year = year(sampleDate)) %>%
  ggplot(aes(daily_mean_temp, Depth, color = as.factor(year))) +
  geom_point(position = position_dodge(width = 1.2)) +
  scale_y_reverse() +
  facet_grid(Waterbody_Name ~ ., scales = "free_y") +
  # this next line is not working; why?
  theme(strip.text = element_text(angle = 90))
  
  #scale_y_continuous(breaks = c(122,152,182,213,244,274,304),
   #                  labels = c("May","June","July","Aug","Sept", "Oct","Nov")) 

```

<br>

Assign AKOATS_IDs to observations
```{r}

# working here 11/17/20 1:50 PM
akoats.meta <- read_excel("data/AKOATS_DATA_2020_working.xlsx") %>%
  rename(AKOATS_ID = seq_id) %>%
  select(AKOATS_ID,Agency_ID)

# streams & beaches
nps.data <- left_join(nps.data,akoats.meta)

# lakes
nps.lakes <- left_join(nps.lakes,akoats.meta)

```


<br>

Save output data
```{r}

# output lake data file seperately, since it has an additional column for "depth" that all the other data does not

# write streams & beaches csv
## reorder columns
x <- nps.data %>% 
  select(AKOATS_ID,Agency_ID,sampleDate,sampleTime,Temperature,useData)
# export csv
write.csv(x,"output/NPS_streams_beaches.csv", row.names = F)

# write lakes csv
## reorder columns
x <- nps.lakes %>% 
  mutate(useData = 1) %>%
  select(AKOATS_ID,Agency_ID,Depth,sampleDate,sampleTime,Temperature,useData) 

# export csv
write.csv(x,"output/Lakes/NPS_lakes.csv", row.names = F)

```



* Among sites and years, various types of loggers are employed (sonde, level logger, HOBO, etc)


