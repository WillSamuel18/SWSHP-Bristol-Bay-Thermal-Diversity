---
title: "NPS Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))


# load packages
library(googledrive)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(tidyverse)

```


# Metadata

NPS provided a sites info file in excel that has the agency id (aka SiteID) and waterbody names that need to be added to the data files.

Edited the site info worksheet so that the site_name field in the pilot data is a separate entry for joining. Also, transferred the A and B descriptors for the Tlikakila sites to the agency id and removed them from the waterbody name.

No Akoats ids on their site info worksheet. Merge with the akoats metadata to identify which sites we have data for. There look to be a lot more sites in AKOATS under Dan Young's name.

FYI: the two Tlikila sites are 36m apart and are basically duplicates. Looking at the data (see below), site B has lots of missing data and air temperatures and doesn't provide any new data to the time series. Just keep site A and use the AKOATS id that Krista provided - see email 

```{r}
nps.metadata  <- read_excel("data/NPS Bartz/Site_Info_RSS.xlsx", skip = 4) %>% 
  mutate(SourceName = "npsSWAN", 
         Contact_person = "Krista Bartz") %>% 
  select(SiteID = Agency_ID, Waterbody_name = Waterbody_Name, Latitude = Lat, Longitude = Long, 
         SourceName, Contact_person, `File for B_Shaftel`, site_name)

nps.metadata

```


```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,
         Sensor_accuracy,Waterbody_name, Waterbody_type) %>%
  # rename(AKOATS_ID = seq_id,
  #        SiteID = Agency_ID) %>% 
  mutate(seq_id = as.numeric(seq_id),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta %>% 
  filter(Contact_person == "Krista Bartz")

```

Looks like Telaquana Lake array is not in AKOATS, but that lake is outside Bristol Bay anyways so should be removed. Also, No Savonoski River in AKOATS - Krista said that is a new site. Tlikakila River site A can be assigned to akoats id 1893 and remove site B - see figure using two datasets below.

```{r}
intersect(names(nps.metadata), names(akoats.meta))
remove <- names(nps.metadata)

akoats.meta %>% dplyr::select(-one_of(remove))

nps.metadata <- left_join(nps.metadata, akoats.meta %>% dplyr::select(-one_of(names(nps.metadata))),
                          by = c("SiteID" = "Agency_ID")) 

nps.metadata <- nps.metadata %>% 
  filter(!(SiteID %in% c("TELAL_01_TEMP", "LACL_tlikr_stream_water_B"))) %>%
  mutate(SiteID = case_when(site_name == "Tlikakila_Water_A" ~ "LACL_tlikr_stream_water",
                             TRUE ~ SiteID),
         seq_id = case_when(site_name == "Tlikakila_Water_A" ~ 1893,
                               TRUE ~ seq_id),
         Waterbody_type = case_when(is.na(Waterbody_type) ~ "S",
                                    TRUE ~ Waterbody_type))
          

#extra fields are for linking to data table, don't keep those for combined metadata.
nps.metadata %>% 
  saveRDS("output/nps_metadata.rds")

nps.metadata %>% 
  filter(Waterbody_type == "S")

#26 total, 17 are streams
```




# Data 

NOTE - Paul sent an updated pilot sites spreadsheet on 120420 that is now being used - these data are QAed.

Read in data from folder "NPS Bartz"

Read-in notes:

* The file "All_Pilot_Sites_20201204.csv" is not in a format consistent with all other stream-site csv files.  Read in separately.

* The following csv files are site files containing all data from an anchored line in a lake w/ loggers at various (5m-10m) depth increments.  Read in and address these separately:

- LCLAR_01_TEMP
- KIJIL_01_TEMP
- TELAL_01_TEMP
- LBROO_01_TEMP
- NAKNL_03_TEMP
- NAKNL_01_TEMP
- NAKNL_02_TEMP

* Some other lake sites are single-logger per site (e.g. lake shore sites)

Read in all files and separate into lake files, pilot sites file, and all other files with just one site per file.

Read in all sites that are not multi-level lake logger sites or pilot sites.

```{r nps files}
nps.files <- list.files(path = "data/NPS Bartz", pattern="*.csv", full.names = T)

lake.file.ind <- c("_01_", "_02_", "_03_")

#lake files only, but remove telaquana lake bc not in bristol bay
nps.lake.files <- nps.files[grepl(paste(lake.file.ind, collapse="|"), nps.files)] 
nps.lake.files <- nps.lake.files[!grepl("TELAL", nps.lake.files)]


#single site files - removing lake files and pilot sites
nps.single.files <- nps.files[!grepl(paste0(paste(lake.file.ind, collapse="|"), "|Pilot"), nps.files)]

```

Single files.

```{r nps loggers single files}
nps.data.1 <-  nps.single.files %>%
  map_df(function(x) read_csv(x, skip = 15, col_types = cols(.default = "c"), col_names = F) %>%
           mutate(file_name = gsub(".csv","",basename(x))))

nps.data.1 <- nps.data.1 %>% 
  mutate(sampleDate = as.Date(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         sampleTime = as_hms(parse_date_time(X2, orders = "Y-m-d H:M:S")),
         Temperature = as.numeric(X3)) %>% 
  rename(Approval = X4,
         Grade = X5,
         Qualifiers = X6) %>% 
  select(-X1, -X2, -X3)

nps.data.1 %>% distinct(Approval)
nps.data.1 %>% distinct(Grade)
nps.data.1 %>% distinct(Qualifiers)

nps.data.1 <- left_join(nps.data.1, nps.metadata %>% select(SiteID, `File for B_Shaftel`, Waterbody_name), 
          by = c("file_name" = "File for B_Shaftel"))

nps.data.1
```

Pilot sites.

```{r nps pilot sites}
nps.data.2 <- read.csv("data/NPS Bartz/All_Pilot_Sites_20201204.csv") %>%
  mutate(sampleDate = as.Date(Date, format = "%m/%d/%Y"),
         sampleTime = as_hms(parse_date_time(Time, orders = "I:M:S p"))) %>%
  select(-X,-Date,-Time) %>%
  pivot_longer(cols = Little_Kijik_Water:Savonoski_Water, names_to = "site_name", values_to = "Temperature") %>%
  filter(!is.na(Temperature))

nps.data.2 %>% distinct(site_name)
```

Site names in data file don't match waterbody names or agency ids and unfortunately neither are unique - same agency id for two datasets on tlikikila river and two sites on lake clark. Edited the site info file so that Tlikakila sites are unique (A and B) and also added the site_names in this pilot file so that the data can be correctly linked to the agency IDs provided by Krista and Paul.

```{r pilot sites join}
nps.data.2 <- left_join(nps.data.2, nps.metadata %>% select(site_name, SiteID, Waterbody_name))

nps.data.2 %>% distinct(site_name, SiteID, Waterbody_name)
```

Check on differences between two Tlikakila river sites, these are basically the same site and can be combined. Site B doesn't look very good, lots of missing data and strange values in early 2018. Just use Site A and assign it to the AKOATS ID provided by Krista.

```{r tlikakila sites comparison}

p <- nps.data.2 %>% 
  filter(site_name %in% c("Tlikakila_Water_A", "Tlikakila_Water_B")) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meanT, color = SiteID, linetype = SiteID)) +
  geom_line(size = 0.3)

ggplotly(p)

```

```{r remove tlikakila b}
nps.data.2 <- nps.data.2 %>% 
  filter(!(site_name == "Tlikakila_Water_B"))
```


Lake data: buoys @ lake centers with temperatures from different depths.


```{r lake arrays}
nps.lakes <- tibble()
for(i in nps.lake.files){
  dat <- read_csv(i, skip = 3) %>%
    filter(X1 != "Timestamp (UTC-08:00)") 
  colnames(dat) <- sub("Water.Temp\\.", "", colnames(dat))
  dat <- dat %>%
    pivot_longer(-X1, names_to = "Depth", values_to = "Temperature") %>% 
    mutate(Temperature = as.numeric(Temperature),
           Depth = as.numeric(gsub("m", "", Depth)),
           sampleDate = as.Date(parse_date_time(X1, orders = "ymdHMS")),
           sampleTime = as_hms(parse_date_time(X1, orders = "ymdHMS")))
  dat <- dat %>% 
    mutate(file_name = gsub(".csv", "", basename(i))) 
  nps.lakes <- bind_rows(nps.lakes, dat) 
}


nps.lakes
```

Assign Agency_ID and waterbody name, and create standard column names as described in "Project_notes.Rmd"

```{r lake arrays join}
nps.lakes <- left_join(nps.lakes, nps.metadata %>% select(SiteID, `File for B_Shaftel`, Waterbody_name), 
          by = c("file_name" = "File for B_Shaftel"))


nps.lakes %>% distinct(SiteID, Waterbody_name)
```

Merge datasets and add a UseData field. Note that in 2019 it looked like there were valid temps greater than 25 at Lake Brooks, set that cutoff higher so those data are not removed.

```{r combined nps data}
keep <- c("SiteID", "sampleDate", "sampleTime", "Temperature", "Waterbody_name", "Depth")

nps.data <- bind_rows(nps.data.1 %>% select(one_of(keep)),
                      nps.data.2 %>% select(one_of(keep)),
                      nps.lakes %>% select(one_of(keep))) %>% 
  mutate(UseData = case_when(Temperature < -1 ~ 0,
                             Temperature > 30 ~ 0,
                             TRUE ~ 1))

nps.lakes %>% distinct(SiteID, Waterbody_name)

#see data review below - some additional flags on lake data before saving.
# saveRDS(nps.data, "output/nps_data.rds")



```


# Data review

From Krista and Paul - all data from pilot study workbook have been QAed in addition to the lake temperature arrays. Krista has also plotted all level logger data from 6/1-9/30 and everything looked fine. 

Pilot study sites:

```{r}
pilot_sites <- nps.metadata %>% 
  filter(!site_name == "NA") %>% 
  select(SiteID)

left_join(pilot_sites, nps.data) %>% 
  filter(UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE)) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")
```


Lake temperature arrays:


```{r}
lake_sites <- nps.metadata %>% 
  filter(grepl(paste(lake.file.ind, collapse="|"), `File for B_Shaftel`)) %>% 
  select(SiteID)

pdf("output/Lake array raw data.pdf", width = 11, height = 8.5)

for (i in 1:nrow(lake_sites)){
  site <- lake_sites %>% slice(i)
  p1 <- left_join(site, nps.data) %>%
    filter(UseData == 1) %>% 
    group_by(Waterbody_name, SiteID, Depth, sampleDate) %>% 
    summarize(meanT = mean(Temperature)) %>%
    # complete(SiteID, Depth, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
    mutate(year = year(sampleDate),
           mo_day = format(sampleDate, "%m-%d")) %>%
    ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(Depth))) +
    geom_line() +
    scale_x_date(date_breaks = "3 months", date_labels = "%b") +
    facet_wrap(~ year) 
    labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
         title = site %>% pull(SiteID)) +
    theme_bw() +
    theme(legend.position = "bottom")
  print(p1)
  
}

dev.off()

```


All other sites.

```{r}
other_sites <- nps.metadata %>% 
  filter(!(SiteID %in% c(lake_sites %>% pull(SiteID), pilot_sites %>% pull(SiteID)))) %>% 
  select(SiteID)

other_sites

left_join(other_sites, nps.data) %>% 
  filter(UseData == 1) %>%
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/NPS Other Sites raw data yearround.pdf", width = 8.5, height = 11)
  
```

Look at data just from June - September for these sites, which Krista said has been QAed.

```{r}
left_join(other_sites, nps.data) %>% 
  filter(UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  filter(month(sampleDate) %in% 6:9) %>% 
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = as.factor(year))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/NPS Other Sites raw data summer.pdf", width = 8.5, height = 11)

```

Lake Brooks only.

```{r}

nps.data %>% 
  filter(SiteID == "KATM_lbrooo_lvl", UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d")) %>% 
  filter(month(sampleDate) %in% 5:10, !(year %in% c(2009:2011, 2014))) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT)) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ year, ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "KATM_lbrooo_lvl") +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("output/Lake Brooks summer.pdf", width = 8.5, height = 11)


p <- nps.data %>% 
  filter(SiteID == "KATM_lbrooo_lvl", UseData == 1, year(sampleDate) == 2006) %>% 
  mutate(dt = parse_date_time(paste(sampleDate, sampleTime), orders = "ymd HMS")) %>% 
  complete(SiteID, dt = seq.POSIXt(min(dt), max(dt), by = "hour")) %>%
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()

ggplotly(p)

nps.data %>% 
  slice(1:5) %>% 
  mutate(paste(sampleDate, sampleTime))
```

Lake Clark only.

```{r}
nps.data %>% 
  filter(SiteID == "LACL_lclaro_lvl", UseData == 1) %>% 
  group_by(Waterbody_name, SiteID, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) %>%
  complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d")) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT)) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b") +
  facet_wrap(~ year, ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "KATM_lbrooo_lvl") +
  theme_bw() +
  theme(legend.position = "bottom")

p <- nps.data %>% 
  filter(SiteID == "LACL_lclaro_lvl", UseData == 1, year(sampleDate) == 2011) %>% 
  mutate(dt = parse_date_time(paste(sampleDate, sampleTime), orders = "ymd HMS")) %>% 
  complete(SiteID, dt = seq.POSIXt(min(dt), max(dt), by = "hour")) %>%
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()

ggplotly(p)



```


Read in excel report where I entered the dates flagged as malfunction or air temperatures from Krista's stage deployment.
Also filter other_sites to just june - september since that is the data that have been qaed.

```{r}
nps_flags <- read_csv("data_preparation/flagged_data/NPS_flagged_data.csv") %>% 
  mutate(flag = 0, 
         sampleDate = as.Date(sampleDate, format = "%m/%d/%Y")) %>% 
  select(SiteID, sampleDate, flag) %>% 
  filter(!is.na(SiteID))

nps.data <- left_join(nps.data, nps_flags) %>% 
  mutate(UseData = case_when(flag == 0 ~ 0,
                             TRUE ~ UseData)) %>% 
  select(-flag)

nps.data <- nps.data %>% 
  mutate(UseData = case_when(SiteID %in% (other_sites %>% pull(SiteID)) & !(month(sampleDate) %in% 6:9) ~ 0,
                             TRUE ~ UseData))
         
nps.data %>% 
  filter(UseData == 1) %>% 
  group_by(SiteID, year(sampleDate)) %>% 
  summarize(mx = max(Temperature, na.rm = TRUE)) %>% 
  arrange(desc(mx))
```

Double check that usedata field assigned correctly to october to may.

```{r}
left_join(other_sites, nps.data) %>% 
  group_by(Waterbody_name, SiteID, sampleDate, UseData) %>% 
  summarize(meanT = mean(Temperature)) %>%
  # complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>%
  mutate(year = year(sampleDate),
         mo_day = format(sampleDate, "%m-%d"),
         facetLabel = paste0(Waterbody_name, " (", SiteID, ")")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, group = as.factor(year), color = as.factor(UseData))) +
  geom_line(size = 0.1) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b") +
  facet_wrap(~ facetLabel, labeller = label_wrap_gen(width = 18, multi_line = TRUE), ncol = 3) +
  labs(x = "Date", y = "Mean Daily Temperature", color = "Year", 
       title = "NPS Original Logger Data by Site and Year") +
  theme_bw() +
  theme(legend.position = "bottom")

```





# Final data file and summary

Everything looks better in original plot above after adding additional flags. Save final dataset.

```{r}

nps.data %>% 
  distinct(Waterbody_name, Depth)

saveRDS(nps.data, "output/nps_data.rds")

```


Summary csv to add as attributes to leaflet map.

```{r}
nps.data %>% 
  group_by(SiteID, Waterbody_name) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/nps_data_summ.rds")

```


# Save daily data and metadata

Save raw data for AKTEMP, which includes date and time. Save metadata file. Save daily data after screening for days with less than 90% of measurements.

This is really for AKSSF so not using lake sites right now. 

```{r save raw data}
source("W:/Github/AKSSF/helper_functions.R")

nps.metadata

save_metadata_files(nps.metadata %>% anti_join(lake_sites) %>% filter(!grepl("beach", SiteID)), "nps_kb")

nps.data.streams <- nps.data %>% 
  mutate(UseData = 1) %>% 
  anti_join(lake_sites) %>%
  filter(!grepl("beach", SiteID))

nps.data.streams %>% 
  distinct(SiteID) %>% 
  left_join(nps.metadata %>% 
  filter(Waterbody_type == "S"))

save_aktemp_files(nps.data.streams, "nps_kb")

nps.daily <- temp_msmt_freq(nps.data.streams) %>% daily_screen(.)

save_daily_files(nps.daily, "nps_kb")
```

