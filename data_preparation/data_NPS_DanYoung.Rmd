---
title: "data_NPS_DanYoung"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

library(googledrive)
library(readxl)
library(plotly)
library(hms)
library(tidyverse)
```

# Data

Read in data. They also provided USGS data for the Chulitna River, but I have that already in the nwis download.

```{r}
dy.files <- list.files("S:/Stream Temperature Data/NPS Dan Young/Dan_Young_Data", pattern = ".csv", full.names = TRUE) 

#one site has different formatting.
dy.files <- dy.files[!grepl("LACL", dy.files)]

dy.dat <- map_df(dy.files, function(x) read_csv(x, comment = "#") %>% mutate(file_name = basename(x)))

dy.dat %>% distinct(Grade)
dy.dat %>% distinct(Qualifiers)
dy.dat %>% distinct(`Approval Level`)

dy.dat <- dy.dat %>% 
  mutate(sampleDate = as.Date(`Timestamp (UTC-08:00)`),
         sampleTime = as_hms(`Timestamp (UTC-08:00)`),
         dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S"),
         SiteID = gsub(".csv","",basename(file_name)),
         UseData = 1) %>%
  mutate(SiteID = case_when(SiteID == "Chulitna_Dan_2007_2011" ~ "Chulitna_SiteB",
                            TRUE ~ SiteID)) %>% 
  rename(Temperature = Value) %>% 
  select(file_name, SiteID, sampleDate, sampleTime, dt, Temperature, UseData)

dy.dat

```

This is a site on the Chulitna River that corresponds to an old USGS gage location.

```{r}
dy.lacl <- read_csv("S:/Stream Temperature Data/NPS Dan Young/Dan_Young_Data/LACL_chulr_temp_20200925export.csv", skip = 1)

dy.lacl <- dy.lacl %>% 
  rename(sampleTime = `Time, GMT-08:00`,
         Temperature = `Temp, Â°C (LGR S/N: 11001174, SEN S/N: 11001174, LBL: LACL_CHULR_TEMP_)`) %>% 
  mutate(sampleDate = as.Date(Date, format = "%m/%d/%Y"), 
         SiteID = 15298040,
         dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M:%S"),
         UseData = 1) %>% 
  select(SiteID, sampleDate, sampleTime, dt, Temperature, UseData)
```

Look at the data:
* Chulitna R looks like it is getting dewatered very regularly. 2019 and 2020 are totally out and too much bad data in 2018 (one month of air  temps) to include it in the dataset.

```{r}
p1 <- dy.lacl %>% 
  ggplot(aes(x = dt, y = Temperature)) + 
  geom_line()

ggplotly(p1)
```

Six other sites, lots of long-term data. 

* Chulitna Site B looks ok, 2008 incomplete, but might be enough to include, ended late August. 2010/11 look good.
* Hardenburg Bay has many years of data and only a few spikes indicating air temps.
* Little Kijik River has many years of data and only some air temp spikes. Need to email Paul about whether this should be combined with their site, which is upstream above a cold water spring. We can keep them separate.
* Newhalen R also looks like it gets dewatered a lot, but a lot of years of data. Pass along and see if there is enough that we can keep it.
* Six mile outlet only has two years of data and it doesn't look the best, incomplete, some air temps.
* Tazimina incomplete. Paul sent this to combine with their site, but 2001 incomplete started in late August and 2002 looks like it was buried. 

```{r}
dy.sites <- dy.dat %>% distinct(SiteID) %>% pull(SiteID)

i <- 6

p2 <- dy.dat %>% 
  filter(SiteID == dy.sites[i]) %>% 
  ggplot(aes(x = dt, y = Temperature)) + 
  geom_line() +
  labs(title = dy.sites[i])

ggplotly(p2)
```

Remove Tazimina and keep Little Kijik Separate for now. Five sites for Priscilla to review.

```{r}
dy.dat %>% 
  filter(!SiteID %in% "Tazimina_Dan") %>% 
  saveRDS("data_preparation/formatted_data/nps_dy_5sites.rds")
```

```{r}
test <- readRDS(file = "data_preparation/formatted_data/nps_dy_5sites.rds")
test %>% distinct(SiteID)
```

# Metadata

Paul provided a crosswalk with lat/longs and akoats ids. Note that Sixmile Outlet is in AKOATS, seq_id is 770.

```{r}
dy.md <- read_excel("S:/Stream Temperature Data/NPS Dan Young/Dan_Young_Data/Crosswalk.xlsx") 

#remove sites we aren't using.
dy.md <- dy.md %>% 
  filter(!`AKOATS ID` %in% c("Chulitna River - USGS - Site 15298040", 
                             "Tazimina River - NPS - Site LACL_tazir_stream_water")) %>% 
  mutate(seq_id = as.numeric(`AKOATS seq_id`),
         seq_id = case_when(`File Name` == "Sixmile_Outlet.csv" ~ 770,
                            TRUE ~ seq_id))

```

AKOATS metadata

```{r}
akoats.meta <- read_excel("S:/EPA AKTEMP/AKOATS_DATA_2020_Working.xlsx", sheet = "AKOATS_COMPLETE", col_types = "text") %>%
  filter(Sample_interval == "continuous") %>% 
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate(seq_id = as.numeric(seq_id),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))
  # rename(AKOATS_ID = seq_id,
  #        SiteID = Agency_ID)  

akoats.meta %>% 
  filter(seq_id %in% c(778, 781, 766, 789))
```
Get metadata from akoats and compare lat/long before just using original akoats metadata for these sites.

```{r}
intersect(names(dy.md), names(akoats.meta))

#all match
left_join(dy.md, akoats.meta) %>% 
  select(Lat, Latitude,
         Long, Longitude)

dy.md <- left_join(dy.md, akoats.meta) %>% 
  select(-(`File Name`:Comments))

#add a siteid field that will link to data with nps in front because the waterbody names are causing problems.
dy.md <- dy.md %>% 
  mutate(SiteID = case_when(Agency_ID == "Chulitna River" ~ "Chulitna_SiteB",
                            Agency_ID == "Little Kijik River below Kijik Lake" ~ "Little Kijik River Dan",
                            Agency_ID == "Sixmile Lake outlet" ~ "Six Mile Outlet",
                            TRUE ~ Agency_ID),
         SiteID = paste0("NPS_", SiteID),
         SiteID = gsub(" ", "_", SiteID))

dy.md %>% distinct(Agency_ID, SiteID)
```

Save metadata file on google drive and locally.

```{r}
source("W:/Github/AKSSF/helper_functions.R")

save_metadata_files(dy.md, "nps_dy")
```

