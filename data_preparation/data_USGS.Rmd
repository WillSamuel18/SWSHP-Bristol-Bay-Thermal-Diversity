---
title: "USGS Water Temp"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))



# load packages
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(dataRetrieval)
library(sf)
library(tidyverse)

```

<br>

Note: USGS gauge site locations do not appear to be on the AKOATS site.

Bristol Bay Region USGS sites with temperature data (from https://waterdata.usgs.gov/ak/nwis/rt):

* USGS 15302000 NUYAKUK R NR DILLINGHAM AK, 2014-05-19 to present
* USGS 15300300 ILIAMNA R NR PEDRO BAY AK, 2013-10-01 to present
* USGS 15302250 NF KOKTULI R NR ILIAMNA AK, 2013-10-01 - present
* USGS 15302812 KOKWOK R 22 MI AB NUSHAGAK R NR EKWOK AK, 2016-10-14 to 2020-04-07

<br>

Read in USGS Sites one at a time with readNWIS package

* Establish today's date
```{r}
todaysDate <- Sys.time() %>% as.Date() %>% as.character()
```

<br>

* Nuyakuk River near Dillingham
```{r}
# following example workflow (from https://owi.usgs.gov/R/dataRetrieval.html#1)

# Nuyakuk River near Dillingham
siteNumber <- "15302000" 
nuyakukInfo <- readNWISsite(siteNumber)
parameterCd <- "00010"


# raw data to date:
nuyakakData <- readNWISuv(siteNumber,parameterCd,
                      "2014-05-19", todaysDate)
```

<br>

* Iliamna River near Pedro bay
```{r}
# Iliamna River near Pedro bay
siteNumber <- "15300300" 
iliamnaInfo <- readNWISsite(siteNumber)
parameterCd <- "00010"

# raw data:
iliamnaData <- readNWISuv(siteNumber,parameterCd,
                      "2013-10-01",todaysDate)

```

<br>

* North Fork Koktuli River near Iliamna
```{r}
# North Fork Koktuli River near Iliamna
siteNumber <- "15302250" 
nfkoktuliInfo <- readNWISsite(siteNumber)
parameterCd <- "00010"

# raw data:
nfkoktuliData <- readNWISuv(siteNumber,parameterCd,
                      "2013-10-01",todaysDate)

```

<br>

* Kokwok river near Ekwok
```{r}
# Kokwok river near Ekwok
siteNumber <- "15302812" 
kokwokInfo <- readNWISsite(siteNumber)
parameterCd <- "00010"

# raw data:
kokwokData <- readNWISuv(siteNumber,parameterCd,
                      "2016-10-14",todaysDate)

```



Join data from all sites, rename and format columns as specified in "Project_notes.Rmd"
```{r}
# join all USGS raw data
usgsData <- bind_rows(nuyakakData,iliamnaData,nfkoktuliData,kokwokData) 

# auto-rename columns
# auto-rename
usgsData <- renameNWISColumns(usgsData)

# join together USGS-provided metadata
usgsMetadata <- bind_rows(nuyakukInfo,iliamnaInfo,nfkoktuliInfo,kokwokInfo) %>%
  select(site_no,station_nm)

# assign USGS station names to temperature data
usgsData <- left_join(usgsData,usgsMetadata)

# transform and rename columns
usgsData <- usgsData %>%
  mutate(sampleDate = date(dateTime),
         sampleTime = hms::as_hms(dateTime)) %>%
  rename(Temperature = Wtemp_Inst,
         SiteID = site_no) %>%
  select(SiteID,sampleDate,sampleTime,Temperature)

```

<br>
Assign AKOATS_IDs where possible
```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "AKOATS_COMPLETE") %>%
  filter(SourceName == "USGS") %>%
  select(seq_id,Agency_ID) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID)

usgsData <- left_join(usgsData,akoats.meta,by = "SiteID")
```



<br>

Visualize available data
```{r}
usgsData %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Original Logger Data by Site and Year - Daily Mean")

```

<br>

There is a not a readily apparent need for extensive QA/QC of USGS water temperature data.  Some 2020 data is described as "provisional."  However, this script can be run again anytime and will update outputs with the most current data, so do so if any data is revealed to be suspect.

<br>

Assign "useData = 1" to all USGS water temp data
```{r}
usgsData <- usgsData %>%
  mutate(useData = 1)
```



Export csv of combined datasets to data folder associated with this .Rproj
```{r}
# reorder columns
x <- usgsData %>% select("AKOATS_ID","SiteID","sampleDate","sampleTime","Temperature","useData")

# export csv
 write.csv(x,"output/USGS.csv", row.names = F)

```


# RS code for getting all sites

First filter from AK down to AKSSF, then separate split for Bristol Bay only.

Read in all sites with daily values for AK and temperature. Convert to an SF object.

```{r usgs temp sites for AK}
usgs_temp <- whatNWISdata(stateCd = "AK", service = "dv", 
                             parameterCd = c("00010"))

usgs_temp <- usgs_temp %>% select(agency_cd:dec_long_va, huc_cd) %>% distinct()

usgs_temp_sf <- st_as_sf(usgs_temp, coords = c("dec_long_va", "dec_lat_va"), crs = "WGS84")
usgs_akalb <- st_transform(usgs_temp_sf, crs = 3338)


ggplot() +
  geom_sf(data = usgs_temp_sf)
```


```{r metadata for usgs sites in akssf study area}
#old filter on Bristol Bay and Cook Inlet only.
# usgs_sa <- st_intersection(usgs_temp_sf, huc8_sa)

akssf_sa <- st_read(dsn = "W:/GIS/AKSSF Southcentral/AKSSF_Hydrography.gdb", layer = "AKSSF_studyarea_HUC8")
st_crs(akssf_sa) == st_crs(usgs_akalb)
usgs_sa <- st_intersection(usgs_akalb, akssf_sa)
usgs_sa_wgs84 <- st_transform(usgs_sa, crs = "wgs84")

usgs_md <- bind_cols(usgs_sa_wgs84, as.data.frame(st_coordinates(usgs_sa_wgs84))) %>% 
  st_drop_geometry() %>% 
  rename(SourceName = agency_cd,
         Agency_ID = site_no,
         Waterbody_name = station_nm,
         Latitude = Y,
         Longitude = X) %>% 
  mutate(SiteID = paste0("usgs_", Agency_ID)) %>% 
  select(SourceName, SiteID, Agency_ID, Waterbody_name, Latitude, Longitude)

usgs_md %>% 
  arrange(Agency_ID, SiteID)
```

AKOATS metadata

```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "AKOATS_COMPLETE", col_types = "text") %>%
  filter(Sample_interval == "continuous") %>% 
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name,Waterbody_type,Sensor_Placement) %>%
  mutate(seq_id = as.numeric(seq_id),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta
```



```{r get akoats ids}
akoats.usgs <- akoats.meta %>% 
  filter(SourceName == "USGS")

akoats.usgs %>% arrange(Waterbody_name)

left_join(usgs_md, akoats.usgs %>% select(Agency_ID, seq_id)) %>% count(is.na(seq_id)) #17 are missing in akoats, 6 have duplicate entries 55 to 61.

#remove older ids.
usgs_md <- left_join(usgs_md, akoats.usgs %>% select(Agency_ID, seq_id, Contact_person, Waterbody_type, Sensor_Placement, Sensor_accuracy)) %>% 
  filter(!(seq_id %in% c(1074, 1066, 1070, 1082, 1051, 1785)))


#older entries have unknown sensor accuracy
akoats.usgs %>% filter(seq_id %in% c(1074, 1075, 1066, 1067, 1070, 1071, 1082, 1083, 1051, 1670))
```



```{r usgs daily data}
usgs_dat <- readNWISdv(siteNumbers = usgs_sa$site_no, parameterCd = "00010", statCd = c("00001", "00002", "00003"))

#no data for Gakona River.
readNWISdv(siteNumbers = 15200000, parameterCd = "00010", statCd = c("00001", "00002", "00003"))

usgs_dat %>% 
  count(X_00010_00001_cd)

usgs_dat %>% 
  count(X_00010_00002_cd)

usgs_dat %>% 
  count(X_00010_00003_cd)

#get list of approved codes 
A_codes <- usgs_dat %>% 
  distinct(X_00010_00003_cd) %>% 
  filter(grepl("A", X_00010_00003_cd)) %>% 
  pull(X_00010_00003_cd)

# a lot of data where mean is accepted, but other values are not.
usgs_dat %>% 
  filter(!(X_00010_00001_cd %in% A_codes) | !(X_00010_00002_cd %in% A_codes), X_00010_00003_cd %in% A_codes)

usgs_dat %>% 
  count(site_no, X_00010_00003_cd %in% A_codes) %>% 
  pivot_wider(names_from = `X_00010_00003_cd %in% A_codes`, values_from = n)

usgs_dat %>% 
  filter(site_no == 15274600)

#not filtering on just accepted data for now, the only other code is provisional, no rejected data.
usgs_daily <- usgs_dat %>% 
  # filter(X_00010_00003_cd %in% A_codes) %>% 
  rename(SourceName = agency_cd,
         Agency_ID = site_no,
         sampleDate = Date,
         maxDT = X_00010_00001,
         minDT = X_00010_00002, 
         meanDT = X_00010_00003) %>% 
  mutate(SiteID = paste0("usgs_", Agency_ID)) %>% 
  select(SourceName, Agency_ID, sampleDate, minDT, maxDT, meanDT, SiteID)

#calculate mean from min and max when not provided and delete rows with no data.
usgs_daily <- usgs_daily %>% 
  mutate(meanDT = case_when(is.na(meanDT) & !is.na(maxDT) & !is.na(minDT) ~ (maxDT + minDT)/2,
                            TRUE ~ meanDT)) %>% 
  filter(!(is.na(meanDT) & is.na(minDT) & is.na(maxDT)))


usgs_daily %>% 
  group_by(Agency_ID) %>% 
  summarize(minyr = min(year(sampleDate)),
            maxyr = max(year(sampleDate))) %>% 
  left_join(usgs_temp %>% distinct(site_no, station_nm), by = c("Agency_ID" = "site_no"))

usgs_daily %>% 
  ggplot(aes(x = sampleDate, y = meanDT)) +
  geom_line() +
  facet_wrap(~SiteID)

```
  
  
Save daily data and metadata for AKSSF. Note that these are all sites and should be filtered to bb only when combining data for that project.


```{r save raw data}
source("W:/Github/AKSSF/helper_functions.R")

summary(usgs_daily)
usgs_daily %>% 
  distinct(SiteID) %>%
  left_join(usgs_md)

#gakona river missing
left_join(usgs_md, usgs_daily %>% group_by(SiteID) %>% summarize(meanT = mean(meanDT, na.rm = TRUE)))

usgs_md <- usgs_md %>% 
  filter(!Agency_ID == "15200000")

save_metadata_files(usgs_md, "usgs")

save_daily_files(usgs_daily, "usgs")
```


