---
title: "UW Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

# load packages
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(tidyverse)

```

# Metadata

AKOATS metadata

```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA", col_types = "text") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```

Focus on just getting lat/longs for a map. Read in all records, convert akoats id to numeric, which will remove the sites with multiple entries - add those in manually. Also, remove sites with missing info - which are the two lynx creek sites with unknown locations.

Start with files provided in 2017 for SASAP project. 

```{r}
uw.meta <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  select(StationName:Long) %>%
  mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  filter(!(is.na(Lat) & is.na(AKOATS_ID))) %>% 
  select(-Lat, -Long)

uw.meta <- uw.meta %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

uw.meta
```

Second spreadsheet linking station names in 2018-2020 data to akoats ids. Deal with duplicate akoats ids entered by Jackie manually.

```{r}

uw.meta2 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "AKOATS links") %>% 
    mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  rename(StationName = "UW_2018-2020 location") %>% 
  select(-Lat, -Long, -`...5`)

uw.meta2 <- uw.meta2 %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

#get sites from 2018-2020 spreadsheet that are new and bind to metadata for sites through 2017.
uw.new.sites <- uw.meta2[!(uw.meta2 %>% pull(StationName)) %in% (uw.meta %>% pull(StationName)),] %>% 
  filter(!StationName == "Kulik K-1")
  
uw.meta <- bind_rows(uw.meta, uw.new.sites)
  
#get lat/longs and waterbody name from akoats for uw sites with ids
uw.akoats.sites <- left_join(uw.meta %>% 
                               filter(!is.na(AKOATS_ID)) %>% 
                               select(StationName, AKOATS_ID), 
                             akoats.meta %>% 
                               select(AKOATS_ID, Waterbody_name, Latitude, Longitude), by = "AKOATS_ID")

uw.meta <- bind_rows(uw.meta %>% 
                       filter(is.na(AKOATS_ID)),
                     uw.akoats.sites) %>% 
  rename(SiteID = StationName) %>% 
  mutate(SourceName = "uwASP")

uw.meta

```

There are several sites (e.g. one AKOATS_ID or one lat/long) that have several site ids in the data. These need to be fixed here and in the data so that one location = one time series.


```{r}
uw.meta %>% count(AKOATS_ID) %>% arrange(desc(n))
uw.meta %>% arrange(AKOATS_ID)

uw.meta <- uw.meta %>% 
  filter(!SiteID %in% c("Aleknagik Agulowak River", "Nerka Lynx Creek Upper", "Nerka Kema Creek Lower", 
                        "Nerka Kema Creek Upper","Nerka Agulukpak River", "Nerka Elva Creek Lower", 
                        "Nerka Coffee Creek", "Little Togiak Creek Lower")) 



uw.meta %>% 
  saveRDS("output/uw_metadata.rds")
```



# Data


## Data files through 2017 - SASAP project

Read in data from folder "Jackie Carter UW". These are the 51 files of stream temperature data originally provided for SASAP project, but never archived on KNB. Data are through 2017. Keep file name when reading in data in case it is needed to inspect date-time formats or other formatting problems. Also, keep other information from Jackie, such as Lake and sample type. We will want sample type to indicate logger accuracy for AKTEMP. And, lake information is a good check that we are locating sites in the correct region of the Wood River watershed. Removed the original Lynx Creek data file in this folder. Jackie provided a new version with only 5 station names and correct locations later.
Jackie provided updated data files for Pick Creek, Eagle Creek, and Wood River to fix time issues.


```{r}
uw.data <- list.files(path = ("data/Jackie Carter UW/Jackie Carter/"),
                      pattern = "*.csv", 
                      full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
           mutate(file_name = gsub(".csv","",basename(x)))) %>%
  select(-X9, -YearSampled) %>% 
  mutate(Temperature = as.numeric(Temperature))

```

Format dates and times. Most dates are "%m/%d/%y" 2-digit year. But, Big Whitefish and Little Whitefish have different format, "%d-%b-%y". 

```{r}
uw.data %>% 
  group_by(StationName) %>%
  slice_head() %>% 
  arrange(StationName)

uw.data <- uw.data %>% 
  mutate(sampleDate = parse_date_time(Date, orders = c("dby", "mdy")),
         sampleTime = as_hms(parse_date_time(Time, orders = c("IMS p", "HMS"))))

```

50 files were read in and there are a total of 54 sites with data.

```{r}
uw.data %>% 
  count(file_name, StationName, year = year(sampleDate)) %>%
  arrange(year, file_name, StationName) %>% 
  pivot_wider(names_from = year, values_from = n)

```
Are there duplicates in this data set or are they from combining across two data requests?

```{r}
uw.data %>%
  distinct() %>% 
  count(StationName, sampleDate, sampleTime) %>% 
  filter(n > 1)
  arrange(desc(n))

uw.data %>% 
  group_by(StationName, sampleDate, sampleTime) %>% 
  mutate(id = row_number()) %>%
  select(StationName, sampleDate, sampleTime, id, Temperature) %>% 
  pivot_wider(names_from = id, values_from = Temperature) %>% 
  mutate(diff = abs(`1`-`2`)) %>% 
  filter(!is.na(diff)) %>% 
  group_by(StationName) %>% 
  summarize(mean = mean(diff),
            sd = sd(diff),
            count = n())
```


## Data files received in November 2020

Becky emailed with Jackie in November 2020 to get metadata information for the 51 data files sent over in 2017 for the SASAP project and also received some new data files.

* data file for sites on Lynx Creek - 5 stations
* data file for Aleknagik Bear Creek for data through 2017
* data file for all stream sites from 2018 through 2020  



```{r}
uw.lynx <- read_csv("data/Jackie Carter UW/Jackie Carter - 2020 data request/UW-FRI_LynxCreekALL_Temps_JC.csv", 
                    col_types = cols(.default = "c")) %>%
  mutate(file_name = "UW-FRI_LynxCreekALL_Temps_JC") %>%
  transform(sampleDate = as.Date(mdy(Date)),
            sampleTime = as_hms(parse_date_time(Time, "I:M:S p")),
            Temperature = as.numeric(Temperature)) 

uw.aleknagik.bear <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/froRSS_AleknagikBearTemps_2008-2017.xlsx") %>%
  mutate(file_name = "froRSS_AleknagikBearTemps_2008-2017",
         sampleTime = as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature)) 

uw.post2017 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "2018-2020 data", cell_rows(1:329775)) %>%
  mutate(sampleTime = hms::as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature),
         file_name = "forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps")

```


Join the multiple data frames in to single object. Start with all information, then pare down to just the fields that we need. For AKTEMP, may try to extract more metadata information from measurement type field to get sensor accuracy for data table.

```{r}
keep <- c("file_name", "Lake", "StationName", "SampleType", "MeasurementType", "sampleDate", "sampleTime", "Temperature")

uw.data.2 <- bind_rows(uw.data %>% select(one_of(keep)),
                       uw.lynx %>% select(one_of(keep)),
                       uw.aleknagik.bear %>% select(one_of(keep)),
                       uw.post2017 %>% select(one_of(keep))) %>% 
  rename(SiteID = StationName) %>% 
  mutate(sampleDate = as.Date(sampleDate))

uw.data.2 %>% 
  summary()

# remove extraneous objects
rm(uw.data, uw.lynx, uw.aleknagik.bear, uw.post2017)
```


Fix duplicate SiteIDs that go to same location (see metadata section above for Jackie's notes on this). Also, Jackie couldn't find locations for two sites on Lynx Creek - beach and middle so remove those time series from the data frame.

```{r}
uw.data.2 %>% distinct(SiteID) %>% arrange(SiteID)

uw.data.2 <- uw.data.2 %>% 
  mutate(SiteID = case_when(SiteID == "Aleknagik Agulowak River" ~ "Agulowak River",
                            SiteID == "Nerka Lynx Creek Upper" ~ "Nerka Lynx Lake Tributary",
                            SiteID %in% c("Nerka Kema Creek Lower", "Nerka Kema Creek Upper") ~ "Nerka Kema Creek",
                            SiteID =="Nerka Agulukpak River" ~ "Agulukpak River",
                            SiteID =="Nerka Elva Creek Lower" ~ "Nerka Elva Creek",
                            SiteID =="Nerka Coffee Creek" ~ "Nerka Chamee Creek",
                            SiteID =="Little Togiak Creek Lower" ~ "Little Togiak Creek",
                            TRUE ~ SiteID)) %>% 
  filter(!SiteID %in% c("Nerka Lynx Creek Middle", "Nerka Lynx Creek Beach"))

sites.md <- uw.meta %>% pull(SiteID)
sites.dat <- uw.data.2 %>% distinct(SiteID) %>% pull(SiteID)

sites.md[!sites.md %in% sites.dat]
sites.dat[!sites.dat %in% sites.md]

```

Add a date-time and year field, which will be needed for data QA. Also can use to identify and remove duplicates in data.

```{r}
uw.data.2 <- uw.data.2 %>% 
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M"),
         year = year(sampleDate))
  
uw.data.2 %>%
  distinct() %>% 
  count(SiteID, dt) %>% 
  count(SiteID, n) %>% 
  arrange(desc(n))

```


Summary csv to add as attributes to leaflet map.

```{r}
uw.data.2 %>% 
  group_by(SiteID) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/uw_data_summ.rds")

```

Save a copy of the data for a table and figure.

```{r}
uw.data.2

saveRDS(uw.data.2, "output/uw_data.rds")
```





## Tim Cline's data from Zotero

Six files each for a separate year of data. Sites are different columns in the dataset.

```{r}
tc.files <- list.files("S:\\Stream Temperature Data\\Tim Cline UW", full.names = TRUE)

tc.11 <- read_csv(tc.files[1]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.12 <- read_csv(tc.files[2]) %>%
  mutate(Date = parse_date_time(Day, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad, -Day) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.13 <- read_csv(tc.files[3]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.14 <- read_csv(tc.files[4]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.15 <- read_csv(tc.files[5]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.16 <- read_csv(tc.files[6]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.dat <- bind_rows(tc.11, tc.12, tc.13, tc.14, tc.15, tc.16)

```

Plot data

```{r}
tc.dat %>% 
  mutate(day = format(Date, "%m-%d")) %>% 
  ggplot(aes(x = as.Date(day, format = "%m-%d"), y = Temperature, color = as.factor(year(Date)))) +
  geom_line() +
  facet_wrap(~SiteID)

```

