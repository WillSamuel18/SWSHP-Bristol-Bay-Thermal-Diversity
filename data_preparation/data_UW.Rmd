---
title: "UW Water Temp Data Pre-2017"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

# load packages
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(tidyverse)

```

# Metadata

Focus on just getting lat/longs for a map. Read in all records, convert akoats id to numeric, which will remove the sites with multiple entries - add those in manually. Also, remove sites with missing info - which are the two lynx creek sites with unknown locations.

Start with files provided in 2017 for SASAP project. 

```{r}
uw.meta <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  select(StationName:Long) %>%
  mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  filter(!(is.na(Lat) & is.na(AKOATS_ID))) %>% 
  select(-Lat, -Long)

uw.meta <- uw.meta %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

uw.meta
```

Second spreadsheet linking station names in 2018-2020 data to akoats ids. Deal with duplicate akoats ids entered by Jackie manually.

```{r}

uw.meta2 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "AKOATS links") %>% 
    mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  rename(StationName = "UW_2018-2020 location") %>% 
  select(-Lat, -Long, -`...5`)

uw.meta2 <- uw.meta2 %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

#get sites from 2018-2020 spreadsheet that are new and bind to metadata for sites through 2017.
uw.new.sites <- uw.meta2[!(uw.meta2 %>% pull(StationName)) %in% (uw.meta %>% pull(StationName)),] %>% 
  filter(!StationName == "Kulik K-1")
  
uw.meta <- bind_rows(uw.meta, uw.new.sites)
  
#get lat/longs and waterbody name from akoats for uw sites with ids
uw.akoats.sites <- left_join(uw.meta %>% 
                               filter(!is.na(AKOATS_ID)) %>% 
                               select(StationName, AKOATS_ID), 
                             akoats.meta %>% 
                               select(AKOATS_ID, Waterbody_name, Latitude, Longitude), by = "AKOATS_ID")

uw.meta <- bind_rows(uw.meta %>% 
                       filter(is.na(AKOATS_ID)),
                     uw.akoats.sites) %>% 
  rename(SiteID = StationName) %>% 
  mutate(SourceName = "uwASP")

uw.meta

```

There are several sites (e.g. one AKOATS_ID or one lat/long) that have several site ids in the data. These need to be fixed here and in the data so that one location = one time series.


```{r}
uw.meta %>% count(AKOATS_ID) %>% arrange(desc(n))
uw.meta %>% arrange(AKOATS_ID)

uw.meta <- uw.meta %>% 
  filter(!SiteID %in% c("Aleknagik Agulowak River", "Nerka Lynx Creek Upper", "Nerka Kema Creek Lower", "Nerka Kema Creek Upper",
                        "Nerka Agulukpak River", "Nerka Elva Creek Lower", "Nerka Coffee Creek", "Little Togiak Creek Lower")) 



uw.meta %>% 
  saveRDS("output/uw_metadata.rds")
```



# Data


## Data files through 2017 - SASAP project

Read in data from folder "Jackie Carter UW". These are the 51 files of stream temperature data originally provided for SASAP project, but never archived on KNB. Data are through 2017. Keep file name when reading in data in case it is needed to inspect date-time formats or other formatting problems. Also, keep other information from Jackie, such as Lake and sample type. We will want sample type to indicate logger accuracy for AKTEMP. And, lake information is a good check that we are locating sites in the correct region of the Wood River watershed. Removed the original Lynx Creek data file in this folder. Jackie provided a new version with only 5 station names and correct locations later.
Jackie provided updated data files for Pick Creek, Eagle Creek, and Wood River to fix time issues.


```{r}
uw.data <- list.files(path = ("data/Jackie Carter UW/Jackie Carter/"),
                      pattern = "*.csv", 
                      full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
           mutate(file_name = gsub(".csv","",basename(x)))) %>%
  select(-X9, -YearSampled) %>% 
  mutate(Temperature = as.numeric(Temperature))

```

Format dates and times. Most dates are "%m/%d/%y" 2-digit year. But, Big Whitefish and Little Whitefish have different format, "%d-%b-%y". 

```{r}
uw.data %>% 
  group_by(StationName) %>%
  slice_head() %>% 
  arrange(StationName)

uw.data <- uw.data %>% 
  mutate(sampleDate = parse_date_time(Date, orders = c("dby", "mdy")),
         sampleTime = as_hms(parse_date_time(Time, orders = c("IMS p", "HMS"))))

```

50 files were read in and there are a total of 54 sites with data.

```{r}
uw.data %>% 
  count(file_name, StationName, year = year(sampleDate)) %>%
  arrange(year, file_name, StationName) %>% 
  pivot_wider(names_from = year, values_from = n)

```


## Date files received in November 2020

Becky emailed with Jackie in November 2020 to get metadata information for the 51 data files sent over in 2017 for the SASAP project and also received some new data files.

* data file for sites on Lynx Creek - 5 stations
* data file for Aleknagik Bear Creek for data through 2017
* data file for all stream sites from 2018 through 2020  



```{r}
uw.lynx <- read_csv("data/Jackie Carter UW/Jackie Carter - 2020 data request/UW-FRI_LynxCreekALL_Temps_JC.csv", 
                    col_types = cols(.default = "c")) %>%
  mutate(file_name = "UW-FRI_LynxCreekALL_Temps_JC") %>%
  transform(sampleDate = as.Date(mdy(Date)),
            sampleTime = as_hms(parse_date_time(Time, "I:M:S p")),
            Temperature = as.numeric(Temperature)) 

uw.aleknagik.bear <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/froRSS_AleknagikBearTemps_2008-2017.xlsx") %>%
  mutate(file_name = "froRSS_AleknagikBearTemps_2008-2017",
         sampleTime = as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature)) 

uw.post2017 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "2018-2020 data", cell_rows(1:329775)) %>%
  mutate(sampleTime = hms::as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature),
         file_name = "forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps")

```


Join the multiple data frames in to single object. Start with all information, then pare down to just the fields that we need. For AKTEMP, may try to extract more metadata information from measurement type field to get sensor accuracy for data table.

```{r}
keep <- c("file_name", "Lake", "StationName", "SampleType", "MeasurementType", "sampleDate", "sampleTime", "Temperature")

uw.data.2 <- bind_rows(uw.data %>% select(one_of(keep)),
                       uw.lynx %>% select(one_of(keep)),
                       uw.aleknagik.bear %>% select(one_of(keep)),
                       uw.post2017 %>% select(one_of(keep))) %>% 
  rename(SiteID = StationName) %>% 
  mutate(sampleDate = as.Date(sampleDate))

uw.data.2 %>% 
  summary()

# remove extraneous objects
rm(uw.data, uw.lynx, uw.aleknagik.bear, uw.post2017)
```

Flag erroneous low and high values until we can do a proper qa.

```{r}
uw.data.2 <- uw.data.2 %>% 
  mutate(UseData = case_when(Temperature < -1 ~ 0,
                             Temperature > 25 ~ 0,
                             TRUE ~ 1))
```

Fix duplicate SiteIDs that go to same location (see metadata section above for Jackie's notes on this). Also, Jackie couldn't find locations for two sites on Lynx Creek - beach and middle so remove those time series from the data frame.

```{r}
uw.data.2 %>% distinct(SiteID) %>% arrange(SiteID)

uw.data.2 <- uw.data.2 %>% 
  mutate(SiteID = case_when(SiteID == "Aleknagik Agulowak River" ~ "Agulowak River",
                            SiteID == "Nerka Lynx Creek Upper" ~ "Nerka Lynx Lake Tributary",
                            SiteID %in% c("Nerka Kema Creek Lower", "Nerka Kema Creek Upper") ~ "Nerka Kema Creek",
                            SiteID =="Nerka Agulukpak River" ~ "Agulukpak River",
                            SiteID =="Nerka Elva Creek Lower" ~ "Nerka Elva Creek",
                            SiteID =="Nerka Coffee Creek" ~ "Nerka Chamee Creek",
                            SiteID =="Little Togiak Creek Lower" ~ "Little Togiak Creek",
                            TRUE ~ SiteID)) %>% 
  filter(!SiteID %in% c("Nerka Lynx Creek Middle", "Nerka Lynx Creek Beach"))

sites.md <- uw.meta %>% pull(SiteID)
sites.dat <- uw.data.2 %>% distinct(SiteID) %>% pull(SiteID)

sites.md[!sites.md %in% sites.dat]
sites.dat[!sites.dat %in% sites.md]

```



Summary csv to add as attributes to leaflet map.

```{r}
uw.data.2 %>% 
  group_by(SiteID) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/uw_data_summ.rds")

```

Save a copy of the data for a table and figure.

```{r}
uw.data.2

saveRDS(uw.data.2, "output/uw_data.rds")
```



# Data Review

(Note nothing below has been reviewed after getting new data, this was original attempt by Ben - good suggestion to develop a shiny app with plotly object for reviewing data.)

Perform a quick visualization and summary table to see extent and form of original data.

* Summary table

```{r}
# create data summary table
uw.data.summary <- uw.data %>%
  mutate(year = year(sampleDate)) %>%
  group_by(SiteID,year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

uw.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)
    
```

<br>

* Visualization

```{r}
uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Original Logger Data by Site and Year - Daily Mean")

```

<br>

Notes:

* Among the whole data set, there are some max temps > 50 C, indicating some logger malfunction and/or air exposure.  There is also a minTemp of -21.5 C

* Different types of instruments were employed to collect UW water temp data over the years, including iButtons, Stage gauges, Tidbits, and Level loggers.

<br>

***

Identify and eliminate likely erroneous data 

* Step 1: Start by visually examining datasets that include observations >25 C.  Does anything about these datasets suggest exposure or malfunction? (Or temperatures recorded pre/post deployment in water?)


<br>

Plot 1 
```{r}
# remove extraneous temporary objects from previous steps
rm(uw.data.summary,uw.metadata)

# identify datasets that include observations >25C
above.25 <- uw.data %>%
  filter(Temperature >= 25) %>%
  mutate(year = year(sampleDate)) %>%
  select(SiteID,year) %>%
  distinct()

# plot reduced dataset of site/years that contain >25C observations -- daily means
uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data by Site and Year - Datasets w/ >25 C")

```
<br>

Process to identify and remove erroneous data:

* Visually identify data that does not seem reasonable
* Plot unreasonable datasets by individual year and site
* Use an anti_join script process to assign a "useData = 0" value to erroneous data observations by start and end dateTime

1.) Examine & remove bad data from data sets w/ observations >25C
```{r}
# list of watersheds w/ 25C observations to examine w/ ggplotly object
l <- above.25 %>%
  select(-year) %>%
  distinct()

y <- c("2020")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(above.25) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Nerka Agulukpak River"
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data (Example)")
  )
```

<br>

2.) Examine & remove bad data from data sets w/ known suspect data

Notes, from "data/Jackie Carter UW/UA email saved at "data/Jackie Carter UW/UA Mail - Bristol Bay Temperature Data.pdf": "I have noticed some weird stuff going on with some of the older data. I think it may be an artifact of how our former database manager imported the data (bulk import, no distinction between logger type so there are multiple readings per hour but they aren't really as similar as one might expect). The known affected locations and years are: Aleknagik Bear Creek 2010-2013, Nerka Fenno Creek 2010-2013, and Nerka Pick Creek 2006-2012."

Examine the three data sets specified above
```{r}
# potentially erroneous data sets:
# Nerka Fenno Creek 2010-2013
# Nerka Pick Creek 2006-2012
# Aleknagik Bear Creek 2010-2013 

# list of watersheds to examine w/ ggplotly object
l <- c("Nerka Fenno Creek","Nerka Pick Creek","Aleknagik Bear Creek")
z <- uw.data %>%
  filter(SiteID %in% l) %>%
  select(SiteID) %>%
  distinct()

# specify year or years to display here
y <- c("2006")

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  inner_join(z) %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == "Aleknagik Bear Creek"
         # hashtag line below if want to see all years for a site
         #,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data (Example)")
  )

```

<br>

3.) Examine & remove bad data from all other datasets (e.g. exposure data)

<br>

```{r}

# list of all watersheds to examine w/ ggplotly object
# do a quick manual run-through this list one at a time to spot visually apparent logger exposure(s)
l <- data.frame(unique(uw.data$SiteID))

# specify year or years to display here
y <- c("2011")
# specify site
site <- "Nerka Rainbow Creek"

# create ggplotly chart
ggplotly(
  p <- uw.data %>%
  select(-sampleTime) %>%
  mutate(year = as.factor(year(sampleDate)),
         day = yday(sampleDate)) %>%
  group_by(day,SiteID,year) %>%
  summarise(daily_mean_Temperature = mean(Temperature)) %>%
  
  # modified SiteID one at a time here to visually inspect datasets
  filter(SiteID == site
         # hashtag line below if want to see all years for a site
         ,year %in% y
         ) %>%
  
  ggplot(aes(day,daily_mean_Temperature, color = year)) +
  geom_point() +
  facet_wrap(. ~ SiteID) +
  ggtitle("Daily Mean Data")
  )

# note: creating a Shiny app for this job would be really useful!  e.g., to be able to easily toggle between various sites and years
```


If further data is is identified to be flagged, edit the csv file at "data/flagged_data/UW_flagged_data.csv"


<br>


Apply flags for good/bad data
```{r}
# created table of erroneous data to be flagged/excised
## @ data/flagged_data/UW_flagged_data.csv

# use flagged data table to assign "useData = 0" to erroneous observations


# read in start/end dates for flagged data
uw_flags <- read_csv("data_cleaning/flagged_data/UW_flagged_data.csv") %>%
  filter(!is.na(day_start)) %>%
  select(-Notes) 




# create table of data to be flagged w/ "useData = 0"
uw_flagged_data <- uw.data %>% 
  mutate(year = year(sampleDate)) %>%
  inner_join(uw_flags,by = c("SiteID","year")) %>%
  mutate(day = yday(sampleDate)) %>%
  filter(day >= day_start & day <= day_end) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_end,-year,-day)




############ NOTE ###############

### Note: the "inner_join" step of the above pipe is behaving oddly.  In this step we want to create a table of all rows to be flagged with useData = 0.  However only a (seemingly random) subset of the sites, years, and days specified in "UW_flagged_data.csv" are being recognized by inner_join.  As a result, not all of the data specified to be flagged as useData = 0 is being flagged.

# Checked through spelling and column classes several times to ensure matches.  As of 11/21/20 not yet diagnosed what is happening at this step.  

# Verbal description of code pipes intended to label data with useData = 0:
# A.) Periods of daily means ID'd to be excluded are manually specified in "UW_flagged_data.csv" by SiteID, start day, end day, and year.

# B.) A dataframe of data to be labeled with useData = 0 is generated using an inner_join pipe with the manually ID'd data.

# C.) A dataframe of data to be labeled with useData = 1 in generated using an anti_join pipe; labeing all data not specified as useData = 0 as useData = 1.

# D.) The two flagged dataframes are joined using bind_rows.

# Step B of the above procedure is not functining.  E.g., search for "Aleknagik Ice Creek" in "uw_flags" and "uw_flagged_data" dataframes.
##################################




# apply flags
uw_nonflagged_data <- anti_join(uw.data,uw_flagged_data,by = c("SiteID","sampleDate","sampleTime")) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
uw.data <- bind_rows(uw_flagged_data,uw_nonflagged_data)

# remove extraneous objects
rm(above.25,l,p,uw_flagged_data,uw_nonflagged_data,alk)

# how much data is being flagged?
uw.data %>%
  group_by(useData) %>%
  summarise(ct = n())

# calc % of flagged data
# (11457/(1089838 +11457))*100
# 1.04% of data is flagged

```


<br>

To do: try using functions in "Temp_flags_function.R" to IDeven more bad data.

<br>


11/10/20: Ready to output file as soon as we get metadata!

Export csv of combined datasets, with newly identified "do not use data" signified with a "0" in the useData column, to data folder associated with this .Rproj
```{r}
# prep for export
 uw.data <- uw.data %>%
   select(-data_SiteID) 

# reorder columns
x <- uw.data %>% select(AKOATS_ID,SiteID,sampleDate,sampleTime,Temperature,useData)

# export csv
write.csv(x,"output/UW.csv", row.names = F)

```

