---
title: "UW Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

Sent Daniel email summarizing TC dataset versus sites in Jackie's database - 3/5/21. Possibly just move forward with additional years of data for sites that Tim originally used for thermal sensitivity.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

# load packages
library(googledrive)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(gridExtra)
library(tidyverse)
```

# Metadata

AKOATS metadata

```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA", col_types = "text") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```

Focus on just getting lat/longs for a map. Read in all records, convert akoats id to numeric, which will remove the sites with multiple entries - add those in manually. Also, remove sites with missing info - which are the two lynx creek sites with unknown locations.

Start with files provided in 2017 for SASAP project. 

```{r}
uw.meta <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  select(StationName:Long) %>%
  mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  filter(!(is.na(Lat) & is.na(AKOATS_ID))) %>% 
  select(-Lat, -Long)

uw.meta <- uw.meta %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

uw.meta
```

Second spreadsheet linking station names in 2018-2020 data to akoats ids. Deal with duplicate akoats ids entered by Jackie manually.

```{r}

uw.meta2 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "AKOATS links") %>% 
    mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  rename(StationName = "UW_2018-2020 location") %>% 
  select(-Lat, -Long, -`...5`)

uw.meta2 <- uw.meta2 %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

#get sites from 2018-2020 spreadsheet that are new and bind to metadata for sites through 2017.
uw.new.sites <- uw.meta2[!(uw.meta2 %>% pull(StationName)) %in% (uw.meta %>% pull(StationName)),] %>% 
  filter(!StationName == "Kulik K-1")
  
uw.meta <- bind_rows(uw.meta, uw.new.sites)
  
#get lat/longs and waterbody name from akoats for uw sites with ids
uw.akoats.sites <- left_join(uw.meta %>% 
                               filter(!is.na(AKOATS_ID)) %>% 
                               select(StationName, AKOATS_ID), 
                             akoats.meta %>% 
                               select(AKOATS_ID, Waterbody_name, Latitude, Longitude), by = "AKOATS_ID")

uw.meta <- bind_rows(uw.meta %>% 
                       filter(is.na(AKOATS_ID)),
                     uw.akoats.sites) %>% 
  rename(SiteID = StationName) %>% 
  mutate(SourceName = "uwASP")

uw.meta

```

There are several sites (e.g. one AKOATS_ID or one lat/long) that have several site ids in the data. These need to be fixed here and in the data so that one location = one time series.


```{r}
uw.meta %>% count(AKOATS_ID) %>% arrange(desc(n))
uw.meta %>% arrange(AKOATS_ID)

uw.meta <- uw.meta %>% 
  filter(!SiteID %in% c("Aleknagik Agulowak River", "Nerka Lynx Creek Upper", "Nerka Kema Creek Lower", 
                        "Nerka Kema Creek Upper","Nerka Agulukpak River", "Nerka Elva Creek Lower", 
                        "Nerka Coffee Creek", "Little Togiak Creek Lower")) 



uw.meta %>% 
  saveRDS("output/uw_metadata.rds")
```



# Data


## Data files through 2017 - SASAP project

Read in data from folder "Jackie Carter UW". These are the 51 files of stream temperature data originally provided for SASAP project, but never archived on KNB. Data are through 2017. Keep file name when reading in data in case it is needed to inspect date-time formats or other formatting problems. Also, keep other information from Jackie, such as Lake and sample type. We will want sample type to indicate logger accuracy for AKTEMP. And, lake information is a good check that we are locating sites in the correct region of the Wood River watershed. Removed the original Lynx Creek data file in this folder. Jackie provided a new version with only 5 station names and correct locations later.
Jackie provided updated data files for Pick Creek, Eagle Creek, and Wood River to fix time issues.


```{r 2017 data files}
uw.data <- list.files(path = ("data/Jackie Carter UW/Jackie Carter/"),
                      pattern = "*.csv", 
                      full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
           mutate(file_name = gsub(".csv","",basename(x)))) %>%
  select(-X9, -YearSampled) %>% 
  mutate(Temperature = as.numeric(Temperature))

```

Format dates and times. Most dates are "%m/%d/%y" 2-digit year. But, Big Whitefish and Little Whitefish have different format, "%d-%b-%y". 

```{r 2017 dates and times}
uw.data %>% 
  group_by(StationName) %>%
  slice_head() %>% 
  arrange(StationName)

uw.data <- uw.data %>% 
  mutate(sampleDate = parse_date_time(Date, orders = c("dby", "mdy")),
         sampleTime = as_hms(parse_date_time(Time, orders = c("IMS p", "HMS"))))

```

50 files were read in and there are a total of 54 sites with data.

```{r 2017 summary}

uw.data %>% 
  count(file_name, StationName, year = year(sampleDate)) %>%
  arrange(year, file_name, StationName) %>% 
  pivot_wider(names_from = year, values_from = n)

uw.data %>% 
  count(MeasurementType, file_name, year = year(sampleDate)) 

```




## Data files received in November 2020

Becky emailed with Jackie in November 2020 to get metadata information for the 51 data files sent over in 2017 for the SASAP project and also received some new data files.

* data file for sites on Lynx Creek - 5 stations
* data file for Aleknagik Bear Creek for data through 2017
* data file for all stream sites from 2018 through 2020  

```{r 2020 data files}
uw.lynx <- read_csv("data/Jackie Carter UW/Jackie Carter - 2020 data request/UW-FRI_LynxCreekALL_Temps_JC.csv", 
                    col_types = cols(.default = "c")) %>%
  mutate(file_name = "UW-FRI_LynxCreekALL_Temps_JC") %>%
  transform(sampleDate = as.Date(mdy(Date)),
            sampleTime = as_hms(parse_date_time(Time, "I:M:S p")),
            Temperature = as.numeric(Temperature)) 

uw.aleknagik.bear <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/froRSS_AleknagikBearTemps_2008-2017.xlsx") %>%
  mutate(file_name = "froRSS_AleknagikBearTemps_2008-2017",
         sampleTime = as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature)) 

uw.post2017 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "2018-2020 data", cell_rows(1:329775)) %>%
  mutate(sampleTime = hms::as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature),
         file_name = "forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps")

```


Join the multiple data frames in to single object. Start with all information, then pare down to just the fields that we need. For AKTEMP, may try to extract more metadata information from measurement type field to get sensor accuracy for data table.

```{r bind all data}
keep <- c("file_name", "Lake", "StationName", "SampleType", "MeasurementType", "sampleDate", "sampleTime", "Temperature")

uw.data2 <- bind_rows(uw.data %>% select(one_of(keep)),
                       uw.lynx %>% select(one_of(keep)),
                       uw.aleknagik.bear %>% select(one_of(keep)),
                       uw.post2017 %>% select(one_of(keep))) %>% 
  rename(SiteID = StationName) %>% 
  mutate(sampleDate = as.Date(sampleDate))

uw.data2 %>% 
  summary()

# remove extraneous objects
rm(uw.data, uw.lynx, uw.aleknagik.bear, uw.post2017)
```


Fix duplicate SiteIDs that go to same location (see metadata section above for Jackie's notes on this). Also, Jackie couldn't find locations for two sites on Lynx Creek - beach and middle so remove those time series from the data frame.

```{r collapse SiteIDs}
uw.data2 %>% distinct(SiteID) %>% arrange(SiteID)

uw.data2 <- uw.data2 %>% 
  mutate(SiteID = case_when(SiteID == "Aleknagik Agulowak River" ~ "Agulowak River",
                            SiteID == "Nerka Lynx Creek Upper" ~ "Nerka Lynx Lake Tributary",
                            SiteID %in% c("Nerka Kema Creek Lower", "Nerka Kema Creek Upper") ~ "Nerka Kema Creek",
                            SiteID =="Nerka Agulukpak River" ~ "Agulukpak River",
                            SiteID =="Nerka Elva Creek Lower" ~ "Nerka Elva Creek",
                            SiteID =="Nerka Coffee Creek" ~ "Nerka Chamee Creek",
                            SiteID =="Little Togiak Creek Lower" ~ "Little Togiak Creek",
                            TRUE ~ SiteID)) %>% 
  filter(!SiteID %in% c("Nerka Lynx Creek Middle", "Nerka Lynx Creek Beach"))

#lat/longs for all 60 sites in data
uw.data2 %>% 
  distinct(SiteID) %>% 
  left_join(uw.meta)

#also 60 sites in metadata so they are matching.
uw.meta

```

Add a date-time and year field, which will be needed for data QA. Also can use to identify and remove duplicates in data.

```{r add dt and year fields}
uw.data2 <- uw.data2 %>% 
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M"),
         year = year(sampleDate))
```

# Review Data

Summary of sites with many duplicate values and the mean and sd of the differences between the duplicate measurements. A couple of sites with either duplicate loggers or multiple files. Check into this and see what is going on here. These are all the sites across both data requests. Note that some of these are ibutton temps and some have very coarse sample intervals (> 2 hr). Possibly filter to remove those first.

```{r duplicate measurements}
uw.data2 %>% 
  group_by(SiteID, sampleDate, sampleTime) %>% 
  mutate(id = row_number()) %>%
  select(SiteID, sampleDate, sampleTime, id, Temperature) %>% 
  pivot_wider(names_from = id, values_from = Temperature) %>% 
  mutate(diff = abs(`1`-`2`)) %>% 
  filter(!is.na(diff)) %>% 
  group_by(SiteID) %>% 
  summarize(mean = mean(diff),
            sd = sd(diff),
            count = n())


uw.data2 %>% 
  group_by(SiteID, sampleDate, sampleTime) %>% 
  mutate(id = row_number()) %>%
  select(SiteID, sampleDate, sampleTime, id, Temperature) %>% 
  pivot_wider(names_from = id, values_from = Temperature) %>% 
  mutate(diff = abs(`1`-`2`)) %>% 
  filter(!is.na(diff), SiteID == "Nerka Fenno Creek") %>% 
  ungroup() %>% 
  summarize(min(sampleDate), max(sampleDate))
```

Look at logger type and measurement frequency. Possibly we should only focus on the newer data.

```{r}
source("W:/Github/AKSSF/helper_functions.R")

uw.data2 <- temp_msmt_freq(uw.data2)

uw.data2 %>% 
  ungroup() %>% 
  count(MeasurementType, file_name, SiteID, year = year(sampleDate), mode_diff) %>% 
  arrange(file_name, year)

uw.data2 %>% 
  filter(SiteID == "Beverley Uno Creek", mode_diff == 1)
```



# Save data


Summary csv to add as attributes to leaflet map.

```{r eval = FALSE}
uw.data2 %>% 
  group_by(SiteID) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/uw_data_summ.rds")

```

Save a copy of the data for a table and figure.

```{r eval = FALSE}
uw.data2

saveRDS(uw.data2, "output/uw_data.rds")
```





# Tim Cline's data from Zotero

Six files each for a separate year of data. Sites are different columns in the dataset.
Data archived on Zenodo: https://zenodo.org/record/3523300#.YEKEN45KhaQ
Unfortunately, the site names don't match Jackie's database and there are no lat/longs. I'll need to work with Jackie to try and get the most current years of data for these sites.

```{r Tim Cline data}
tc.files <- list.files("S:\\Stream Temperature Data\\Tim Cline UW", full.names = TRUE)
tc.files <- tc.files[!grepl("Climate", tc.files)]

tc.11 <- read_csv(tc.files[1]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.12 <- read_csv(tc.files[2]) %>%
  mutate(Date = parse_date_time(Day, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad, -Day) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.13 <- read_csv(tc.files[3]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.14 <- read_csv(tc.files[4]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.15 <- read_csv(tc.files[5]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.16 <- read_csv(tc.files[6]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.dat <- bind_rows(tc.11, tc.12, tc.13, tc.14, tc.15, tc.16) 
tc.dat %>% 
  distinct(SiteID)

```

Plot data

```{r TC data daily time series}
tc.dat %>% 
  mutate(day = format(Date, "%m-%d")) %>% 
  ggplot(aes(x = as.Date(day, format = "%m-%d"), y = Temperature, color = as.factor(year(Date)))) +
  geom_line() +
  facet_wrap(~SiteID)

tc.dat %>% distinct(SiteID)

summary(tc.dat)
```


```{r merge TC and UW site names}

tc.sites <- tc.dat %>% distinct(SiteID) %>% pull(SiteID) 
uw.sites <- uw.data2 %>% ungroup() %>% distinct(SiteID) %>% arrange(SiteID) %>% pull(SiteID)

siteMerge <- data.frame()
for(i in tc.sites) {
  uwIn = grep(i, uw.sites)
  #figure out what to do when can't be found. need to skip and population new row with NA
  if(length(uwIn > 0)) {
    newrow <- data.frame(tc = i, uw = uw.sites[uwIn])
  } else {
    newrow <- data.frame(tc = i, uw = NA)
  } 
  siteMerge <- bind_rows(siteMerge, newrow)
}

siteMerge
```

```{r eval = FALSE}
# write_csv(siteMerge, path = "output/UW_site_merge.csv")
```

Manually enter in some of the names in Jackie's dataset that logically match Tim's dataset. Talked with Daniel and cleared up some of the matches: Bear = Aleknagik Bear (NerkaBear already in Tim's dataset), Moose = Beverly Moose, Whitefish = Big Whitefish. The other sites with just one year of data in Tim's dataset, but not in the database are from P. Lisi's research and can be dropped since they didn't continue those sites (ChauekX and UpnukXX). Still not sure about MooseSpit because I forgot to ask Daniel, but emailed him about it.

```{r TC and UW data summaries}
#note that bear, beaver, and moose 
siteMerge2 <- read_csv("output/UW_site_merge_rs.csv")

jul.range <- tc.dat %>% 
  distinct(Date) %>% 
  mutate(jd = format(Date, "%j")) %>% 
  summarize(range(jd))

tc.summ <- tc.dat %>% 
  count(SiteID, year = year(Date)) %>%
  mutate(Source = "Tim Cline") #%>% 
  pivot_wider(names_from = year, values_from = n, names_prefix = "TC_") %>% 
  select(SiteID, TC_2011, TC_2012, TC_2013, TC_2014, TC_2015, TC_2016) %>% 
  select(SiteID, TC_2011:TC_2016)

uw.summ <- left_join(siteMerge2, uw.data2, by = c("uw" = "SiteID")) %>%
  rename(SiteID = uw) %>% 
  filter(format(sampleDate, "%j") %in% 152:258) %>% 
  distinct(SiteID, year = year(sampleDate), sampleDate) %>%
  count(SiteID, year) %>%
  mutate(Source = "UW Database") #%>% 
  pivot_wider(names_from = year, values_from = n, names_prefix = "UW_") %>% 
  select(SiteID, paste0("UW_", 2006:2020)) %>% 
  select(SiteID, UW_2011:UW_2016)

```

```{r TC and UW data availability figure}
p3 <- bind_rows(left_join(siteMerge2, tc.summ, by = c("tc" = "SiteID")),
          left_join(siteMerge2, uw.summ , by = c("uw" = "SiteID"))) %>%
  filter(!is.na(Source)) %>% 
  ggplot() +
  geom_tile(aes(x = year, y = reorder(tc, desc(tc)), fill = n)) +
  # scale_fill_gradient(limits = range(min(uw.summ$n), max(uw.summ$n))) +
  # coord_cartesian(xlim = c(2017,2020)) + 
  geom_vline(aes(xintercept = 2016.5)) +
  facet_wrap(~Source) +
  theme_minimal() +
  theme(legend.position = "bottom", axis.title.y = element_blank()) 

p3

ggsave("output/UW Dataset.pdf", plot = p3, width = 12, height = 8, units = "in")

```

Try to filter on just Tim's sites and get the new data. Look at just summertime and see if it is a relatively complete and QAed dataset.


```{r TC sites combine with UW db}
tc.dat2 <- left_join(tc.dat, siteMerge2, by = c("SiteID" = "tc")) %>% 
  rename(TC_SiteID = SiteID,
         SiteID = uw,
         sampleDate = Date,
         meanDT = Temperature) %>% 
  mutate(sampleDate = as.Date(sampleDate)) %>% 
  filter(!is.na(SiteID), !is.na(meanDT)) #sites not in uw database with only 1 year of data.

tc.dat %>% distinct(SiteID)
tc.dat2 %>% distinct(TC_SiteID, SiteID)
tc.dat2 %>% summary

#Filter on sites in Tim's dataset, years after 2016, and convert to dailies.
uw.tcmerge <- uw.data2 %>% 
  ungroup() %>% 
  left_join(siteMerge2, by = c("SiteID" = "uw")) %>% 
  rename(TC_SiteID = tc) %>% 
  filter(!is.na(TC_SiteID), year(sampleDate) > 2016) %>% 
  mutate(UseData = 1) %>% 
  daily_screen(.) %>% 
  ungroup()

#Bind with TC dataset to get complete dataset.
intersect(names(tc.dat2), names(uw.tcmerge))
uw.daily <- bind_rows(tc.dat2, uw.tcmerge) %>% 
  select(SiteID, sampleDate, meanDT)
summary(uw.daily)

uw.daily %>% 
  distinct(SiteID)
```

Plot of daily temperatures over all years for each site, rolling pdf.

These data are generally ok but probably need a little more review before analysis. Ok to include in AFS summary, just include June - August, some bad temps in September for sure.

```{r}
sites <- uw.daily %>% distinct(SiteID) %>% pull(SiteID)

pdf("output/UW Daily Dataset by Site.pdf")

for(i in sites) {
  dat <- uw.daily %>% filter(SiteID == i, month(sampleDate) %in% 6:9)
  p1 <- dat %>%
    # complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>% 
    mutate(year = year(sampleDate),
           mo_day = format(sampleDate, "%m-%d")) %>% 
    ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanDT, color = as.factor(year))) +
    geom_line() +
    scale_x_date(date_breaks = "1 month", date_labels = "%b") +
    scale_color_discrete(limits = factor(2011:2020)) +
    facet_wrap(~ SiteID, labeller = label_wrap_gen(width = 15, multi_line = TRUE), ncol = 5) +
    labs(x = "Date", y = "Mean Daily Temperature", color = "Year") +
    theme_bw() +
    ggtitle(paste("UW Daily Logger Data for", i))
  print(p1)
}

dev.off()


```


Save metadata and daily data files for AFS summary.

```{r}
uw.meta %>% 
  filter(SiteID %in% sites) %>% 
  save_metadata_files(., "uw_draft")

uw.daily %>% 
  save_daily_files(., "uw_draft")

uw.daily %>% 
  distinct(SiteID, year(sampleDate)) %>% 
  count(SiteID) %>% 
  arrange(desc(n))

uw.daily %>% summary()
```
Save formatted data from UW database for review by Priscilla.

```{r}
#Filter on sites in Tim's dataset, years after 2016, and convert to dailies.
uw.post16 <- uw.data2 %>% 
  ungroup() %>% 
  left_join(siteMerge2, by = c("SiteID" = "uw")) %>% 
  rename(TC_SiteID = tc) 


```

Are there any sites in Jackie's db that we should include in the QA because they have a lot of data?

```{r}
uw.5sites <- uw.post16 %>% 
  filter(is.na(TC_SiteID), month(sampleDate) %in% 6:8) %>% 
  distinct(SiteID, sampleDate, year) %>% 
  count(SiteID, year) %>% 
  arrange(year) %>% 
  filter(n > 64) %>% #70% of days in JJA
  count(SiteID) %>% 
  filter(n > 2) %>% 
  pull(SiteID)
  # pivot_wider(names_from = year, values_from = n)

uw.data2 %>% 
  ungroup() %>% 
  filter(SiteID %in% uw.5sites) %>% 
  distinct(SiteID, mode_diff)

```


Look at data for just these five sites:
* Agulowak looks like it has two summers with pretty bad data, dewatered.
* Agulukpak has some air temps too and very strange data.
* sunshine - air temp but patterns look better.

Send to Priscilla and Dustin as a separate dataset that is *possibly* useable.

```{r}
p1 <- uw.data2 %>% 
  filter(SiteID %in% uw.5sites[5]) %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()  

ggplotly(p1)
```



```{r}

uw.post16.tcsites <- uw.post16 %>% 
  filter(!is.na(TC_SiteID), year(sampleDate) > 2016) %>% 
  mutate(UseData = 1) 

  
#good times.
uw.post16.tcsites %>% 
  distinct(mode_diff)

#all tidbit
uw.post16.tcsites %>% 
  distinct(MeasurementType)

uw.post16.tcsites %>% 
  count(SiteID, year) %>% 
  pivot_wider(names_from = year, values_from = n)

#june 1 to mid-september
uw.post16.tcsites %>% 
  group_by(SiteID, year) %>% 
  summarize(minDate = min(sampleDate),
            maxDate = max(sampleDate))
    
#only one record for this site-year
uw.post16.tcsites %>% 
  filter(SiteID == "Nerka Seventh Creek", year == 2019)

#no dups
uw.post16.tcsites %>% 
  count(SiteID, sampleDate, sampleTime, dt, year, Temperature) %>% 
  arrange(desc(n))

uw.forQA <- uw.post16.tcsites %>% 
  filter(!is.na(mode_diff),
         !(SiteID == "Nerka Seventh Creek" & year == 2019)) %>% 
  select(file_name, SiteID, sampleDate, sampleTime, dt, year, Temperature, UseData)
```

Save dataset of Tim's sites as priority for QA and also the other 5 sites with at least 3 years of data in Jackie's database as lower priority for QA.

```{r}
saveRDS(uw.forQA, "data_preparation/formatted_data/uw.post16.tcsites.rds")

uw.data2 %>% 
  filter(is.na(mode_diff))

uw.5sites.forQA <- uw.data2 %>% 
  filter(!is.na(mode_diff),
         SiteID %in% uw.5sites) %>%
  mutate(UseData = 1) %>% 
  select(file_name, SiteID, sampleDate, sampleTime, dt, year, Temperature, UseData)

saveRDS(uw.5sites.forQA, "data_preparation/formatted_data/uw.5sites.rds")

```

