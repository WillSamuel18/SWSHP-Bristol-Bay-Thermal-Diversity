---
title: "UW Water Temp Data"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu).

Sent Daniel email summarizing TC dataset versus sites in Jackie's database - 3/5/21. Possibly just move forward with additional years of data for sites that Tim originally used for thermal sensitivity.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

# load packages
library(googledrive)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(gridExtra)
library(tidyverse)
```

# Metadata

AKOATS metadata

```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA", col_types = "text") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```

Focus on just getting lat/longs for a map. Read in all records, convert akoats id to numeric, which will remove the sites with multiple entries - add those in manually. Also, remove sites with missing info - which are the two lynx creek sites with unknown locations.

Start with files provided in 2017 for SASAP project. 

```{r}
uw.meta <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  select(StationName:Long) %>%
  mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  filter(!(is.na(Lat) & is.na(AKOATS_ID))) %>% 
  select(-Lat, -Long)

uw.meta <- uw.meta %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

uw.meta
```

Second spreadsheet linking station names in 2018-2020 data to akoats ids. Deal with duplicate akoats ids entered by Jackie manually.

```{r}

uw.meta2 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "AKOATS links") %>% 
    mutate(Latitude = as.double(gsub("째","",Lat)),
         Longitude = as.double(gsub("째","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  rename(StationName = "UW_2018-2020 location") %>% 
  select(-Lat, -Long, -`...5`)

uw.meta2 <- uw.meta2 %>% 
  mutate(AKOATS_ID = case_when(StationName == "Nerka Lynx Creek Cold Tributary" ~ 1943,
                               StationName == "Nerka Bear Creek" ~ 1946,
                               StationName == "Aleknagik Pfifer Creek" ~ 1751,
                               StationName == "Aleknagik Silver Salmon Creek" ~ 1752,
                               TRUE ~ AKOATS_ID))

#get sites from 2018-2020 spreadsheet that are new and bind to metadata for sites through 2017.
uw.new.sites <- uw.meta2[!(uw.meta2 %>% pull(StationName)) %in% (uw.meta %>% pull(StationName)),] %>% 
  filter(!StationName == "Kulik K-1")
  
uw.meta <- bind_rows(uw.meta, uw.new.sites)
  
#get lat/longs and waterbody name from akoats for uw sites with ids
uw.akoats.sites <- left_join(uw.meta %>% 
                               filter(!is.na(AKOATS_ID)) %>% 
                               select(StationName, AKOATS_ID), 
                             akoats.meta %>% 
                               select(AKOATS_ID, Waterbody_name, Latitude, Longitude), by = "AKOATS_ID")

uw.meta <- bind_rows(uw.meta %>% 
                       filter(is.na(AKOATS_ID)),
                     uw.akoats.sites) %>% 
  rename(SiteID = StationName) %>% 
  mutate(SourceName = "uwASP")

uw.meta

```

There are several sites (e.g. one AKOATS_ID or one lat/long) that have several site ids in the data. These need to be fixed here and in the data so that one location = one time series.


```{r}
uw.meta %>% count(AKOATS_ID) %>% arrange(desc(n))
uw.meta %>% arrange(AKOATS_ID)

uw.meta <- uw.meta %>% 
  filter(!SiteID %in% c("Aleknagik Agulowak River", "Nerka Lynx Creek Upper", "Nerka Kema Creek Lower", 
                        "Nerka Kema Creek Upper","Nerka Agulukpak River", "Nerka Elva Creek Lower", 
                        "Nerka Coffee Creek", "Little Togiak Creek Lower")) 



uw.meta %>% 
  saveRDS("output/uw_metadata.rds")
```



# Data


## Data files through 2017 - SASAP project

Read in data from folder "Jackie Carter UW". These are the 51 files of stream temperature data originally provided for SASAP project, but never archived on KNB. Data are through 2017. Keep file name when reading in data in case it is needed to inspect date-time formats or other formatting problems. Also, keep other information from Jackie, such as Lake and sample type. We will want sample type to indicate logger accuracy for AKTEMP. And, lake information is a good check that we are locating sites in the correct region of the Wood River watershed. Removed the original Lynx Creek data file in this folder. Jackie provided a new version with only 5 station names and correct locations later.
Jackie provided updated data files for Pick Creek, Eagle Creek, and Wood River to fix time issues.


```{r 2017 data files}
uw.data <- list.files(path = ("data/Jackie Carter UW/Jackie Carter/"),
                      pattern = "*.csv", 
                      full.names = T) %>%
  map_df(function(x) read_csv(x,col_types = cols(.default = "c"),col_names = T) %>% 
           mutate(file_name = gsub(".csv","",basename(x)))) %>%
  select(-X9, -YearSampled) %>% 
  mutate(Temperature = as.numeric(Temperature))

```

Format dates and times. Most dates are "%m/%d/%y" 2-digit year. But, Big Whitefish and Little Whitefish have different format, "%d-%b-%y". 

```{r 2017 dates and times}
uw.data %>% 
  group_by(StationName) %>%
  slice_head() %>% 
  arrange(StationName)

uw.data <- uw.data %>% 
  mutate(sampleDate = parse_date_time(Date, orders = c("dby", "mdy")),
         sampleTime = as_hms(parse_date_time(Time, orders = c("IMS p", "HMS"))))

```

50 files were read in and there are a total of 54 sites with data.

```{r 2017 summary}

uw.data %>% 
  count(file_name, StationName, year = year(sampleDate)) %>%
  arrange(year, file_name, StationName) %>% 
  pivot_wider(names_from = year, values_from = n)

uw.data %>% 
  count(MeasurementType, file_name, year = year(sampleDate)) 

```




## Data files received in November 2020

Becky emailed with Jackie in November 2020 to get metadata information for the 51 data files sent over in 2017 for the SASAP project and also received some new data files.

* data file for sites on Lynx Creek - 5 stations
* data file for Aleknagik Bear Creek for data through 2017
* data file for all stream sites from 2018 through 2020  

```{r 2020 data files}
uw.lynx <- read_csv("data/Jackie Carter UW/Jackie Carter - 2020 data request/UW-FRI_LynxCreekALL_Temps_JC.csv", 
                    col_types = cols(.default = "c")) %>%
  mutate(file_name = "UW-FRI_LynxCreekALL_Temps_JC") %>%
  transform(sampleDate = as.Date(mdy(Date)),
            sampleTime = as_hms(parse_date_time(Time, "I:M:S p")),
            Temperature = as.numeric(Temperature)) 

uw.aleknagik.bear <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/froRSS_AleknagikBearTemps_2008-2017.xlsx") %>%
  mutate(file_name = "froRSS_AleknagikBearTemps_2008-2017",
         sampleTime = as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature)) 

uw.post2017 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "2018-2020 data", cell_rows(1:329775)) %>%
  mutate(sampleTime = hms::as_hms(Time),
         sampleDate = as.Date(Date),
         Temperature = as.numeric(Temperature),
         file_name = "forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps")

```


Join the multiple data frames in to single object. Start with all information, then pare down to just the fields that we need. For AKTEMP, may try to extract more metadata information from measurement type field to get sensor accuracy for data table.

```{r bind all data}
keep <- c("file_name", "Lake", "StationName", "SampleType", "MeasurementType", "sampleDate", "sampleTime", "Temperature")

uw.data2 <- bind_rows(uw.data %>% select(one_of(keep)),
                       uw.lynx %>% select(one_of(keep)),
                       uw.aleknagik.bear %>% select(one_of(keep)),
                       uw.post2017 %>% select(one_of(keep))) %>% 
  rename(SiteID = StationName) %>% 
  mutate(sampleDate = as.Date(sampleDate))

uw.data2 %>% 
  summary()

# remove extraneous objects
rm(uw.data, uw.lynx, uw.aleknagik.bear, uw.post2017)
```


Fix duplicate SiteIDs that go to same location (see metadata section above for Jackie's notes on this). Also, Jackie couldn't find locations for two sites on Lynx Creek - beach and middle so remove those time series from the data frame.

```{r collapse SiteIDs}
uw.data2 %>% distinct(SiteID) %>% arrange(SiteID)

uw.data2 <- uw.data2 %>% 
  mutate(SiteID = case_when(SiteID == "Aleknagik Agulowak River" ~ "Agulowak River",
                            SiteID == "Nerka Lynx Creek Upper" ~ "Nerka Lynx Lake Tributary",
                            SiteID %in% c("Nerka Kema Creek Lower", "Nerka Kema Creek Upper") ~ "Nerka Kema Creek",
                            SiteID =="Nerka Agulukpak River" ~ "Agulukpak River",
                            SiteID =="Nerka Elva Creek Lower" ~ "Nerka Elva Creek",
                            SiteID =="Nerka Coffee Creek" ~ "Nerka Chamee Creek",
                            SiteID =="Little Togiak Creek Lower" ~ "Little Togiak Creek",
                            TRUE ~ SiteID)) %>% 
  filter(!SiteID %in% c("Nerka Lynx Creek Middle", "Nerka Lynx Creek Beach"))

#lat/longs for all 60 sites in data
uw.data2 %>% 
  distinct(SiteID) %>% 
  left_join(uw.meta)

#also 60 sites in metadata so they are matching.
uw.meta

```

Add a date-time and year field, which will be needed for data QA. Also can use to identify and remove duplicates in data.

```{r add dt and year fields}
uw.data2 <- uw.data2 %>% 
  mutate(dt = as.POSIXct(paste(sampleDate, sampleTime), format = "%Y-%m-%d %H:%M"),
         year = year(sampleDate))
```

# Review Data

Summary of sites with many duplicate values and the mean and sd of the differences between the duplicate measurements. A couple of sites with either duplicate loggers or multiple files. Check into this and see what is going on here. These are all the sites across both data requests. Note that some of these are ibutton temps and some have very coarse sample intervals (> 2 hr). Possibly filter to remove those first.

```{r duplicate measurements}
uw.data2 %>% 
  group_by(SiteID, sampleDate, sampleTime) %>% 
  mutate(id = row_number()) %>%
  select(SiteID, sampleDate, sampleTime, id, Temperature) %>% 
  pivot_wider(names_from = id, values_from = Temperature) %>% 
  mutate(diff = abs(`1`-`2`)) %>% 
  filter(!is.na(diff)) %>% 
  group_by(SiteID) %>% 
  summarize(mean = mean(diff),
            sd = sd(diff),
            count = n())


uw.data2 %>% 
  group_by(SiteID, sampleDate, sampleTime) %>% 
  mutate(id = row_number()) %>%
  select(SiteID, sampleDate, sampleTime, id, Temperature) %>% 
  pivot_wider(names_from = id, values_from = Temperature) %>% 
  mutate(diff = abs(`1`-`2`)) %>% 
  filter(!is.na(diff), SiteID == "Nerka Fenno Creek") %>% 
  ungroup() %>% 
  summarize(min(sampleDate), max(sampleDate))
```

Look at logger type and measurement frequency. Possibly we should only focus on the newer data.

```{r}
source("W:/Github/AKSSF/helper_functions.R")

uw.data2 <- temp_msmt_freq(uw.data2)

uw.data2 %>% 
  ungroup() %>% 
  count(MeasurementType, file_name, SiteID, year = year(sampleDate), mode_diff) %>% 
  arrange(file_name, year)

uw.data2 %>% 
  filter(SiteID == "Beverley Uno Creek", mode_diff == 1)
```



# Save data


Summary csv to add as attributes to leaflet map.

```{r eval = FALSE}
uw.data2 %>% 
  group_by(SiteID) %>% 
  summarize(startYear = min(year(sampleDate)),
            endYear = max(year(sampleDate)),
            totYears = length(unique(year(sampleDate)))) %>% 
  saveRDS("output/uw_data_summ.rds")

```

Save a copy of the data for a table and figure.

```{r eval = FALSE}
uw.data2

saveRDS(uw.data2, "output/uw_data.rds")
```







# Tim Cline's data from Zotero

Six files each for a separate year of data. Sites are different columns in the dataset.
Data archived on Zenodo: https://zenodo.org/record/3523300#.YEKEN45KhaQ
Unfortunately, the site names don't match Jackie's database and there are no lat/longs. I'll need to work with Jackie to try and get the most current years of data for these sites.

```{r Tim Cline data}
tc.files <- list.files("S:\\Stream Temperature Data\\Tim Cline UW", full.names = TRUE)
tc.files <- tc.files[!grepl("Climate", tc.files)]

tc.11 <- read_csv(tc.files[1]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.12 <- read_csv(tc.files[2]) %>%
  mutate(Date = parse_date_time(Day, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad, -Day) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.13 <- read_csv(tc.files[3]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.14 <- read_csv(tc.files[4]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.15 <- read_csv(tc.files[5]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.16 <- read_csv(tc.files[6]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.dat <- bind_rows(tc.11, tc.12, tc.13, tc.14, tc.15, tc.16) 
tc.dat %>% 
  distinct(SiteID)

```

Plot data

```{r TC data daily time series}
tc.dat %>% 
  mutate(day = format(Date, "%m-%d")) %>% 
  ggplot(aes(x = as.Date(day, format = "%m-%d"), y = Temperature, color = as.factor(year(Date)))) +
  geom_line() +
  facet_wrap(~SiteID)

tc.dat %>% distinct(SiteID)

summary(tc.dat)
```

This was saved and then edited outside R with Daniel's help, fixed site names in following code chunks.

```{r merge TC and UW site names, eval = FALSE}

tc.sites <- tc.dat %>% distinct(SiteID) %>% pull(SiteID) 
uw.sites <- uw.data2 %>% ungroup() %>% distinct(SiteID) %>% arrange(SiteID) %>% pull(SiteID)

siteMerge <- data.frame()
for(i in tc.sites) {
  uwIn = grep(i, uw.sites)
  #figure out what to do when can't be found. need to skip and population new row with NA
  if(length(uwIn > 0)) {
    newrow <- data.frame(tc = i, uw = uw.sites[uwIn])
  } else {
    newrow <- data.frame(tc = i, uw = NA)
  } 
  siteMerge <- bind_rows(siteMerge, newrow)
}

siteMerge
```

```{r eval = FALSE}
# write_csv(siteMerge, path = "output/UW_site_merge.csv")
```

Manually enter in some of the names in Jackie's dataset that logically match Tim's dataset. Talked with Daniel and cleared up some of the matches: Bear = Aleknagik Bear (NerkaBear already in Tim's dataset), Moose = Beverly Moose, Whitefish = Big Whitefish. The other sites with just one year of data in Tim's dataset, but not in the database are from P. Lisi's research and can be dropped since they didn't continue those sites (ChauekX and UpnukXX). Still not sure about MooseSpit because I forgot to ask Daniel, but emailed him about it.

```{r TC and UW data summaries}
#note that bear, beaver, and moose 
siteMerge2 <- read_csv("output/UW_site_merge_rs.csv")

jul.range <- tc.dat %>% 
  distinct(Date) %>% 
  mutate(jd = format(Date, "%j")) %>% 
  summarize(range(jd))

tc.summ <- tc.dat %>% 
  count(SiteID, year = year(Date)) %>%
  mutate(Source = "Tim Cline") #%>% 
  pivot_wider(names_from = year, values_from = n, names_prefix = "TC_") %>% 
  select(SiteID, TC_2011, TC_2012, TC_2013, TC_2014, TC_2015, TC_2016) %>% 
  select(SiteID, TC_2011:TC_2016)

uw.summ <- left_join(siteMerge2, uw.data2, by = c("uw" = "SiteID")) %>%
  rename(SiteID = uw) %>% 
  filter(format(sampleDate, "%j") %in% 152:258) %>% 
  distinct(SiteID, year = year(sampleDate), sampleDate) %>%
  count(SiteID, year) %>%
  mutate(Source = "UW Database") #%>% 
  pivot_wider(names_from = year, values_from = n, names_prefix = "UW_") %>% 
  select(SiteID, paste0("UW_", 2006:2020)) %>% 
  select(SiteID, UW_2011:UW_2016)

```

```{r TC and UW data availability figure}
p3 <- bind_rows(left_join(siteMerge2, tc.summ, by = c("tc" = "SiteID")),
          left_join(siteMerge2, uw.summ , by = c("uw" = "SiteID"))) %>%
  filter(!is.na(Source)) %>% 
  ggplot() +
  geom_tile(aes(x = year, y = reorder(tc, desc(tc)), fill = n)) +
  # scale_fill_gradient(limits = range(min(uw.summ$n), max(uw.summ$n))) +
  # coord_cartesian(xlim = c(2017,2020)) + 
  geom_vline(aes(xintercept = 2016.5)) +
  facet_wrap(~Source) +
  theme_minimal() +
  theme(legend.position = "bottom", axis.title.y = element_blank()) 

p3

ggsave("output/UW Dataset.pdf", plot = p3, width = 12, height = 8, units = "in")

```

Try to filter on just Tim's sites and get the new data. Look at just summertime and see if it is a relatively complete and QAed dataset.


```{r TC sites combine with UW db}
tc.dat2 <- left_join(tc.dat, siteMerge2, by = c("SiteID" = "tc")) %>% 
  rename(TC_SiteID = SiteID,
         SiteID = uw,
         sampleDate = Date,
         meanDT = Temperature) %>% 
  mutate(sampleDate = as.Date(sampleDate)) %>% 
  filter(!is.na(SiteID), !is.na(meanDT)) #sites not in uw database with only 1 year of data.

tc.dat %>% distinct(SiteID)
tc.dat2 %>% distinct(TC_SiteID, SiteID)
tc.dat2 %>% summary

#Filter on sites in Tim's dataset, years after 2016, and convert to dailies.
uw.tcmerge <- uw.data2 %>% 
  ungroup() %>% 
  left_join(siteMerge2, by = c("SiteID" = "uw")) %>% 
  rename(TC_SiteID = tc) %>% 
  filter(!is.na(TC_SiteID), year(sampleDate) > 2016) %>% 
  mutate(UseData = 1) %>% 
  daily_screen(.) %>% 
  ungroup()

#Bind with TC dataset to get complete dataset.
intersect(names(tc.dat2), names(uw.tcmerge))
uw.daily <- bind_rows(tc.dat2, uw.tcmerge) %>% 
  select(SiteID, sampleDate, meanDT)
summary(uw.daily)

uw.daily %>% 
  distinct(SiteID)
```

Plot of daily temperatures over all years for each site, rolling pdf.

These data are generally ok but probably need a little more review before analysis. Ok to include in AFS summary, just include June - August, some bad temps in September for sure.

```{r}
sites <- uw.daily %>% distinct(SiteID) %>% pull(SiteID)

pdf("output/UW Daily Dataset by Site.pdf")

for(i in sites) {
  dat <- uw.daily %>% filter(SiteID == i, month(sampleDate) %in% 6:9)
  p1 <- dat %>%
    # complete(SiteID, sampleDate = seq.Date(min(sampleDate), max(sampleDate), by = "day")) %>% 
    mutate(year = year(sampleDate),
           mo_day = format(sampleDate, "%m-%d")) %>% 
    ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanDT, color = as.factor(year))) +
    geom_line() +
    scale_x_date(date_breaks = "1 month", date_labels = "%b") +
    scale_color_discrete(limits = factor(2011:2020)) +
    facet_wrap(~ SiteID, labeller = label_wrap_gen(width = 15, multi_line = TRUE), ncol = 5) +
    labs(x = "Date", y = "Mean Daily Temperature", color = "Year") +
    theme_bw() +
    ggtitle(paste("UW Daily Logger Data for", i))
  print(p1)
}

dev.off()


```


Save metadata and daily data files for AFS summary.

```{r}

uw.daily %>% 
  save_daily_files(., "uw_draft")

uw.daily %>% 
  distinct(SiteID, year(sampleDate)) %>% 
  count(SiteID) %>% 
  arrange(desc(n))

uw.daily %>% summary()

uw.meta %>% 
  arrange(SiteID)
```
Save formatted data from UW database for review by Priscilla.

```{r}
#Filter on sites in Tim's dataset, years after 2016, and convert to dailies.
uw.post16 <- uw.data2 %>% 
  ungroup() %>% 
  left_join(siteMerge2, by = c("SiteID" = "uw")) %>% 
  rename(TC_SiteID = tc) 


```

Are there any sites in Jackie's db that we should include in the QA because they have a lot of data?

```{r}
uw.5sites <- uw.post16 %>% 
  filter(is.na(TC_SiteID), month(sampleDate) %in% 6:8) %>% 
  distinct(SiteID, sampleDate, year) %>% 
  count(SiteID, year) %>% 
  arrange(year) %>% 
  filter(n > 64) %>% #70% of days in JJA
  count(SiteID) %>% 
  filter(n > 2) %>% 
  pull(SiteID)
  # pivot_wider(names_from = year, values_from = n)

uw.data2 %>% 
  ungroup() %>% 
  filter(SiteID %in% uw.5sites) %>% 
  distinct(SiteID, mode_diff)


uw.post16 %>% distinct(SiteID, TC_SiteID)
```


Look at data for just these five sites:
* Agulowak looks like it has two summers with pretty bad data, dewatered.
* Agulukpak has some air temps too and very strange data.
* sunshine - air temp but patterns look better.

Send to Priscilla and Dustin as a separate dataset that is *possibly* useable.

```{r}
p1 <- uw.data2 %>% 
  filter(SiteID %in% uw.5sites[5]) %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line()  

ggplotly(p1)
```



```{r}

uw.post16.tcsites <- uw.post16 %>% 
  filter(!is.na(TC_SiteID), year(sampleDate) > 2016) %>% 
  mutate(UseData = 1) 

  
#good times.
uw.post16.tcsites %>% 
  distinct(mode_diff)

#all tidbit
uw.post16.tcsites %>% 
  distinct(MeasurementType)

uw.post16.tcsites %>% 
  count(SiteID, year) %>% 
  pivot_wider(names_from = year, values_from = n)

#june 1 to mid-september
uw.post16.tcsites %>% 
  group_by(SiteID, year) %>% 
  summarize(minDate = min(sampleDate),
            maxDate = max(sampleDate))
    
#only one record for this site-year
uw.post16.tcsites %>% 
  filter(SiteID == "Nerka Seventh Creek", year == 2019)

#no dups
uw.post16.tcsites %>% 
  count(SiteID, sampleDate, sampleTime, dt, year, Temperature) %>% 
  arrange(desc(n))

uw.forQA <- uw.post16.tcsites %>% 
  filter(!is.na(mode_diff),
         !(SiteID == "Nerka Seventh Creek" & year == 2019)) %>% 
  select(file_name, SiteID, sampleDate, sampleTime, dt, year, Temperature, UseData)
```

Save dataset of Tim's sites as priority for QA and also the other 5 sites with at least 3 years of data in Jackie's database as lower priority for QA.

```{r save data to google drive}
saveRDS(uw.forQA, "data_preparation/formatted_data/uw.post16.tcsites.rds")

uw.data2 %>% 
  filter(is.na(mode_diff))

uw.5sites.forQA <- uw.data2 %>% 
  filter(!is.na(mode_diff),
         SiteID %in% uw.5sites) %>%
  mutate(UseData = 1) %>% 
  select(file_name, SiteID, sampleDate, sampleTime, dt, year, Temperature, UseData)

saveRDS(uw.5sites.forQA, "data_preparation/formatted_data/uw.5sites.rds")

tc.dat2 <- tc.dat2 %>% 
  mutate(SiteID = paste0("UW_", SiteID))
tc.dat2 %>% distinct(SiteID) %>% arrange(SiteID)

save_daily_files(tc.dat2, "UW_tc")

uw.meta <- uw.meta %>% 
  mutate(SiteID = paste0("UW_", SiteID))


#before saving metadata, remove any sites that we are not using.
all.sites <- bind_rows(data.frame(SiteID = paste0("UW_", c(uw.5sites, 
                                                           uw.post16.tcsites %>% distinct(SiteID) %>% pull(SiteID)))),
                       tc.dat2 %>% distinct(SiteID)) %>% 
  distinct(SiteID)

#all 50 sites have locations in metadata
left_join(all.sites, uw.meta)

#10 sites that we are not using, double-check that they have limited use.
left_join(uw.meta, all.sites %>% mutate(keep = 1)) %>% filter(is.na(keep))

remove_sites <- left_join(uw.meta, all.sites %>% mutate(keep = 1)) %>% filter(is.na(keep)) %>% arrange(SiteID) %>%  pull(SiteID)

#all have only 1 or two years of data.
uw.data2 %>% 
  ungroup() %>% 
  mutate(SiteID = paste0("UW_", SiteID)) %>% 
  filter(SiteID %in% remove_sites, month(sampleDate) %in% 6:8) %>% 
  distinct(SiteID, sampleDate, year) %>% 
  count(SiteID, year) %>% 
  filter(n > 64) %>% #70% of days in JJA
  count(SiteID) 


uw.meta %>% 
  filter(!SiteID %in% remove_sites) %>% 
  save_metadata_files(., "UW")

```


# TC sites in UW database for max, min, and mean

For the most part, we moved forward with Tim Cline's sites and data from zenodo. We added in a few other sites with longer time series.

```{r read in final data frame}
temp_dat <- readRDS("W:/Github/AKSSF/data_preparation/final_data/summer_data_wair2021-11-23.rds")
temp_md <- readRDS("W:/Github/AKSSF/data_preparation/final_data/md.rds")

# temp_md %>% distinct(SourceName)

uw_dat <- left_join(temp_dat, temp_md %>% distinct(SiteID, SourceName)) %>% 
  filter(SourceName == "uwASP")

uw_dat %>% 
  filter(is.na(minDT))

uw_dat %>% 
  mutate(Year = year(sampleDate)) %>% 
  filter(is.na(maxDT)) %>% 
  distinct(SiteID, Year) %>% 
  group_by(SiteID) %>% 
  summarize(years = paste(Year, collapse = ", "))
```


```{r Tim Cline data}
tc.files <- list.files("S:\\Stream Temperature Data\\Tim Cline UW", full.names = TRUE)
tc.files <- tc.files[!grepl("Climate", tc.files)]

tc.11 <- read_csv(tc.files[1]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.12 <- read_csv(tc.files[2]) %>%
  mutate(Date = parse_date_time(Day, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad, -Day) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.13 <- read_csv(tc.files[3]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTempDill, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.14 <- read_csv(tc.files[4]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.15 <- read_csv(tc.files[5]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.16 <- read_csv(tc.files[6]) %>%
  mutate(Date = parse_date_time(Date, orders = c('m/d/y', 'm/d/Y'))) %>% 
  select(-DOY, -AirTemp, -SolRad) %>% 
  pivot_longer(names_to = "SiteID", values_to = "Temperature", -Date) %>% 
  select(SiteID, Date, Temperature) %>% 
  arrange(SiteID, Date)

tc.dat <- bind_rows(tc.11, tc.12, tc.13, tc.14, tc.15, tc.16) 
tc.dat %>% 
  distinct(SiteID)

```

Send Jackie a new data request with Tim's site and years so I can get daily min mean and max.

```{r sites and years for jackie, eval = FALSE}
siteMerge2 <- read_csv("W:/Github/SWSHP-Bristol-Bay-Thermal-Diversity/output/UW_site_merge_rs.csv")

uw_daily <- uw.data2 %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanDT = mean(Temperature, na.rm = TRUE),
            maxDT = max(Temperature, na.rm = TRUE),
            minDT = min(Temperature, na.rm = TRUE))

left_join(tc.dat, siteMerge2, by = c("SiteID" = "tc")) %>%
  mutate(sampleDate = as.Date(Date)) %>% 
  left_join(uw_daily, by = c("uw" = "SiteID", "sampleDate" = "sampleDate")) %>% 
  filter(is.na(meanDT) & !is.na(Temperature)) %>% #TC has data, UW db does not 
  count(SiteID, uw, year(sampleDate))

#save complete set of sites and years with uw site name to send to jackie.

left_join(tc.dat, siteMerge2, by = c("SiteID" = "tc")) %>%
  mutate(Year = year(Date)) %>% 
  distinct(SiteID, Year, uw) %>% 
  write.csv("output/TC_siteyears.csv")

```


Add in new data from Jackie for Tim Cline's sites. These were originally received as only daily means, but now we have daily min and max as well. There may still be some UW sites that we don't have min and max for, which means we won't be able to intersect the TS with thermal regimes.

Note: there were some duplicate entries for Rainbow creek with very different temps. Jackie fixed the sampleType to indicate which to use -- some were really high so probably air temps from a level logger or stream gage. Still duplicates for this site in 2011, but just filter to include the series with colder temps.


```{r read in data and fix rainbow dups}
uw_daily <- read_excel("S:\\Stream Temperature Data\\Jackie Carter UW\\Request for dailies from TC sites\\forRS_WRStreamTemps_Cline_RainbowFix.xlsx")

#duplicates
dups <- uw_daily %>% count(SampleType, Nerka, StationName, `TimCline Name`, YearSampled, Date) %>% filter(n > 1)

#totally different, keep mins
left_join(dups, uw_daily)

rainbow_2011_fix <- left_join(dups, uw_daily) %>% 
  group_by(SampleType, Nerka, StationName, `TimCline Name`, YearSampled, Date) %>% 
  summarize(MinOfTemperature = min(MinOfTemperature),
            MaxOfTemperature = min(MaxOfTemperature),
            AvgOfTemperature = min(AvgOfTemperature))

uw_daily2 <- anti_join(uw_daily, dups) %>% 
  bind_rows(rainbow_2011_fix)

#checks
(nrow(uw_daily) - nrow(uw_daily2)) == (nrow(dups))
#lots of duplicate sample types, Jackie said to keep ibuttons/temperature if I can
uw_daily2 %>% count(Nerka, StationName, `TimCline Name`, YearSampled, Date) %>% filter(n > 1)
```

Fix the sample type field because there will still be duplicates here that should be removed.

```{r uw new fields}
uw_daily2 <- uw_daily2 %>% 
  mutate(SiteID = paste0("UW_", StationName),
         sampleDate = as.Date(Date),
         file_name = "forRS_WRStreamTemps_Cline.xlsx",
         sampleType = case_when(SampleType %in% c("Level Logger", "LevelLogger", "Level logger") ~ "Level_Logger",
                                SampleType %in% c("Stage Gauge") ~ "Stage_Gauge",
                                SampleType %in% c("iButtons", "ibuttons") ~ "iButton",
                                SampleType %in% c("Temperature") ~ "Tidbit",
                                TRUE ~ SampleType)) %>% 
  select(sampleType, SiteID, StationName, sampleDate, meanDT = AvgOfTemperature, minDT = MinOfTemperature, maxDT = MaxOfTemperature) %>% 
  filter(month(sampleDate) %in% 6:9)
```


Finally remove all the data from level loggers or stage gauges when other data exist. We'll still need to rely on these loggers for some dates and sites. Last set of dups was from a tidbit for one month at little togiak creek, just keep the ibutton data since that lasts for much longer and looks the same.

```{r get rid of sample type dups}
type_dups <- uw_daily2 %>% 
  count(SiteID, sampleDate) %>% 
  filter(n > 1)

left_join(type_dups, uw_daily2) %>% 
  ggplot(aes(x = sampleDate, y = meanDT, color = sampleType)) +
  geom_line() +
  facet_wrap(~SiteID, scales = "free")


#22010 - 1328 = 20682

uw_daily3 <- uw_daily2 %>% 
  group_by(SiteID, sampleDate) %>% 
  mutate(dups = n()) %>% 
  ungroup() %>% 
  filter(!(dups == 2 & sampleType %in% c("Level_Logger", "Stage_Gauge")),
         !(SiteID == "UW_Little Togiak C Creek" & year(sampleDate) == 2013 & sampleType == "Tidbit")) %>% 
  select(-dups)


```

Still trying to reconcile daily data from jackie with uw data used for akssf project. some maxes are really high so these are obviously days that should be removed bc most likely air temperatures. Also, 277 (but out of 16K) where means are off by more than 1 degree. Probably should just drop all the sampleDates with high maxes and large differences in mean values so I'm sticking with the means that we used to calculate ts, but dropping some dates/sites/years from the thermal regime analysis. Once I'm done, save a new daily uw dataset with all sites merged to the final data folder and also google drive.


Look for obvious air temperatures in this dataset. Filter to just includes the sites and dates that we included in the original DFA analysis. There are ~500 dates that aren't in Jackie's dataset, just drop those. 

```{r pdf of sites and years}
uw_daily4 <- left_join(uw_dat %>% filter(is.na(minDT)) %>% select(SiteID, sampleDate),
                       uw_daily3) %>% 
  filter(!is.na(meanDT))

summary(uw_daily4)

sites <- uw_daily4 %>% distinct(SiteID)

pdf("UW Dailies 3Feb22.pdf")

for(i in 1:nrow(sites)) {
  p1 <- sites %>% slice(i) %>% left_join(uw_daily4) %>% 
    mutate(Day = format(sampleDate, "%m-%d"),
           Year = year(sampleDate)) %>% 
    ggplot() +
    geom_line(aes(x = as.Date(Day, format = "%m-%d"), y = meanDT), color = "blue") +
    geom_line(aes(x = as.Date(Day, format = "%m-%d"), y = minDT)) +
    geom_line(aes(x = as.Date(Day, format = "%m-%d"), y = maxDT), color = "red") +
    facet_wrap(~Year) +
    labs(title = sites %>% slice(i) %>% pull(SiteID), x = "Date", y = "Temperature")
  print(p1)  
}

dev.off()

```

Interactive plot so I can ID dates and sites to flag/remove. First take on data to remove is from the pdf plot created above:

* 9: 2015 at Aleknagik Eagle Creek from June til early August
* 12: 2015 at hansen creek in June
* 13: happy creek drop 2012 and 2016, they must have been buried.
* 16: 2016 spike at end of september for nerka joe creek
* 21: 2016 mission creek got buried starting in August
* 27: 2016 seventh creek got buried
* 28: 2014 silver salmon creek has air spike on first day of data in June
* 30: 2012 for stovall creek looks buried.....maybe a rainy summer, leave in.
* 31: teal creek may also have been buried in 2011 and 2012, but hard to tell without notes, leave in.
* 31: nerka teal creek had an air temp spike in June 2015.
* 33; big whitefish may have had some burials as well - 2012 and 2013, partially buried in 2016? Since this is a pattern for 2012, leave in. Can't confirm without notes that the logger was buried on retrieval.
* 40: grant river likely has air temperatures in 2016 in July to early August.
* 45: youth creek has air temp spike on last day of data in september

```{r interactive plotting for dates}
nrow(sites) #45
i = 13

p1 <-  sites %>% slice(i) %>% left_join(uw_daily4) %>% 
    mutate(Day = format(sampleDate, "%m-%d"),
           Year = year(sampleDate)) %>% 
    ggplot() +
    geom_line(aes(x = as.Date(Day, format = "%m-%d"), y = meanDT), color = "blue") +
    geom_line(aes(x = as.Date(Day, format = "%m-%d"), y = minDT)) +
    geom_line(aes(x = as.Date(Day, format = "%m-%d"), y = maxDT), color = "red") +
    facet_wrap(~Year) +
    labs(title = sites %>% slice(i) %>% pull(SiteID), x = "Date", y = "Temperature")

ggplotly(p1)
```

Remove bad data - air temps and burials.

```{r remove air and burials}
uw_qa <- read_excel("data_preparation/uw_dailies_qa.xlsx")

#expand so sample dates are flagged for removal
uw_qa <- uw_qa %>% 
  rowwise() %>%
  transmute(SiteID,
            sampleDate = list(seq(startDate, endDate, by = "day"))) %>%
  unnest(sampleDate) %>% 
  mutate(remove = 1)

uw_daily5 <- left_join(uw_daily4, uw_qa) %>% 
  filter(is.na(remove)) %>% 
  select(-remove)

summary(uw_daily5)
```


Finally compare the data to the means for sites in the original saved dataset that are missing min and max.

```{r remove days with large diffs in mean temps}

remove_temp_diffs <- left_join(temp_dat %>% filter(is.na(minDT)) %>% select(SiteID, sampleDate, temp = meanDT), uw_daily5) %>% 
  mutate(temp_diff = abs(meanDT - temp)) %>% 
  filter(temp_diff > 2) %>% 
  select(SiteID, sampleDate) %>% 
  mutate(remove = 1)


uw_daily6 <- left_join(uw_daily5, remove_temp_diffs) %>% 
  filter(is.na(remove)) %>% 
  select(-remove)

summary(uw_daily6)
```

Merge the new UW data back to the original file of dailies and save this -- JUST FOR THERMAL REGIME ANALYSIS. Don't overwrite or confuse with the original daily file that Tim is using for DFA.

```{r save file for thermal regime analysis}
#missing 16,845
uw_dat %>% 
  count(is.na(minDT))

#15731, pretty good.
uw_daily6

tr_dat <- left_join(temp_dat, temp_md %>% distinct(SiteID, SourceName)) %>% #237,898
  filter(!SourceName == "uwASP") %>% #205,430
  bind_rows(uw_daily6 %>% select(SiteID, sampleDate, meanDT, minDT, maxDT) %>% mutate(SourceName = "uwASP")) %>%  #221,711
  select(-file_name, -airDT)

summary(tr_dat)

#still some missing data for USGS sites
tr_dat %>% 
  filter(is.na(minDT)) %>% 
  distinct(SiteID, year(sampleDate)) %>% 
  arrange(SiteID)

tr_dat2 <- tr_dat %>% 
  filter(!(is.na(minDT)|is.na(maxDT)))

nrow(tr_dat) - nrow(tr_dat2)
summary(tr_dat2)

saveRDS(tr_dat2, paste0("W:/Github/AKSSF/data_preparation/final_data/daily_dat_forTR_", Sys.Date(), ".rds"))
```





