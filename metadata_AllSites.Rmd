---
title: "All Sites Metadata"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Benjamin Meyer (benjamin.meyer.ak@gmail.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(dataRetrieval)
# library(rsay)

# define function to exclude multiple strings in a column
'%ni%' <- Negate('%in%')
```

<br>

# Data

Summarize extent and metadata of all successfully read-in data so date

Create list of unique data sets by site and year from read-in folder (`\data`)
```{r warning=FALSE}

# read in streams & beaches
dat <-list.files(path = paste0("output"),
               pattern = "*.csv", 
               full.names = T) %>%
     map_df(function(x) read_csv(x,skip = 1,col_types = cols(.default = "c"),col_names = F) %>% 
            mutate(filename = gsub(".csv","",basename(x))))
# rename columns
colnames(dat) <- c("AKOATS_ID","SiteID","Date","Time","Temperature","useData","filename")

# how many streams & beaches observations? (7,429,464)
n_obs_sb <- nrow(dat)

# read in lakes
dat.lakes <-list.files(path = paste0("output/Lakes"),
               pattern = "*.csv", 
               full.names = T) %>%
     map_df(function(x) read_csv(x,skip = 1,col_names = F) %>% 
            mutate(filename = gsub(".csv","",basename(x))))
# rename columns
colnames(dat.lakes) <- c("AKOATS_ID","SiteID","Depth","Date","Time","Temperature","useData","filename")

# how many lakes observations? (5,467,621)
n_obs_lakes <- nrow(dat.lakes)

```

<br>

Visualization exercise: if each water temperature observation in our enormous data set were an average sized sockeye, what distance would it stretch end to end?
```{r Analogy}

# average sockeye FL = 2 ft = 609.6 mm
# earth circumference = 40,007.863 km
# distance from Seattle to Tokyo = 7,712 km

sockeye.length.km <- 609.6 / 1000000 # length of one sockeye in km
total_obs <- n_obs_sb + n_obs_lakes
dist <- total_obs * sockeye.length.km # = 8100 km

```

If each of the observations in our data set were an average sized sockeye salmon (~2 ft), they would stretch `r dist` km end to end, that's 300 km more than the distance from Seattle to Tokyo!

We have a total of `r n_obs_sb` individual streams & beaches temperature observations!  And, we have a total of `r n_obs_lakes` observations from multi-level buoy lake sites!  


<br>


# Metadata

Associate metadata with all temp data sets.  Proceed one agency at a time, as it is not all in a consistent format.

Note: original approach involved collating all metadata to a single table, then using left_join to assign to overall data frame either by AKOATS_ID or SiteID.  However computer processing power was insufficient to handle this task.  Thus metadata is joined one-at-a-time by agency.

* AKOATS metadata

```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "AKOATS_COMPLETE", col_types = "text") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```

<br>

* UW metadata

Focus on just getting lat/longs for a map. Read in all records, convert akoats id to numeric, which will remove the sites with multiple entries. That's ok because Jackie provided lat/longs for all of those sites. Also, remove sites with missing info - which are the two lynx creek sites with unknown locations.

```{r}
uw.meta <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  select(StationName:Long) %>%
  mutate(Latitude = as.double(gsub("°","",Lat)),
         Longitude = as.double(gsub("°","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  filter(!(is.na(Lat) & is.na(AKOATS_ID))) %>% 
  select(-Lat, -Long)

uw.meta2 <- read_excel("data/Jackie Carter UW/Jackie Carter - 2020 data request/forRS_UAA_2018-2020_WoodRiverStreamAndLakeTemps.xlsx", sheet = "AKOATS links") %>% 
    mutate(Latitude = as.double(gsub("°","",Lat)),
         Longitude = as.double(gsub("°","",Long)),
         AKOATS_ID = as.numeric(AKOATS_ID)) %>% 
  rename(StationName = "UW_2018-2020 location") %>% 
  select(-Lat, -Long, -`...5`)

#get sites from 2018-2020 spreadsheet that are new and bind to metadata for sites through 2017.
uw.new.sites <- uw.meta2[!(uw.meta2 %>% pull(StationName)) %in% (uw.meta %>% pull(StationName)),] %>% 
  filter(!StationName == "Kulik K-1")
  
uw.meta <- bind_rows(uw.meta, uw.new.sites)
  
#get lat/longs from akoats for uw sites with ids
uw.akoats.sites <- left_join(uw.meta %>% 
                               filter(!is.na(AKOATS_ID)) %>% 
                               select(StationName, AKOATS_ID), 
                             akoats.meta %>% 
                               select(SourceName, AKOATS_ID, Latitude, Longitude), by = "AKOATS_ID")

uw.meta <- bind_rows(uw.meta %>% 
                       filter(is.na(AKOATS_ID)),
                     uw.akoats.sites) %>% 
  rename(SiteID = StationName) %>% 
  mutate(SourceName = "uwASP")

uw.meta %>% 
  saveRDS("output/uw_metadata.rds")
```



```{r}
# UW metadata
# read in UW metadata resolutions emailed from R Shaftel on 11/13/20
uw.meta <- read_excel("data/Jackie Carter UW/uw_metadata_thru_2017.xlsx", sheet = "Sheet1") %>%
  # still waiting on Lynx Creek SiteID reslutions, so exclude for now
  # filter(data_SiteID != "UW-FRI_LynxCreekALL_Temps") %>%
  # use only one AKOATS_ID for sites with multiple entries
  separate(AKOATS_ID, sep = " and", into = "AKOATS_ID") %>%
  separate(AKOATS_ID, sep = " or", into = "AKOATS_ID") %>%
  select(-rss_notes, -`jc_notes Nov13`, -fileName) %>%
  # remove degree symbols
  mutate(Latitude = as.double(gsub("°","",Latitude)),
         Longitude = as.double(gsub("°","",Longitude))) #%>%
  # create other metadata as possible as known
  mutate(Contact_person = "Jackie Carter",
         Contact_email = "jlcarter@uw.edu",
         SourceName = "uwASP",
         Contact_telephone = "206-543-7563",
         Sensor_accuracy = "",
         Waterbody_name = "")  %>%
  # retain only sites missing an AKOATS_ID and instead have coords provided by UW
  filter(AKOATS_ID == "NA") %>%
  rename(SiteID = StationName) %>%
  mutate_all(as.character)

# join with AKOATS-provided metadata where possible, and use UW provided metadata where not

# all uw metadata w/ AKOATS IDs
uw.meta.1 <- bind_rows(akoats.meta,uw.meta) %>%
  filter(SourceName == "uwASP",
         !is.na(AKOATS_ID))

# join uw.meta.1 metadata to overall dataframe (join by AKOATS_ID)
dat.uw.1 <- dat %>%
  filter(filename == "UW",
         !is.na(filename)) %>%
  select(-SiteID) %>%
  inner_join(uw.meta.1, by = "AKOATS_ID")


# all uw metadata WITHOUT AKOATS IDs
uw.meta.2 <- bind_rows(akoats.meta,uw.meta) %>%
  filter(SourceName == "uwASP",
         is.na(AKOATS_ID)) %>%
  select(-AKOATS_ID)

# join uw.meta.2 metadata to overall dataframe (join by SiteID)
dat.uw.2 <- dat %>%
  filter(filename == "UW",
         is.na(AKOATS_ID)) %>%
  inner_join(uw.meta.2, by = "SiteID")

uw.dat <- bind_rows(dat.uw.1,dat.uw.2) %>%
  mutate_all(as.character)

rm(uw.meta,uw.meta.1,uw.meta.2,dat.uw.1,dat.uw.2)

```

<br>

* CIK metadata

Just site names and lat/long right now. Sue didn't provide lat/longs in her metadata file so need to try and match names in akoats. Still missing Roadhouse creek, checking w Dan to see if that is a site we sample... sounds familiar.

```{r}

# CIK metadata from AKOATS database
cik.meta <- akoats.meta %>%
  filter(SourceName == "CIK") %>% 
  select(SiteID, SourceName, Latitude, Longitude)

#remove sites in cook inlet.
cik.meta <- cik.meta[!grepl("CIK", cik.meta$SiteID),]

cik.meta <- bind_rows(cik.meta,
                      data.frame(SiteID = "Ben Courtney Creek",
                                 SourceName = "CIK",
                                 Latitude = 59.272417,
                                 Longitude = -156.357567))

cik.meta <- cik.meta %>% 
  mutate(SiteID = case_when(grepl("Gibraltar", SiteID) ~ "Gibraltar River",
                            TRUE ~ SiteID))
  
saveRDS(cik.meta, "output/cik_metadata.rds")

```



```{r}

# CIK metadata from AKOATS database
cik.meta.1 <- akoats.meta %>%
  filter(SourceName == "CIK") %>%
  mutate_all(as.character)

# CIK metadata from provided excel file
cik.meta.2 <- read_excel("data/CIK Bristol Bay village and lodge sites/cik_metadata/CIK Bristol Bay village and lodge sites.xlsx") %>%
  separate(`KNB ID`, sep = "_",into = c("a","AKOATS_ID")) %>%
  #select(SiteID,AKOATS_ID,Waterbody_name) %>%
  filter(!is.na(AKOATS_ID)) %>%
  distinct() %>%
  mutate_all(as.character)

# "Ben Courtney Creek" location missing.  Coordinates provided via email from Sue Mauger on 11/16/20 @ 59.272417; -156.357567

# create ben courtney creek metadata
cik.meta.bc <- data.frame("Ben Courtney Creek") %>%
  mutate(AKOATS_ID = as.character(""),
         SiteID = "Ben Courtney Creek",
         Contact_person = "Sue Mauger",
         SourceName = "CIK",
         Contact_email = "sue@inletkeeper.org",
         Contact_telephone = "907-235-4068",
         Latitude = 59.272417,
         Longitude = -156.357567,
         Sensor_accuracy = "") %>%
  select(-X.Ben.Courtney.Creek.) %>%
  mutate_all(as.character)

# also missing Roadhouse Creek info!

# join ben courtney creek metadata to other CIK metadata
cik.meta <- bind_rows(cik.meta.1,cik.meta.2,cik.meta.bc) %>%
  filter(!is.na(SourceName))

# join with AKOATS-provided metadata where possible, and use CIK provided metadata where not
dat.cik <- dat %>%
  #  correct mis-spellings at one site to match metadata
  mutate(SiteID = str_replace(SiteID,"Panaraqak Creek","Panaruqak Creek")) %>%
  mutate(SiteID = str_replace(SiteID,"Panaraquk Creek","Panaruqak Creek")) %>%
  # modify one site name to match metadata
  mutate(SiteID = str_replace(SiteID,"Gibraltar River","Gibraltar Creek")) %>%
  filter(filename == "CIK") %>%
  select(-AKOATS_ID) %>%
  left_join(cik.meta, by = "SiteID") %>%
  distinct() %>%
  select("AKOATS_ID","Date","Time","Temperature","useData","filename","SiteID",
         "Contact_person","SourceName","Contact_email","Contact_telephone",
         "Latitude","Longitude","Sensor_accuracy","Waterbody_name") %>%
  mutate_all(as.character)

rm(cik.meta,cik.meta.1,cik.meta.2,cik.meta.bc)
```

<br>

* FWS metadata

```{r}
# FWS metadata
togiak1.meta <- read_excel("data/FWS/Togiak_Oct2020_Part1.xlsx", sheet = "AKOATS_metadata")
togiak2.meta <- read_excel("data/FWS/Togiak_Oct2020_Part2.xlsx", sheet = "AKOATS_metadata")
egegik.meta <- read_excel("data/FWS/WRB_Oct2020.xlsx", sheet = "AKOATS_metadata")
fws.meta <- bind_rows(togiak1.meta,togiak2.meta,egegik.meta)

# specify strings to remove
strings <- c("I have no data for Togiak Lake",
             "I have no data for Togiak lake",
             "New Sites:",
             "was wrongly named as 571001154134300",
             "was wrongly named as 571359154244300",
             "was wrongly named as 580223156504200")

# format fws metadata
fws.meta <- fws.meta %>% 
  # choose columns to retain
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  filter(SiteID %ni% strings,
         !is.na(SiteID)) %>%
  distinct() 

# join fws data to metadata
fws.dat <- dat %>%
  filter(filename == "FWS") %>%
  select(-AKOATS_ID) %>%
  left_join(fws.meta,by = "SiteID") %>%
  mutate_all(as.character)

# note: in the metadata tab in "WRB_Oct2020.xlsx", there are three rows describing previous "wrongly named" sites, their associated AKOATS_ID, and the correct names.  Changes made are highlighted by row in yellow in "data/AKOATS_DATA_2020_working.xlsx"

rm(togiak1.meta,togiak2.meta,egegik.meta,fws.meta)
```

<br>

* UAA-ACCS metadata

Just get lat/longs for reading into a map.

```{r}
# UAA-ACCS metadata
accs.meta <- read_csv("data/ACCS-UAA/metadata/SiteLevelMetadata_Bogan.csv", col_types = cols(.default = "c")) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) 

accs.meta <- accs.meta %>% 
  select(SourceName, SiteID, AKOATS_ID, Latitude, Longitude) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))


saveRDS(accs.meta, "output/accs_metadata.rds")
```

```{r}
# UAA-ACCS metadata
accs.meta <- read_csv("data/ACCS-UAA/metadata/SiteLevelMetadata_Bogan.csv", col_types = cols(.default = "c")) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) 


# join uaa-accs data to metadata
accs.dat <- dat %>%
  filter(filename == "UAA-ACCS") %>%
  select(-SiteID) %>%
  left_join(accs.meta,by = "AKOATS_ID") %>%
  mutate_all(as.character)

rm(accs.meta)

```

<br>

* NPS metadata

```{r}
# We will use AKOATS provided metadata where available.  However a number of NPS sites do not have AKOATs_ID values,  so use NPS-provided metadata in these cases.

# AKOATS-provided metadata - streams & beaches
nps.names.sb <- dat %>%
  filter(filename == "NPS_streams_beaches") %>%
  select(SiteID) %>%
  distinct() %>%
  inner_join(akoats.meta) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) 

# AKOATS-provided metadata - lakes
nps.names.lakes <- dat.lakes %>%
  filter(filename == "NPS_lakes") %>%
  select(SiteID) %>%
  distinct() %>%
  inner_join(akoats.meta) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) 

# combine
nps.meta.1 <- bind_rows(nps.names.sb,nps.names.lakes) %>%
  mutate_all(as.character)

# which NPS sites are missing from AKOATS-provided metadata?

## list of all NPS site names
## streams & beaches
nps.names.sb <- dat %>%
  filter(filename == "NPS_streams_beaches") %>%
  select(SiteID) %>%
  distinct() %>%
  data.frame()

## lakes
nps.names.lakes <- dat.lakes %>%
  filter(filename == "NPS_lakes") %>%
  select(SiteID) %>%
  distinct() %>%
  data.frame()

## get metadata for these sites from NPS-provided sheet
nps.names.missing <- bind_rows(nps.names.sb,nps.names.lakes) %>%
  anti_join(akoats.meta)

# read in NPS-provided
nps.meta.2 <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  rename(SiteID = Agency_ID,
         Latitude = Lat,
         Longitude = Long,
         Waterbody_name = Waterbody_Name) %>%
  inner_join(nps.names.missing) %>%
  mutate(AKOATS_ID = as.double(""),
         Contact_person = "Krista Bartz",
         Contact_email = "krista_bartz@nps.gov",
         Contact_telephone = "(907) 644-3685",
         SourceName = "npsSWAN",
         Sensor_accuracy = "") %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate_all(as.character)

# join all available NPS metadata
nps.meta <- bind_rows(nps.meta.1,nps.meta.2)

# join NPS streams & beaches data to metadata
nps.dat.sb <- dat %>%
  filter(filename == "NPS_streams_beaches") %>%
  select(-AKOATS_ID) %>%
  left_join(nps.meta,by = "SiteID") %>%
  mutate_all(as.character)

# join NPS lakes data to metadata
nps.dat.lakes <- dat.lakes %>%
  select(-AKOATS_ID) %>%
  left_join(nps.meta,by = "SiteID") %>%
  mutate_all(as.character)

rm(nps.meta,nps.meta.1,nps.meta.2,nps.names.sb,nps.names.missing,nps.names.lakes)
```

<br>

* USGS metadata

```{r}
# USGS metadata
## read in directly from online w/ readNWIS pkg fxn
usgs.sites <- c("15302000","15300300","15302250","15302812")
usgs.meta <- readNWISsite(usgs.sites)

# examine akoats meta data for usgs sites
usgs.meta.akoats <- akoats.meta %>% filter(SiteID %in% usgs.sites)

# note: there is no AKOATS_ID yet for USGS site 15302812 ("KOKWOK R 22 MI AB NUSHAGAK R NR EKWOK AK)

# create metadata for site missing from akoats
usgs.meta.1 <- usgs.meta %>%
  filter(site_no == "15302812") %>%
  select(site_no,station_nm,dec_lat_va,dec_long_va) %>%
  mutate(AKOATS_ID = "",
         Contact_person = "",
         SourceName = "USGS",
         Contact_email = "",
         Contact_telephone ="",
         Sensor_accuracy = "",
         Waterbody_name = "Ekwok River") %>%
  rename(SiteID = site_no,
         Latitude = dec_lat_va,
         Longitude = dec_long_va) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate_all(as.character)

# format usgs-provided metadata to match other metadata
usgs.meta.2 <- usgs.meta %>%
  filter(site_no != "15302812") %>%
  rename(SiteID = site_no) %>%
  left_join(usgs.meta.akoats,by = "SiteID") %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate_all(as.character)

# join usgs metadata to single table
usgs.meta <- bind_rows(usgs.meta.1,usgs.meta.2) 

# join usgs data to metadata
usgs.dat <- dat %>%
  filter(filename == "USGS") %>%
  select(-AKOATS_ID) %>%
  left_join(usgs.meta,by = "SiteID") %>%
  mutate_all(as.character)

rm(usgs.meta,usgs.meta.1,usgs.meta.2,usgs.meta.akoats)

```

<br>



Join all data in to one table (takes a few minutes...)
```{r}
# combine
dat <- bind_rows(uw.dat,dat.cik,fws.dat,accs.dat,nps.dat.sb,usgs.dat) 
rm(uw.dat,dat.cik,fws.dat,accs.dat,nps.dat.sb,usgs.dat)
```


Note 11/22/20: there are a few sites that have instantaneous minimum temperatures still ~ -2 to -5 C even after extensive double-checking that daily mean temps in this range have been labeled with useData = 0.  Likely diagnosis: there are some instantaneous min temps in this range that are masked when doing QC at the daily mean level.

Solution applied on 11/22/20: all temps < -1 C are excluded here at this step.

```{r}
# prep for summary table
## streams & beaches
dat <- dat %>%
  filter(useData == 1,
         Temperature > -1,
         !is.na(Temperature),
         !is.na(Time)) %>%
  rename(sampleTime = Time,
         sampleDate = Date) %>%
  transform(sampleTime = hms::as_hms(sampleTime),
            sampleDate = as.Date(sampleDate),
            Temperature = as.numeric(Temperature)) %>%
  mutate(year = year(sampleDate)) 


## lakes
dat.lakes <- nps.dat.lakes %>%
  filter(useData == 1,
         !is.na(Temperature)) %>%
  rename(sampleTime = Time,
         sampleDate = Date) %>%
  transform(sampleTime = hms::as_hms(sampleTime),
            sampleDate = as.Date(sampleDate),
            Temperature = as.numeric(Temperature)) %>%
  mutate(year = year(sampleDate)) 

rm(nps.dat.lakes)

```

<br>

Create summary metadata table
```{r}
fnl.tbl <- dat %>%
  group_by(AKOATS_ID,SiteID,Contact_person,SourceName,Latitude,
           Longitude,Sensor_accuracy,Waterbody_name) %>%
  summarise(start_year = min(year),
            end_year = max(year),
            max_temp = max(Temperature),
            min_temp = min(Temperature),
            mean_temp = mean(Temperature)
            )

# temperature exclusion pipes appear to not be functioning in data_UW.Rmd as of 11/21/20, thus min/max/mean most sites incorrect.  gah.

write.csv(fnl.tbl, "output/summary_tables/summary_table.csv", row.names = F)

# announce completion
speak("All done! Check it out!")

# 
```


<br>

Create lakes summary metadata table
```{r}
# 
fnl.tbl.lakes <- dat.lakes %>%
  group_by(AKOATS_ID,Depth,SiteID,Contact_person,SourceName,Latitude,
           Longitude,Sensor_accuracy,Waterbody_name) %>%
  summarise(start_year = min(year),
            end_year = max(year),
            max_temp = max(Temperature),
            min_temp = min(Temperature),
            mean_temp = mean(Temperature)
            )

write.csv(fnl.tbl.lakes, "output/summary_tables/summary_table_lakes.csv", row.names = F)

# announce completion
rsay("All done! Check it out!")

```

