---
title: "All Sites Metadata"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Current metadata sections are in each data cleaning script. This is only for combining them.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
library(tidyverse)
library(lubridate)
library(readr)
library(readxl)
library(hms)
library(plotly)
library(DT)
library(dataRetrieval)
# library(rsay)

```



# Metadata

Read in metadata for all the sites from the different providers/projects. Combine into one data frame:

* data_SiteID - this is the correct site id to link to the data table, if needed.
* SiteID - this is the site id provided by the agency, but not always unique or linked directly to the data as provided
* AKOATS_ID
* waterbody_name - we will need this to confirm correct catchment location
* latitude
* longitude
* Source_Name
* Contact_Name

These data are being used for two projects, the Southwest partnership project, which is primarly based on the empirical data. And also the AKSSF, where we need accurate locations so that we can extract hydrologically-correct variables for modeling. The final metadata file at the end of this script is being read into the AKSSF repo to have one complete metadata file for all data in the AKSSF regions. The locations will be reviewed in GIS to link to catchments (e.g. add a catchment_ID) so that we can extract daymet air temperatures and correctly link them to sites.


Read in from metadata from data preparation scripts and combine as one for plotting.

```{r}
fws <- readRDS("output/fws_metadata.rds")

nps <- readRDS("output/nps_metadata.rds") 

cik <- readRDS("output/cik_metadata.rds")

accs <- readRDS("output/accs_metadata.rds")

uw <- readRDS("output/uw_metadata.rds")

bb_md <- bind_rows(fws, nps, cik, accs, uw)

saveRDS(bb_md, "output/bristol_bay_site_metadata.rds")

```



## AKOATS metadata

```{r}
akoats.meta <- read_excel("data/AKOATS_DATA_2020_Working.xlsx", sheet = "CONTINUOUS_DATA", col_types = "text") %>%
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  rename(AKOATS_ID = seq_id,
         SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats.meta

```






## NPS metadata - OLD

```{r}
# We will use AKOATS provided metadata where available.  However a number of NPS sites do not have AKOATs_ID values,  so use NPS-provided metadata in these cases.

# AKOATS-provided metadata - streams & beaches
nps.names.sb <- dat %>%
  filter(filename == "NPS_streams_beaches") %>%
  select(SiteID) %>%
  distinct() %>%
  inner_join(akoats.meta) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) 

# AKOATS-provided metadata - lakes
nps.names.lakes <- dat.lakes %>%
  filter(filename == "NPS_lakes") %>%
  select(SiteID) %>%
  distinct() %>%
  inner_join(akoats.meta) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) 

# combine
nps.meta.1 <- bind_rows(nps.names.sb,nps.names.lakes) %>%
  mutate_all(as.character)

# which NPS sites are missing from AKOATS-provided metadata?

## list of all NPS site names
## streams & beaches
nps.names.sb <- dat %>%
  filter(filename == "NPS_streams_beaches") %>%
  select(SiteID) %>%
  distinct() %>%
  data.frame()

## lakes
nps.names.lakes <- dat.lakes %>%
  filter(filename == "NPS_lakes") %>%
  select(SiteID) %>%
  distinct() %>%
  data.frame()

## get metadata for these sites from NPS-provided sheet
nps.names.missing <- bind_rows(nps.names.sb,nps.names.lakes) %>%
  anti_join(akoats.meta)

# read in NPS-provided
nps.meta.2 <- read_excel("data/NPS Bartz/Site_Info.xlsx", skip = 4) %>%
  rename(SiteID = Agency_ID,
         Latitude = Lat,
         Longitude = Long,
         Waterbody_name = Waterbody_Name) %>%
  inner_join(nps.names.missing) %>%
  mutate(AKOATS_ID = as.double(""),
         Contact_person = "Krista Bartz",
         Contact_email = "krista_bartz@nps.gov",
         Contact_telephone = "(907) 644-3685",
         SourceName = "npsSWAN",
         Sensor_accuracy = "") %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate_all(as.character)

# join all available NPS metadata
nps.meta <- bind_rows(nps.meta.1,nps.meta.2)

# join NPS streams & beaches data to metadata
nps.dat.sb <- dat %>%
  filter(filename == "NPS_streams_beaches") %>%
  select(-AKOATS_ID) %>%
  left_join(nps.meta,by = "SiteID") %>%
  mutate_all(as.character)

# join NPS lakes data to metadata
nps.dat.lakes <- dat.lakes %>%
  select(-AKOATS_ID) %>%
  left_join(nps.meta,by = "SiteID") %>%
  mutate_all(as.character)

rm(nps.meta,nps.meta.1,nps.meta.2,nps.names.sb,nps.names.missing,nps.names.lakes)
```


## USGS metadata

```{r}
# USGS metadata
## read in directly from online w/ readNWIS pkg fxn
usgs.sites <- c("15302000","15300300","15302250","15302812")
usgs.meta <- readNWISsite(usgs.sites)

# examine akoats meta data for usgs sites
usgs.meta.akoats <- akoats.meta %>% filter(SiteID %in% usgs.sites)

# note: there is no AKOATS_ID yet for USGS site 15302812 ("KOKWOK R 22 MI AB NUSHAGAK R NR EKWOK AK)

# create metadata for site missing from akoats
usgs.meta.1 <- usgs.meta %>%
  filter(site_no == "15302812") %>%
  select(site_no,station_nm,dec_lat_va,dec_long_va) %>%
  mutate(AKOATS_ID = "",
         Contact_person = "",
         SourceName = "USGS",
         Contact_email = "",
         Contact_telephone ="",
         Sensor_accuracy = "",
         Waterbody_name = "Ekwok River") %>%
  rename(SiteID = site_no,
         Latitude = dec_lat_va,
         Longitude = dec_long_va) %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate_all(as.character)

# format usgs-provided metadata to match other metadata
usgs.meta.2 <- usgs.meta %>%
  filter(site_no != "15302812") %>%
  rename(SiteID = site_no) %>%
  left_join(usgs.meta.akoats,by = "SiteID") %>%
  select(AKOATS_ID,SiteID,Contact_person,SourceName,Contact_email,Contact_telephone,Latitude,Longitude,Sensor_accuracy,Waterbody_name) %>%
  mutate_all(as.character)

# join usgs metadata to single table
usgs.meta <- bind_rows(usgs.meta.1,usgs.meta.2) 

# join usgs data to metadata
usgs.dat <- dat %>%
  filter(filename == "USGS") %>%
  select(-AKOATS_ID) %>%
  left_join(usgs.meta,by = "SiteID") %>%
  mutate_all(as.character)

rm(usgs.meta,usgs.meta.1,usgs.meta.2,usgs.meta.akoats)

```



Join all data in to one table (takes a few minutes...)
```{r}
# combine
dat <- bind_rows(uw.dat,dat.cik,fws.dat,accs.dat,nps.dat.sb,usgs.dat) 
rm(uw.dat,dat.cik,fws.dat,accs.dat,nps.dat.sb,usgs.dat)
```


Note 11/22/20: there are a few sites that have instantaneous minimum temperatures still ~ -2 to -5 C even after extensive double-checking that daily mean temps in this range have been labeled with useData = 0.  Likely diagnosis: there are some instantaneous min temps in this range that are masked when doing QC at the daily mean level.

Solution applied on 11/22/20: all temps < -1 C are excluded here at this step.

```{r}
# prep for summary table
## streams & beaches
dat <- dat %>%
  filter(useData == 1,
         Temperature > -1,
         !is.na(Temperature),
         !is.na(Time)) %>%
  rename(sampleTime = Time,
         sampleDate = Date) %>%
  transform(sampleTime = hms::as_hms(sampleTime),
            sampleDate = as.Date(sampleDate),
            Temperature = as.numeric(Temperature)) %>%
  mutate(year = year(sampleDate)) 


## lakes
dat.lakes <- nps.dat.lakes %>%
  filter(useData == 1,
         !is.na(Temperature)) %>%
  rename(sampleTime = Time,
         sampleDate = Date) %>%
  transform(sampleTime = hms::as_hms(sampleTime),
            sampleDate = as.Date(sampleDate),
            Temperature = as.numeric(Temperature)) %>%
  mutate(year = year(sampleDate)) 

rm(nps.dat.lakes)

```

<br>

Create summary metadata table
```{r}
fnl.tbl <- dat %>%
  group_by(AKOATS_ID,SiteID,Contact_person,SourceName,Latitude,
           Longitude,Sensor_accuracy,Waterbody_name) %>%
  summarise(start_year = min(year),
            end_year = max(year),
            max_temp = max(Temperature),
            min_temp = min(Temperature),
            mean_temp = mean(Temperature)
            )

# temperature exclusion pipes appear to not be functioning in data_UW.Rmd as of 11/21/20, thus min/max/mean most sites incorrect.  gah.

write.csv(fnl.tbl, "output/summary_tables/summary_table.csv", row.names = F)

# announce completion
speak("All done! Check it out!")

# 
```


<br>

Create lakes summary metadata table
```{r}
# 
fnl.tbl.lakes <- dat.lakes %>%
  group_by(AKOATS_ID,Depth,SiteID,Contact_person,SourceName,Latitude,
           Longitude,Sensor_accuracy,Waterbody_name) %>%
  summarise(start_year = min(year),
            end_year = max(year),
            max_temp = max(Temperature),
            min_temp = min(Temperature),
            mean_temp = mean(Temperature)
            )

write.csv(fnl.tbl.lakes, "output/summary_tables/summary_table_lakes.csv", row.names = F)

# announce completion
rsay("All done! Check it out!")

```

